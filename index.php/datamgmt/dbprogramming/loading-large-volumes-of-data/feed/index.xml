<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Loading Large Volumes of Data into SQL Azure with SSIS</title>
	<atom:link href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/feed/" rel="self" type="application/rss+xml" />
	<link>/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/</link>
	<description>A Technical Community for IT Professionals</description>
	<lastBuildDate>Tue, 26 Feb 2019 12:40:14 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.6.1</generator>
	<item>
		<title>By: dd</title>
		<link>/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-3962113</link>
		<dc:creator><![CDATA[dd]]></dc:creator>
		<pubDate>Mon, 16 Mar 2015 12:40:53 +0000</pubDate>
		<guid isPermaLink="false">/index.php/2013/03/loading-large-volumes-of-data/#comment-3962113</guid>
		<description><![CDATA[I haven&#039;t used Azure yet, but I am very curious. Given the incredibly slow times using the method you outlined, I was surprised you didn&#039;t give the times for other methods \ variations. You mentioned BCP and Bulk Copy - but what were the timings? Did you try more than 4 segments? I would be curious of how long it would take a straight-forward Select * into, with a linked server connection to Azure.]]></description>
		<content:encoded><![CDATA[<p>I haven&#8217;t used Azure yet, but I am very curious. Given the incredibly slow times using the method you outlined, I was surprised you didn&#8217;t give the times for other methods \ variations. You mentioned BCP and Bulk Copy &#8211; but what were the timings? Did you try more than 4 segments? I would be curious of how long it would take a straight-forward Select * into, with a linked server connection to Azure.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Ted Krueger (onpnt)</title>
		<link>/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-5969</link>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
		<pubDate>Wed, 03 Apr 2013 07:42:25 +0000</pubDate>
		<guid isPermaLink="false">/index.php/2013/03/loading-large-volumes-of-data/#comment-5969</guid>
		<description><![CDATA[In order to assist anyone else doing this amount of data, I had to reevaluate my outbound speed, calculate that to the packet size I was sending, trim it down and now I&#039;m sending without SQL Azure actively disconnecting me once and awhile due to it waiting for a large amount of data choking it at some points.  I&#039;ve been actively pushing data (at around 40 million out now) for 23 hours.  &lt;br /&gt;
&lt;br /&gt;
It was also critical to have a structure in the table(s) in which I could restart the process from.  A simple identity column met that.  Although that meant I had to order the data in the source.  Making sure there was a NC index there to do that with a good execution plan helped.  Then, if the line cuts, I can determine the max ID and just edit the query to start from the next.  &lt;br /&gt;
&lt;br /&gt;
This was needed because if you try to delete 20 million rows on SQL Azure, you&#039;ll likely get a log space error after waiting for awhile.  I have to check to see how to manipulate that or do it another way.  But, the only way around to get it done quicker was to drop the table which was not logged like a delete (as most of you know).&lt;br /&gt;
&lt;br /&gt;
A little planning and work, it can be done.]]></description>
		<content:encoded><![CDATA[<p>In order to assist anyone else doing this amount of data, I had to reevaluate my outbound speed, calculate that to the packet size I was sending, trim it down and now I&#8217;m sending without SQL Azure actively disconnecting me once and awhile due to it waiting for a large amount of data choking it at some points.  I&#8217;ve been actively pushing data (at around 40 million out now) for 23 hours.  </p>
<p>It was also critical to have a structure in the table(s) in which I could restart the process from.  A simple identity column met that.  Although that meant I had to order the data in the source.  Making sure there was a NC index there to do that with a good execution plan helped.  Then, if the line cuts, I can determine the max ID and just edit the query to start from the next.  </p>
<p>This was needed because if you try to delete 20 million rows on SQL Azure, you&#8217;ll likely get a log space error after waiting for awhile.  I have to check to see how to manipulate that or do it another way.  But, the only way around to get it done quicker was to drop the table which was not logged like a delete (as most of you know).</p>
<p>A little planning and work, it can be done.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Ted Krueger (onpnt)</title>
		<link>/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-5968</link>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
		<pubDate>Sat, 23 Mar 2013 12:41:32 +0000</pubDate>
		<guid isPermaLink="false">/index.php/2013/03/loading-large-volumes-of-data/#comment-5968</guid>
		<description><![CDATA[Yep.  All was done :)&lt;br /&gt;
&lt;br /&gt;
I&#039;ll get you something later to play with]]></description>
		<content:encoded><![CDATA[<p>Yep.  All was done ðŸ™‚</p>
<p>I&#8217;ll get you something later to play with</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: SQLDenis</title>
		<link>/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-5967</link>
		<dc:creator><![CDATA[SQLDenis]]></dc:creator>
		<pubDate>Sat, 23 Mar 2013 08:11:52 +0000</pubDate>
		<guid isPermaLink="false">/index.php/2013/03/loading-large-volumes-of-data/#comment-5967</guid>
		<description><![CDATA[Your upload speed could be a factor as well. If you only have 600Kb upload then it might be a problem&lt;br /&gt;
&lt;br /&gt;
I assume you sized the database as well as explained in this post &lt;a href=&quot;/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/sizing-database-files&quot;&gt;Sizing database files&lt;/a&gt;  otherwise the files might be growing and thus slowing down your inserts&lt;br /&gt;
&lt;br /&gt;
Can you give me the DDL of the table you are using, you can change the column names. I want to run a test with a million rows between RDS and Azure]]></description>
		<content:encoded><![CDATA[<p>Your upload speed could be a factor as well. If you only have 600Kb upload then it might be a problem</p>
<p>I assume you sized the database as well as explained in this post <a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/sizing-database-files">Sizing database files</a>  otherwise the files might be growing and thus slowing down your inserts</p>
<p>Can you give me the DDL of the table you are using, you can change the column names. I want to run a test with a million rows between RDS and Azure</p>
]]></content:encoded>
	</item>
</channel>
</rss>
