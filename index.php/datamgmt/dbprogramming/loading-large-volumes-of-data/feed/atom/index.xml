<?xml version="1.0" encoding="UTF-8" ?><feed
	xmlns="http://www.w3.org/2005/Atom"
	xml:lang="en-US"
	xmlns:thr="http://purl.org/syndication/thread/1.0"
	>
	<title type="text">Comments on Loading Large Volumes of Data into SQL Azure with SSIS</title>
	<subtitle type="text">A Technical Community for IT Professionals</subtitle>

	<updated>2019-02-26T12:40:14Z</updated>

	<link rel="alternate" type="text/html" href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comments" />
	<link rel="self" type="application/atom+xml" href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/feed/atom/" />
	<id>/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/feed/atom/</id>
<generator uri="https://wordpress.org/" version="4.6.1">WordPress</generator>
	<entry>
		<title>By: dd</title>
		<link rel="alternate" href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-3962113" type="text/html" />

		<author>
			<name>dd</name>
			
		</author>

		<id>/index.php/2013/03/loading-large-volumes-of-data/#comment-3962113</id>
		<updated>2015-03-16T12:40:53Z</updated>
		<published>2015-03-16T12:40:53Z</published>
		<content type="html" xml:base="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-3962113"><![CDATA[<p>I haven&#8217;t used Azure yet, but I am very curious. Given the incredibly slow times using the method you outlined, I was surprised you didn&#8217;t give the times for other methods \ variations. You mentioned BCP and Bulk Copy &#8211; but what were the timings? Did you try more than 4 segments? I would be curious of how long it would take a straight-forward Select * into, with a linked server connection to Azure.</p>
]]></content>
		<thr:in-reply-to ref="/index.php/2013/03/loading-large-volumes-of-data/" href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/" type="text/html" />
	</entry>
	<entry>
		<title>By: Ted Krueger (onpnt)</title>
		<link rel="alternate" href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-5969" type="text/html" />

		<author>
			<name>Ted Krueger (onpnt)</name>
			
		</author>

		<id>/index.php/2013/03/loading-large-volumes-of-data/#comment-5969</id>
		<updated>2013-04-03T07:42:25Z</updated>
		<published>2013-04-03T07:42:25Z</published>
		<content type="html" xml:base="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-5969"><![CDATA[<p>In order to assist anyone else doing this amount of data, I had to reevaluate my outbound speed, calculate that to the packet size I was sending, trim it down and now I&#8217;m sending without SQL Azure actively disconnecting me once and awhile due to it waiting for a large amount of data choking it at some points.  I&#8217;ve been actively pushing data (at around 40 million out now) for 23 hours.  </p>
<p>It was also critical to have a structure in the table(s) in which I could restart the process from.  A simple identity column met that.  Although that meant I had to order the data in the source.  Making sure there was a NC index there to do that with a good execution plan helped.  Then, if the line cuts, I can determine the max ID and just edit the query to start from the next.  </p>
<p>This was needed because if you try to delete 20 million rows on SQL Azure, you&#8217;ll likely get a log space error after waiting for awhile.  I have to check to see how to manipulate that or do it another way.  But, the only way around to get it done quicker was to drop the table which was not logged like a delete (as most of you know).</p>
<p>A little planning and work, it can be done.</p>
]]></content>
		<thr:in-reply-to ref="/index.php/2013/03/loading-large-volumes-of-data/" href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/" type="text/html" />
	</entry>
	<entry>
		<title>By: Ted Krueger (onpnt)</title>
		<link rel="alternate" href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-5968" type="text/html" />

		<author>
			<name>Ted Krueger (onpnt)</name>
			
		</author>

		<id>/index.php/2013/03/loading-large-volumes-of-data/#comment-5968</id>
		<updated>2013-03-23T12:41:32Z</updated>
		<published>2013-03-23T12:41:32Z</published>
		<content type="html" xml:base="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-5968"><![CDATA[<p>Yep.  All was done ðŸ™‚</p>
<p>I&#8217;ll get you something later to play with</p>
]]></content>
		<thr:in-reply-to ref="/index.php/2013/03/loading-large-volumes-of-data/" href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/" type="text/html" />
	</entry>
	<entry>
		<title>By: SQLDenis</title>
		<link rel="alternate" href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-5967" type="text/html" />

		<author>
			<name>SQLDenis</name>
			
		</author>

		<id>/index.php/2013/03/loading-large-volumes-of-data/#comment-5967</id>
		<updated>2013-03-23T08:11:52Z</updated>
		<published>2013-03-23T08:11:52Z</published>
		<content type="html" xml:base="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/#comment-5967"><![CDATA[<p>Your upload speed could be a factor as well. If you only have 600Kb upload then it might be a problem</p>
<p>I assume you sized the database as well as explained in this post <a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/sizing-database-files">Sizing database files</a>  otherwise the files might be growing and thus slowing down your inserts</p>
<p>Can you give me the DDL of the table you are using, you can change the column names. I want to run a test with a million rows between RDS and Azure</p>
]]></content>
		<thr:in-reply-to ref="/index.php/2013/03/loading-large-volumes-of-data/" href="/index.php/datamgmt/dbprogramming/loading-large-volumes-of-data/" type="text/html" />
	</entry>
</feed>
