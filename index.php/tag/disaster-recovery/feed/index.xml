<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>disaster recovery &#8211; LessthanDot</title>
	<atom:link href="/index.php/tag/disaster-recovery/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>A Technical Community for IT Professionals</description>
	<lastBuildDate>Sat, 09 Mar 2019 12:50:36 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.6.1</generator>
	<item>
		<title>LessThanDot Outage &#8211; Aug 22, 2013 to Sept 4, 2013</title>
		<link>/index.php/itprofessionals/itservicemanagement/lessthandot-outage-2013/</link>
		<comments>/index.php/itprofessionals/itservicemanagement/lessthandot-outage-2013/#comments</comments>
		<pubDate>Wed, 04 Sep 2013 20:38:00 +0000</pubDate>
		<dc:creator><![CDATA[Eli Weinstock-Herman (tarwn)]]></dc:creator>
				<category><![CDATA[IT Service Management]]></category>
		<category><![CDATA[Other]]></category>
		<category><![CDATA[Running My Own IT Business]]></category>
		<category><![CDATA[disaster recovery]]></category>
		<category><![CDATA[outage]]></category>

		<guid isPermaLink="false">/index.php/2013/09/lessthandot-outage-2013/</guid>
		<description><![CDATA[Over the past couple weeks you may have noticed that LessThanDot was completely down. I know we did. We're working on solutions to our missing DR plan and shoring up some holes we found in our infrastructure (we had backups, they were just a little, er,&#8230;]]></description>
				<content:encoded><![CDATA[<p>Over the past couple weeks you may have noticed that LessThanDot was completely down. I know we did. We&#8217;re working on solutions to our missing DR plan and shoring up some holes we found in our infrastructure (we had backups, they were just a little, er, stale).</p>
<p>Cue jokes about how a group with professional developers, architects, project managers, managers, consultants, etc didn&#8217;t have a DR plan.</p>
<p>Rest assured, this site means a lot to us (the founders), us (the people who blog here), and us (the people who get information or look up resources here). Along with updates to the &#8220;holy crap where did the site go&#8221; plan, this may be the jab we need to get back to some other feature-y changes we had lost steam on.</p>
<p>During the outage, I kept a temporary Azure website up with some information about the outage and a link to the monitoring (that I finally implemented an hour into the outage, oh well). We&#8217;ll have additional details and reactions posted in the blogs about the outage in the near future, but here is the content of that post for those that had not yet had a chance to see it:</p>
<hr />
<p> Thursday evening (my time), twitter messages started flowing in from LTD&#8217;ers who couldn&#8217;t access the site. </p>
<div style="text-align:  center; color:  #666666;">
            <img src="http://lessthandot.azurewebsites.net/images/tweets.png" alt="Twitter: Who ate our server?" title="Twitter: Who ate our server?" style="max-width: 600px;" /><br />
            Indeed.
        </div>
<h2>DDOS?</h2>
<p>        Our host, <a href="http://host4geeks.com/">host4geek</a> responded 4-5 hours into the incident to let us know a DDOS was occurring against the facility our server was in (which would not be covered by the <a href="https://host4geeks.com/tos/">SLA</a>). I added the site monitoring I had always intended to add but<br />
        never got around to. That way I&#8217;d get an alert when it came back up (or a measurement if it did not). </p>
<div style="text-align:  center; color:  #666666;">
            <a href="http://stats.pingdom.com/0nt3y09cs5iy/935183"><img src="https://share.pingdom.com/banners/4931d952" alt="Uptime Report for http://lessthandot.com:/ Last 30 days" title="Uptime Report for http://lessthandot.com:/ Last 30 days" width="300" height="165" style="max-width:  300px" /></a><br />
            Click image for the full status page
        </div>
<p>Thank you <a href="http://pingdom.com/">pingdom</a> for a slick and easy setup process. The stats will look funky since I set this up an hour or two into the outage, so there&#8217;s no successful uptime recorded earlier in the month.</p>
<h2>Where&#8217;s the Server?</h2>
<p>
        Late Friday afternoon we received an update from Host4Geeks. Apparently they have a provider for the facitility and that provider had been planning a facility move. The provider had scheduled server migrations and then apparently Thursday night decided to just load up the remaining servers on a truck and take them to a third facility. or something. From<br />
        what I can tell this doesn&#8217;t fall under natural disasters, DDOS attacks, or planned maintenance, so we&#8217;ll also be working on getting credits for the service outage.
        </p>
<h2>So&#8230;ETA?</h2>
<p>
        So where is our server? I&#8217;m not entirely sure. We have not been given an ETA on when we can expect to be back online. Obviously this sucks for you, because you were probably trying to get to some very useful content. It sucks for us because we&#8217;d like to write some more really useful content. In the meantime, we&#8217;ll be working on some more content for everyone, considering our hosting options, and we&#8217;ll update this page as we have more information.
        </p>
<h2>Updated, Server Still MIA, 2013-08-27 9:45PM EDT (28th 1:45AM UTC)</h2>
<p>
        Good news, our host has informed us they recovered some of their servers! Well, not our server, but woohoo! Er&#8230;yeah. We have now been down for over 120 hours. We will be back, it&#8217;s just a question of whether they find our server before they have to offer us free hosting for life or not. We&#8217;ve also been discussing better disaster recovery plans (it&#8217;s a good thing none of us do that for a living, oh wait&#8230;), so on a positive note once we get through this outage we&#8217;ll be in a better place. And how often can you tell people your server is down because it was migrated to a truck in a dark parking lot somewhere&#8230;
        </p>
<h2 id="updated">Updated, No SLA, No ETA, 2013-09-03</h2>
<p>
        After prodding them again, our &#8216;host&#8217; has admitted they have no ETA on getting the remaining servers (ours included). We have also noticed that dedicated servers are specifically excluded from the SLA, so in essence the policy of our host is &#8220;downtime? sucks to be you&#8221;. We also found out that we forgot to finish setting up backups when we set up the server with them, so our most recent backup is March. They have told us our options are to setup a new server from our backups or &#8220;wait indefinitely&#8221; for our server to be located and brought online.
        </p>
<p>
        We will likely have to go the new server setup route, the next question we will have to consider is whether to do it with this host (who is paid through the end of the year), or find another and raise funds early for another year.
        </p>
<hr />
<p>Luckily as we were building out the new server, the old one finally came back online. More to come soon (including back to our regularly scheduled outpouring of technical blog goodness).</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/itprofessionals/itservicemanagement/lessthandot-outage-2013/feed/</wfw:commentRss>
		<slash:comments>4</slash:comments>
		</item>
		<item>
		<title>SQL Server and High Availability</title>
		<link>/index.php/datamgmt/dbprogramming/sql-server-high-availability/</link>
		<comments>/index.php/datamgmt/dbprogramming/sql-server-high-availability/#respond</comments>
		<pubDate>Fri, 11 Jun 2010 17:57:31 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[disaster recovery]]></category>
		<category><![CDATA[dr]]></category>
		<category><![CDATA[ha]]></category>
		<category><![CDATA[high availability]]></category>
		<category><![CDATA[log shipping]]></category>
		<category><![CDATA[mirroring]]></category>
		<category><![CDATA[restore]]></category>
		<category><![CDATA[sql server 2008]]></category>
		<category><![CDATA[sql university]]></category>

		<guid isPermaLink="false">/index.php/2010/06/sql-server-high-availability/</guid>
		<description><![CDATA[Realistically, 100% is unachievable given the nature of computing.  There are needs for a SQL Server and Windows Server to be rebooted at least once a year.  This is to allow for updates on both SQL Server and Windows to be maintained.  So the ranking method we use for measuring high availability is the "nines" scale.  The five nines is a goal that most database administrators and teams set for their standards.  The five nines level is a height of availability that is truly an achievement and one to be proud of.]]></description>
				<content:encoded><![CDATA[<p><img src="/wp-content/uploads/blogs/DataMgmt/sqlu_logo.gif" alt="" title="" width="150" height="166" align="left" />
<p>Welcome to the last class of HA / DR week for SQL University.  It has been a great week discussing these topics with all of you.  We <a href="/index.php/DataMgmt/DBAdmin/sqlu-taking-a-break-for-recess">recapped</a> those classes in order to highlight the key points over the week yesterday.  So far we&#8217;ve covered a great deal but really have only scratched the surface of SQL Server features for HA and DR.  Today will be another scratch in the surface regarding the High Availability points for SQL Server.  Be sure to check the resources links through this article.  They will greatly add an extension to today and further build your knowledge of the vast amount of abilities we have at our disposal.  </p>
<p>Now that HA / DR week is completed, please take a moment to rate this week (and others) by filling out the <a href="https://spreadsheets.google.com/a/sqlchicken.com/viewform?hl=en&amp;formkey=dDBoSW02QldrTTc2dER3WVZheUlEX3c6MQ#gid=0">SQL University Course Evaluation</a> and select HA/DR Week.  </p>
<h2><strong>What is HA?</strong></h2>
<p>High Availability (HA) for SQL Server can be defined in one sentence: Keep data available 100% of the time.   That really is the objective of HA and nothing short of that.</p>
<p>Realistically, 100% is unachievable given the nature of computing.  There are needs for a SQL Server and Windows Server to be rebooted at least once a year.  This is to allow for updates on both SQL Server and Windows to be maintained.  So the ranking method we use for measuring high availability is the &#8220;nines&#8221; scale.  The five nines is a goal that most database<br />
administrators and teams set for their standards.  The five nines level is a height of availability that is truly an achievement and one to be proud of.  </p>
<p>The table below illustrates the availability calculation.  This table has been used for quite some time and has been adopted by IT teams as the measurement for downtime. Downtime equates to uptime (depends on if you are talking to a CTO, CEO or an IT manager how to phrase it).  </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/ha_1.gif" alt="" title="" width="547" height="310" /></div>
<p align="center"><i><a href="http://en.wikipedia.org/wiki/High_availability">High Availability</a></i>
</p>
<p>The six nines goal is almost unachievable but can be done in some infrastructures.  Why are the six nines almost out of reach?  Given that a reboot on Windows takes a little under 2 minutes, reaching only a 31 seconds downtime goal becomes a bit farfetched.  Even if clustering and mirroring is setup, the failover times still will add up to around 15 seconds per failover.  The five nines is a goal that you can achieve with a mid-level installation and still meet the needs to keep your systems up to date yearly.  </p>
<p>Windows clustering has come a long way over the last few versions.  Windows Server 2008 in particular has a sound clustering service that is more stable than its predecessors (In this author’s opinion).  Windows clustering will give you the ability to automatically failover due to hardware failures and most Operating System failures.  The concept of this ability alone allows the setup to be valuable to HA strategies.  Clustering consists of two or more physical servers.  These physical servers act like a partnership and are always in communication with each other ensuring that their specific roles are being met.  These servers can be in a state of Active or Passive but one must be in a Active state at all times.  </p>
<p>A benefit to Windows Clustering is the storage location of the actual databases.  This would be located on disk (NAS, SAN etc…) outside the physical servers.  Given this, we enhance HA by having the ability to replicate the disk along with the safety of the cluster acting in partnership with DR.  Mirroring can be added to the cluster fully enhancing the entire landscape of the true HA strategy.  Windows Server 2008 Clustering all together is a straight forward setup and deployment but will take added knowledge and research to ensure the cluster is configured to the specifics of each environment.  With any cluster, there is complexity in the configurations and ensuring the two servers stay in sync for programs, hardware and services but administrative tasks are much lighter than they were in the past.  </p>
<h2><strong>Maintenance and downtime </strong></h2>
<p>Downtime can be caused by any type of disruption to the availability of data.  Maintenance tasks like index rebuilding, updates and ETL loads all fall into that category.  When doing large index rebuild tasks, the tables may become unavailable to the users.  In the database view of this, they are simply locked at this time.  This equates to almost complete loss of the ability to access them though and thus, a failure in our HA strategy.  These tasks should be planned out careful to prevent this type of failure in HA.  </p>
<p>Windows and SQL Server updates must be maintained.  This is not optional.  Every update should be analyzed carefully and the full extent of its impact on the systems taken into consideration.  Some updates may cause SQL Server services to stop or other supporting services that are needed to maintain availability.  If they are, the downtime should be planned carefully to prevent the HA landscape from doing its job of preventing loss and availability.  That means pausing mirroring or failing over nodes in a cluster to retain the availability of data services as much as possible.  </p>
<h2><strong>Geo-Clustering </strong></h2>
<p>Geo-Clustering allows us to plan High Availability across geographically located sites.  This is achieved through replication of disk to each site and essentially mirrors each system to the next.  This HA option of high availability as a hefty price tag on hardware and networking abilities.  Recently, Paul Randal (<a href="http://sqlskills.com/blogs/paul/">Blog</a> | <a href="http://twitter.com/paulrandal">Twitter</a>)published a White paper <a href="http://blogs.msdn.com/b/tommills/archive/2010/06/02/new-sql-server-2008-r2-high-availability-whitepaper-published.aspx"><i>Proven SQL Server Architectures for High Availability and Disaster Recovery</i></a>.  This went over HA setups (and DR) that are in use in businesses now and proven to work effectively.  This white paper goes into detail on actual in-place configurations in Geo-Clustering as well.  This class lucks out on the timing of this publication by us being able to link to it as a resource in understanding large business HA and Geo-Clustering.  </p>
<h2><strong>Database Mirroring</strong></h2>
<p>Database mirroring was introduced in SQL Server 2005 Standard and Enterprise editions.  Prior to the introduction of database mirroring, there was difficulty in achieving HA with SQL Server.  Clustering and disk replication were options but still limited.  With database mirroring the options for HA landscapes opens up greatly.  This doubled with the advancements in Windows Clustering added a great deal of enterprise abilities to SQL Server.  </p>
<p>Our landscape for data services can take on a new form for HA and exist without Windows Clustering as well.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/ha_2.gif" alt="" title="" width="405" height="386" /></div>
<p>What this type of HA landscape provides is even more stripped down overhead in administrative burdens and we have cost savings opportunities to assist in providing a HA solution.</p>
<p>In the diagram above, the landscape consists of two physical database servers.  Those database servers a standalone installations and active as each unique entity to the infrastructure.    Alias naming can be added to the scenario to ensure the clients access the database servers in the event of a failover.  Alias naming can be forgone with the changes in recent years to the ability of connections from applications to allow for a failover specification.  </p>
<pre>Data Source=myServerAddress;Failover Partner=myMirrorServerAddress;Initial Catalog=myDataBase;Integrated Security=True;</pre>
<p align="center"><i>Resource:  <a href="http://www.connectionstrings.com/sql-server-2008">ConnectionStrings.com</a></i></p>
<p>We can see in the example connection string above the Failover parameter which allows us to make applications much smarter as opposed to recent years with actually developing tests in code to determine the availability of data services.  </p>
<p>Another addition is added to the diagram that is provided with Enterprise Edition.  That is snapshots capabilities.  In this landscape (and clustering) a path can exist from another reporting server to snapshots that are taken of the mirror.  This reporting solution is a huge benefit to the entire structure of the data services by lower the activity on the OLTP side.  Reporting activity has always been an historical hardship of database administrators and keeping services for both reports and operations from preventing each other’s ability to serve the business.  </p>
<h2><strong>Working with database mirroring</strong></h2>
<p>To go deeper into database mirroring, we will work from a recent setup located at, <a href="/index.php/DataMgmt/DBAdmin/sql-server-2008-mirroring-setup">Mirroring Hands On with Developer Edition</a>.  In this setup, Developer edition is used to provide all of the possible configurations provided in mirroring.  The setup and purchase of Developer Edition is highly recommended to become familiar with these features.</p>
<p>The primary features we lose when moving to Standard Edition are Asynchronous<br />
Mirroring and Snapshot capabilities of the Mirror.  One important factor of snapshot abilities is, the snapshot cannot occur while the mirroring is in a synchronizing state or applying transactions.  In an asynchronous setup (High performance), depending on the number of transactions, this state can be harder to schedule for snapshot creations.  These two features are a large part of mirroring and the flexibility in configuring it.  Weigh in the needs of mirroring greatly and the loss of these features when not putting the budget in for Enterprise Edition.</p>
<p>After going through the developer setup in the link above, we can start to look at options in mirroring and things to watch for.</p>
<h2><strong>Operating Modes</strong></h2>
<p>
Database Mirroring allows for three operating modes.</p>
<ul>
<li>High Availability</li>
<li>High protection</li>
<li>High performance</li>
</ul>
<p>In order to achieve HA with out of the box SQL Server, we enlist in the High Availability operating mode.  This setup runs in a synchronous set of operations that apply transactions to the logs on both the mirror and principal prior to returning success commits back to the applications.  The mirroring landscape consists of three physical servers, the principal, the mirror and the witness.</p>
<blockquote><p><span class="MT_red">Note: The witness can be any edition of SQL Server but the principal and mirror must be the same edition.  The principal can be a previous version than the mirror but this is only recommended in upgrade methods.  The witness can be located on the mirror but this is not recommended due to the chance of losing the mirror and thus, losing the witness.</span></p></blockquote>
<p>The remaining two operating modes take the witness and automatic failover out of the landscape.  This is not optimal in achieving HA.  Without automation and the active ping in mirroring, the downtime is greater and human interaction is required.</p>
<h2><strong>What can go wrong?</strong></h2>
<p>The worst thing that can happen in database mirroring is losing the network behind it.  This can cause a complete loss of data services.<br />
Another known problem is referred to as Split-brain.  In this situation, both the Principal and Mirror have taken on the role of a principal.  In the landscape in which alias’s are utilized this can be a severe problem to accessing the databases.</p>
<p>Corruption (page errors) that occurs on the Principal will follow to the mirror.  Database mirroring does enlist in <i>automatic page-repair</i>.  This is now available in SQL Server 2008 and SQL Server 2008 R2.  The automated page-repair will attempt to repair any page errors that are sent to the mirror by requesting the transactions (or fresh copy of the pages) again from the principal.  This repair will work in some scenarios but if the corruption is too great and requires any type of loss in data, an error will persist.</p>
<p>When a mirror does receive errors (even if capable of fixing), the mirror goes into a suspended state.  This suspended state will persist until the error is resolved.  If the error cannot be resolved by automation of the mirroring abilities, the mirror will stay in the suspended state.  This usually will cause a clean setup of the mirroring landscape.  In a worst case scenario, the errors that came from the principal are great enough that repairing the principal is the priority. </p>
<p>Transaction log growth must be maintained in mirroring.  Database mirroring requires the databases to be in Full recovery.  This means that the transaction logs must be maintained and sized properly.  If they are not, the logs will grow and at some point the need to shrink them will undoubtedly occur.  These operations are not allowed in a mirror though and the mirror will need to be broken to do them.  It is highly recommended to take all the necessary steps to maintain a full recovery state.  Keep this in mind while performing index maintenance especially.</p>
<h2><strong>Final Thoughts</strong></h2>
<p>The combination of Windows Clustering and Database Mirroring will allow for the objective of data services to meet High Availability goals.  With Database Mirroring, we can achieve HA with the operating mode of High Availability while not needing a clustered environment (although recommended).  This landscape lowers cost and allows for flexibility in some applications that may have particular needs outside of the clustering scope.</p>
<p>Achieving High Availability should be seamless to the user community.  Failover situations must be automated in order to retain HA and limited to no exposure to the users at the time a failover is needed.  Working with the applications and vendors to achieve these goals is sometimes required but often fully achievable.</p>
<p>This concludes the SQL University DR/HA Week.  I hope you enjoyed talking about the topics and look forward to hearing all of your feedback on taking what we’ve discussed while planning your own data securing strategies with HA and DR.  As with all of the SQL University weeks, your feedback is greatly appreciated so we know how we are doing.  When you have a moment please take a moment to fill out the <a href="https://spreadsheets.google.com/a/sqlchicken.com/viewform?hl=en&amp;formkey=dDBoSW02QldrTTc2dER3WVZheUlEX3c6MQ#gid=0">SQL University Course Evaluation</a> and select HA/DR Week.  </p>
<p>Thank you for attending once again and thanks to Jorge Segarra (<a href="http://sqlchicken.com/">Blog</a> | <a href="http://twitter.com/sqlchicken">Twitter</a>) for allowing me the pleasure of discussing all of these topics with you.  </p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbprogramming/sql-server-high-availability/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>SQL University &#8211;  Recess time!</title>
		<link>/index.php/datamgmt/dbprogramming/sqlu-taking-a-break-for-recess/</link>
		<comments>/index.php/datamgmt/dbprogramming/sqlu-taking-a-break-for-recess/#respond</comments>
		<pubDate>Thu, 10 Jun 2010 17:10:13 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[disaster recovery]]></category>
		<category><![CDATA[dr]]></category>
		<category><![CDATA[ha]]></category>
		<category><![CDATA[high availability]]></category>
		<category><![CDATA[log shipping]]></category>
		<category><![CDATA[mirroring]]></category>
		<category><![CDATA[restore]]></category>
		<category><![CDATA[sql server 2008]]></category>
		<category><![CDATA[sql university]]></category>

		<guid isPermaLink="false">/index.php/2010/06/sqlu-taking-a-break-for-recess/</guid>
		<description><![CDATA[The recess bell just rang for SQL University HA / DR classrooms.  While all of the SQL kiddies are running around the playground and playing with the things they have learned over this semester, the chalkboard is going to get a workout so when they get back, they can take the notes they slacked on earlier.]]></description>
				<content:encoded><![CDATA[<p>
The recess bell just rang for SQL University HA / DR classrooms.  While all of the SQL kiddies are running around the playground and playing with the things they have learned over this semester, the chalkboard is going to get a workout.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/bart.gif" alt="" title="" width="628" height="339" /></div>
<p align="center"><i>Image courtesy of <a href="http://www.addletters.com/pictures/bart-simpson-generator/233605.htm">Bart Simpson Chalkboard Generator</a> and <a href="http://toadworld.com/BLOGS/tabid/67/EntryID/543/Default.aspx">linkback to Jeff Smith excellent article</a></i></p>
<p>
Over the last week we’ve gone over a lot regarding HA and DR.  The first day we defined situations and the key factors that are needed to be successful in obtaining secure data services and high availability of those data services.  Any of these two strategies to protect our data against disasters, local and remote; always start with the definitions required to plan out the implementation.  We learned together that just throwing things like log shipping into the mix may not truly give us the protection we need.  This would happen if we leave important business entities out of our planning and document how our systems would come back from disasters.
</p>
<p>We done this over the week by showing the features that SQL Server has to offer without much added cost.  When budget is available, we also discussed briefly landscapes like Geo-clustering, SAN replication and truly next to real-time mirrored data centers for recovering in the event of disasters.  These were terms in passing so we didn’t venture off the scope of each class but their importance is there nonetheless. </p>
<p>
We defined a list of the important notes we set off to discuss that effect the decisions of how we can accomplish HA and DR</p>
<ol>
<li>Size of databases</li>
<li>Network capabilities</li>
<li>Budget</li>
<li>Features available per edition of SQL Server</li>
<li>Maintenance load</li>
<li>Allowable downtime</li>
<li>Personnel resources required</li>
<li>Initial setup downtime</li>
<li>Will DR fit into the HA strategy?</li>
<li>Documentation – knowledge transfer</li>
<li>Can we test this?</li>
</ol>
<p>Automation of Disaster and Recovery played an important part in our discussions.  Do we automate is the key to decide in your unique strategies.  DR typical is a manual failover process while HA only becomes HA when automation is playing into the events of a localized disaster.</p>
<p>SQL Server backups were stressed on day two as the foundation of all that is DR (and HA recovery).  With careful planning and testing on both HA and DR, we achieve secure and always available data services.  But when even that strategy fails, we must turn to our backups to recover both the data and the DR and HA strategies themselves.  </p>
<p>Today we discussed log shipping as a cost effective disaster and recovery method.  Log shipping is a great alternative when budgets are low.  Problems do arise in the effectiveness when our data services are larger than our check books though.  All-in-all, log shipping can provide security in a DR situation and leaving it as a possibility when strategizing is always good. </p>
<p>So now we move to our last day and we will discuss High Availability.  The class will show database mirroring in our newer versions of SQL Server.  We’ll discuss things like split-brain scenarios in mirroring and certificate usage so we can mirror across different domains.  Finally, we need to briefly discuss SAN replication and geo-clustering again.  Although we will be brief on this topic, we want to emphasize them because when the budget is there, nothing really matches them for real-world HA.</p>
<p><strong>Recess is now over so let’s get back to work!</strong></p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbprogramming/sqlu-taking-a-break-for-recess/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Log Shipping for cheap DR</title>
		<link>/index.php/datamgmt/dbprogramming/log-ship-to-dr-sqlu/</link>
		<comments>/index.php/datamgmt/dbprogramming/log-ship-to-dr-sqlu/#comments</comments>
		<pubDate>Thu, 10 Jun 2010 10:50:34 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[disaster recovery]]></category>
		<category><![CDATA[dr]]></category>
		<category><![CDATA[ha]]></category>
		<category><![CDATA[high availability]]></category>
		<category><![CDATA[log shipping]]></category>
		<category><![CDATA[mirroring]]></category>
		<category><![CDATA[restore]]></category>
		<category><![CDATA[sql server 2008]]></category>

		<guid isPermaLink="false">/index.php/2010/06/log-ship-to-dr-sqlu/</guid>
		<description><![CDATA[Welcome to day three of HA and DR week of SQL University.  Today we are going to look at cheap DR.  Yes, setting up DR can be inexpensive.  The best part of this strategy is it comes along with most of the editions of SQL Server.  The method is Log Shipping.  Log shipping (LS) has a bad name in the Disaster / Recovery (DR) world.  There are concerns with the ability to fail back to primary sites in the case of disasters, and LS is often thought of as a maintenance intense setup along with file mess.  Today’s class will go over some methods to handle these and other concerns, along with the simplicity of configuring and monitoring LS in SQL Server 2008 (R2).]]></description>
				<content:encoded><![CDATA[<p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/sqlu_logo.gif" alt="" title="" width="150" height="166" align="left" /></div>
<p>Welcome to day three of HA and DR week of SQL University.  Today we are going to look at cheap DR.  Yes, setting up DR can be inexpensive.  The best part of this strategy is it comes along with most of the editions of SQL Server.  The method is Log Shipping.  </p>
<p>Log shipping (LS) has a bad name in the Disaster / Recovery (DR) world.  There are concerns with the ability to fail back to primary sites in the case of disasters, and LS is often thought of as a maintenance intense setup along with file mess.  Today’s class will go over some methods to handle these and other concerns, along with the simplicity of configuring and monitoring LS in SQL Server 2008 (R2).
</p>
<p></p>
<h2><strong>What is Log Shipping</strong></h2>
<p>
Log Shipping consists of three events.  Backup transaction log, Copy remotely and Restore to subscriber(s).  Any one primary (publisher or the logs) can have one or more secondary databases (subscriber to the logs).  </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_7.gif" alt="" title="" width="500" height="444" /></div>
<p>When configuring Log Shipping, all of the configuration settings are held in the MSDB database. </p>
<p>These tables consist off the following</p>
<blockquote><p>log_shipping_primary_databases<br />
log_shipping_primary_secondaries<br />
log_shipping_monitor_primary<br />
log_shipping_monitor_history_detail<br />
log_shipping_monitor_error_detail<br />
log_shipping_secondary<br />
log_shipping_secondary_databases<br />
log_shipping_monitor_secondary</p></blockquote>
<p>The system procedures for configuring log shipping are located in the master database.</p>
<blockquote><p>sp_add_log_shipping_monitor_jobs<br />
sp_add_log_shipping_primary<br />
sp_add_log_shipping_secondary<br />
sp_create_log_shipping_monitor_account<br />
sp_delete_log_shipping_monitor_info<br />
sp_delete_log_shipping_monitor_jobs<br />
sp_delete_log_shipping_primary<br />
sp_delete_log_shipping_secondary<br />
sp_get_log_shipping_monitor_info<br />
sp_log_shipping_get_date_from_file<br />
sp_log_shipping_in_sync<br />
sp_log_shipping_monitor_backup<br />
sp_log_shipping_monitor_restore<br />
sp_remove_log_shipping_monitor_account<br />
sp_update_log_shipping_monitor_info</p></blockquote>
<p>
When Log Shipping is enabled and configured, all subscribers must be in either Recovering or Read-Only (Standby) status.  Placing a subscriber into a Read-Only mode is common, but when a log is applied to the subscriber, all connections must be closed.  This is due to the subscriber database being required to go into recovering so the log can be applied.  Often, subscribers are used for reporting so considering the state of connectivity to the database is critical in being effective for availability.</p>
<p>Log shipping has some definite advantages on its side for being used in DR.  The overhead of the processing (backup, copy and restore) can be managed and, if thought through well, can leave a very small footprint on the normal operations of the database servers.  Maintenance is very minimal for both setup and administration.  Most database administrators already work closely with backup and restore, so log shipping is easy to learn.  The major cost in log shipping is disk for storage, a secondary SQL Server and the network backbone to handle the files moving across the lines.  This, compared to some DR cost overhead, is very minimal.
</p>
<p>
In the following steps we will set up log shipping completely on a local default SQL Server instance.  </p>
<blockquote><p><span class="MT_red">Note: as always, database size is a factor.  Databases in the Terybyte range can easily be logged shipped given the resources behind it.  Steps that are outlined will change greatly in time of execution due to the size of a database.  An initial restore for one will take some time at first.</span>  </p></blockquote>
<h2><strong>Setting up Log Shipping with SSMS</strong></h2>
<p>
A few things should be prepared prior to configuring Log Shipping. </p>
<ul>
<li>Share location for the log backups on the publisher</li>
<li>Share location for the log backups to be copied to on the subscribers</li>
<li>Security setup for accessing these shares (Agent account by default)</li>
</ul>
<p>Log Shipping can be setup completely using T-SQL, but in the push for the “point, click and run” theory of SSMS, we will use it to setup, configure and get our lab running.</p>
<p>Log shipping is available in every edition of SQL Server but Express.  In order to show our setup we will be using Developer Edition.  Developer Edition is available for a very low cost and is identical in features to Enterprise.  We will be using the AdventureWorks database to setup Log Shipping.  If you do not have this database, you can download it here.
</p>
<ol>
<li>Open SSMS and connect to your SQL Server.</li>
<li>Expand the databases tree, right click AdventureWorks and select properties</li>
<li>Select Transaction Log Shipping on the right</li>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_1.gif" alt="" title="" width="628" height="561" /></div>
<p>By default, Log Shipping is disabled on each database.  In order to go further, we need to check “Enable this as a primary database in a log shipping configuration”.  This will set the transaction log backups available for us to open and configure.</p>
<blockquote><p><span class="MT_red">Note: If you have other transaction log backups running, they should be turned off prior to starting the new Log Shipping plans.</span></p></blockquote>
<li>Click the &#8220;Backup Settings&#8221; button to open the configuration wizard.</li>
<p>Earlier we mentioned preparing for Log Shipping and the shares required.  You can use admin shares (e.g. \onpnt_xpsd$) but this isn’t recommended as the admin shares should be for administrative purposes only.  For our setup we will be using the following for processing backups:</p>
<p>\onpnt_xpspub_logs<br />
\onpnt_xpssub_logs</p>
<li>Enter your share into the “Network path to backup folder” field</li>
<p>For now, we will leave the default 72 hours to retain log backups.  </p>
<blockquote><p><span class="MT_red">Note: the retention of the log backups must be taken into consideration for recovery from backups.  Take into consideration the retention of other Full and Differential backups when setting this removal option.  You do not want to delete log backups that could be required to recover to a point in time by restores.</span></p></blockquote>
<p>The log shipping configuration will add a SQL Server Agent job that will monitor the thresholds set when configuring them.  If the table log_shipping_monitor_primary shows a backup date greater than the backup thresh hold value, an error will be raised in SQL Server.   In order to actively be notified, operators need to be setup on the agent and the job so you will receive these errors as they occur. </p>
<p>I would not recommend leaving the default Job name.  Make use of this name so you can easily find the job and know what it is for.  If you work on SQL Servers that have hundreds of jobs or Job Servers, using meaningful names in your environment make maintenance much faster and easier on everyone.</p>
<p>The next step is to determine the interval of the log backups.  The default 15 minutes is common but in a high-transaction database, 15 minutes can mean severe loss of data in a disaster.  I recommend really putting some thought into this setting.  Ensure you do not hurt performance with backups tripping over themselves or occurring so often they cause problems.  At the same time, ensure that you are protecting against the least amount of loss the business will accept.</p>
<p>Leave the default 15 minutes for now.  If you want to alter this schedule, click the Schedule button and the SQL Agent scheduling window will come up.  </p>
<p>To show compression, next to &#8220;Set backup compression&#8221;, select Compress Backup.  If you are on an earlier version than 2008 R2 and any edition other than Enterprise or Developer, this option will not be available.  SQL Server 2008 R2 allows compression in Standard and Enterprise (including Developer).  Pre-R2, only Enterprise and Developer have this option. </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_2.gif" alt="" title="" width="628" height="646" /></div>
<li>Click OK after ensuring everything is completed as shown above.</li>
<p>The next step in the process is to set any subscribers and monitoring servers if you use them outside the publisher.  In preparation you can restore a full backup and bring the tail log into the AdventureWorks subscriber database.  This can also be done from the subscriber steps.</p>
<li>Click Add under the Secondary databases</li>
<li>Click Connect and connect to the instance you want to Log Ship to</li>
<li>On the Initialize Secondary Database tab, select &#8220;Yes, generate a full backup of the primary…&#8221;</li>
<p>This will back the AdventureWorks database up and restore it as the database we specify to be the subscriber.   Ensure if you do this lab on a single SQL Server to change the name of the Secondary database to something other than the default of the primary.  </p>
<li>Click the restore options and ensure the data and log files go into the correct directories per your disk configurations. </li>
<li>Select the Copy Files tab and enter the share we created earlier (\onpnt_xpssub_logs)</li>
<p>We can leave the default schedule to restore again, but I still recommend changing the job name to a something more meaningful and easy to read </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_3.gif" alt="" title="" width="628" height="543" /></div>
<li>Click the Restore Transaction Log tab and select Standby Mode and Disconnect user in the database when restoring backups.  This will be required to prevent restore problems.</li>
<li>Click OK and OK again to save all of our configurations.  </li>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_4.gif" alt="" title="" width="628" height="551" /></div>
<p>After clicking OK, a dialog will be shown while the backup and restore of AdventureWorks runs.  The SQL Agent jobs that will control log shipping will also be created after these steps succeed. </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_5.gif" alt="" title="" width="628" height="334" /></div>
<p>Once the restore is done and logs have shipped, you will start to notice them moving in the publication and subscriber shares</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_6.gif" alt="" title="" width="628" height="282" /></div>
</ol>
<p>Now that LS is running we can look into the process and logging of the events.  The log_shipping_monitor_history table is extremely useful for validating the entire process between the instances.  The Message column has logged information that will explain in detail the process that is occurring:</p>
<pre>select [message] from log_shipping_monitor_history_detail</pre>
<p>Results:</p>
<blockquote><p>Starting transaction log copy. Secondary ID: &#8217;07af9839-f962-40db-9392-2107e9cc2053&#8242;<br />
Retrieving copy settings. Secondary ID: &#8217;07af9839-f962-40db-9392-2107e9cc2053&#8242;<br />
Retrieved copy settings. Primary Server: &#8216;ONPNT_XPS&#8217;, Primary Database: &#8216;AdventureWorks&#8217;, Backup Source Directory: &#8216;\onpnt_xpspub_logs&#8217;, Backup Destination Directory: &#8216;\onpnt_xpssub_logs&#8217;, Last Copied File: &#8216;<none>&#8216;<br />
Copying log backup files. Primary Server: &#8216;ONPNT_XPS&#8217;, Primary Database: &#8216;AdventureWorks&#8217;, Backup Source Directory: &#8216;\onpnt_xpspub_logs&#8217;, Backup Destination Directory: &#8216;\onpnt_xpssub_logs&#8217;<br />
Checking to see if any previously copied log backup files that are required by the restore operation are missing. Secondary ID: &#8217;07af9839-f962-40db-9392-2107e9cc2053&#8242;<br />
The copy operation was successful. Secondary ID: &#8217;07af9839-f962-40db-9392-2107e9cc2053&#8242;, Number of log backup files copied: 0<br />
Starting transaction log copy. Secondary ID: &#8217;07af9839-f962-40db-9392-2107e9cc2053&#8242;</none></p></blockquote>
<p>Another extremely practical usage of these tables is, in the event of a disaster, being able to later analyze data that may have been lost in log backups that did not get copied to secondary servers.  The log_shipping_secondary has a column, &#8220;last_copied_file&#8221;.  This column has helped me determine exactly where a subscribing database is at in the restores several times in the past.    </p>
<p>SSMS and built in reporting already available also provides us with a great way to monitor conditions of log shipping.  Right click the database server in SSMS, scroll to reports and in standard reports, click the Transaction Log Shipping Status.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_9.gif" alt="" title="" width="500" height="406" /></div>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_10.gif" alt="" title="" width="1007" height="270" /></div>
</p>
<h2><strong>Catches but not pitfalls</strong></h2>
<p>
Now that log shipping is setup we should discuss some pitfalls to watch for.</p>
<p>One big problem that comes up with any fully logged database is maintenance tasks.  Index maintenance is a prime example.  The growth in logging on the transaction log that happens from index maintenance while in Full recovery is large.  Once this maintenance and logging begins, the log grows and this means the backups grow as they do.  Coming up with the best time to do these types of maintenance tasks and the interval in which you should log ship the transaction logs is critical for this.  If you have a 10GB log file and you rebuild a 5GB Index, it will take the log space to do the rebuild.  This could cause growth in the log which is something we don’t really want happening a lot.  So if the scheduled log backup is set to keep the free space down in the log during these tasks and normal operations, you can manage the logs very well and keep them in check while performing to the best they can.</p>
<p>
Networks will need to be able to handle the files moving.  Imagine if you start to copy a 10GB file over the regular LAN that users are connected to and working on.  This happens in regular offices often.  Users copy large files down or up to user home directories and it slows the entire network.  The only way to clean it up is to stop the copy or wait for it to finish.  This can be the same problem if the network isn’t configured to handle it.  Meet with the network administrators and make sure everyone knows the traffic that will be added to the network. </p>
<p>
With any database that is shipped, mirrored or restored to another site, remember that logins, SQL Agent Jobs, configurations outside of the databases and objects such as endpoints or linked servers will not be sent.  These must be done outside of the normal tasks.  SSIS can assist in this with the use of SMO or the Transfer Server Objects Task.  PowerShell can also bring in a useful container to work these tasks on schedules.  It is a good idea to move these as SQL script files to the offsite server and apply them.  Test and test this often.</p>
<h2><strong>Bell rang!</strong></h2>
<p>Log shipping is a quick and great method for small to mid-size databases or databases that have a good foundation and planned strategy for dealing with the pitfalls of large logs.  Administration is light and monitoring is very well done and built into SQL Server for use.  There are added benefits of LSN tracking in the logs and file monitoring as well.  The log shipping system tables have a wealth of information in them that can be used for other tasks.  </p>
<p>When planning your own DR strategy, leave log shipping on the list of options.  The cost factor and resource utilization may have it in the lead for being a good choice for your safety measures with Disaster / Recovery.  </p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbprogramming/log-ship-to-dr-sqlu/feed/</wfw:commentRss>
		<slash:comments>6</slash:comments>
		</item>
		<item>
		<title>The SQL Server backup &#8211; foundation of any Disaster / Recovery</title>
		<link>/index.php/datamgmt/dbprogramming/the-sql-server-backup-foundation-of-any/</link>
		<comments>/index.php/datamgmt/dbprogramming/the-sql-server-backup-foundation-of-any/#comments</comments>
		<pubDate>Tue, 08 Jun 2010 08:39:36 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[disaster recovery]]></category>
		<category><![CDATA[dr]]></category>
		<category><![CDATA[ha]]></category>
		<category><![CDATA[high availability]]></category>
		<category><![CDATA[mirroring]]></category>
		<category><![CDATA[restore]]></category>
		<category><![CDATA[sql server 2008]]></category>

		<guid isPermaLink="false">/index.php/2010/06/the-sql-server-backup-foundation-of-any/</guid>
		<description><![CDATA[Welcome to our second class of HA and DR week of SQL University.  Today we are going to focus on the concept, “Backups are for sissies!”  OK, we’re really going to look at backup and restore for Disaster / Recovery (DR) and how being a sissy and always backing up our databases and testing out restores is a proven strategy for DR.   When all else fails and the walls are falling down on the database servers, backups will be your life preserver.  Backups are the foundation for Disaster and Recovery (DR).  Backups can also save you when high Availability (HA) completely fails you.   Let’s get started!]]></description>
				<content:encoded><![CDATA[<p><div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/sqlu_logo.gif" alt="" title="" width="150" height="166" align="left" /></div>
<p>Welcome to our second class for HA and DR week of <a href="http://sqlchicken.com/sql-university/">SQL University</a>.  Yesterday we went through defining HA and DR along with some common practices you can use.  Today we are going to focus on the concept, &#8220;Backups are for sissies!&#8221;  OK, we’re really going to look at backup and restore for Disaster / Recovery (DR) and how being a sissy and always backing up our databases and testing out restores is a proven strategy for DR.   </p>
<p>When all else fails and the walls are falling down on the database servers, backups will be your life preserver.  Backups are the foundation for Disaster and Recovery (DR).  Backups can also save you when high Availability (HA) completely fails you.   Let’s get started!</p>
<p></p>
<h2>Define a backup</h2>
<p>A full database backup contains a full representation of the database <a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/transaction-log-size-cold-shrink-ldf">with enough of the transaction log in order to recover</a> everything when a restore is performed.  In SQL Server, we have several types of backups to help with various configurations and sizes of our databases.</p>
<p><strong>Full Backup</strong> – Defined above as a full representation of the entire database and enough of the log to recover the point in which the backup was executed</p>
<p><strong>Differential Backup</strong> – Differential backups add benefits to backup strategies by giving you a quicker recovery path.  These backups contain the changes between the previous full backup and when the differential was executed.  If you are recovering from a disaster and have a database in Full Recovery Model with transaction log backups, you are not required to restore the base, the differentials, and all the transaction log backups.  Only the base and the differential are required to bring you to the point in time of the differential backup.  </p>
<p><strong>Transaction Log Backup</strong> – This type of backup contains the transactions since the last backup.  This only applies to databases in Full or Bulk-Logged Recovery Model.  For recovery to point-in-time, transaction log backups are needed, as well as required when in Full recovery to maintain the log files.  By default, the Model database is set to Full recovery model which dictates how new databases are created.  This means that any database you create without specifying default properties will adopt the Model’s recovery model.  A common practice is to alter the Model database and set the recovery model to Simple.  This will prevent out-of-control log growth when these types of backups are not put in place. </p>
<p><strong>Partial Backup</strong> – Partial backups are primarily used when all you want is in the primary filegroup of a database.  This leaves all the other filegroups out of the backup and reduces the size of the overall backup file.  </p>
<p><strong>File backups</strong> &#8211; File backups help with large databases and recovering only portions of the database. They are exactly what they are named: a method for backing up a file or filegroup in a SQL Server database. Given that some databases exceed the Terabyte size, a File backup can be very useful in protecting critical tables as one chunk in a File backup. They also can be used in ETL operations for a quick backup strategy and rollback point while not needing to recover an entire database if corruption occurs.</p>
<h2>Last Resort Recovery</h2>
<p>While taking backups into consideration in DR strategies, there is one primary goal: get them offsite. Database backups will not help a DBA recover if they are located on the same systems and in the same location as the primary databases. In order for backups to become a DR strategy, we need to get them offsite. This can include another location that only has one server powering a disk array, or a complete mirror of the data center itself. The backups will not help us if </p>
<ol>
<li>They are not run</li>
<li>They are in the same site as the disaster</li>
</ol>
<p>Although backups are the foundation of DR, they are a slow method of restoring the databases. For Terabyte databases, a backup and restore can take several hours.  This downtime is money lost. Given the restore times, backups should complement DR with all the other strategies put in place to recover. Backups are the almighty in terms recovering when all else fails.     </p>
<h2>Location of backups</h2>
<p>
The location of the backups is important in order to make them helpful in DR. The backups will always be required to be offsite from the primary locations, without exception. This includes all of the sites and their own unique databases. This means that each location can become a DR site for others for retaining the backup files offsite. </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/back_rec_diagram.gif" alt="" title="" width="826" height="352" /></div>
<p>In the diagram above, database servers A and B on Site 1 rely on the offsite backups on Site 2 for recovery. Site 2, however, has a local application and database server C that is unique to the facility. Having this database exist at Site 1 would be not utilizing resources well and could cause performance problems for the application in Site 2. In order to bring server C into the DR strategy, the database backups are sent offsite to Site 1.</p>
<p>Getting backups offsite can consist of a few methods.  </p>
<ol>
<li>Backup to tape and physically ship them offsite</li>
<li>Backup to repositories and move them on designated network lines or off hours</li>
<li>Backup directly to the offsite via UNC paths</li>
</ol>
<p>Backing up to tape has been a common practice since the mainframe heydays.  It is cost effective and with newer tape abilities, multi-Terabyte single tapes can hold and retain data with expected shelf life of 50 to 100 years.   <a href="http://en.wikipedia.org/wiki/Linear_Tape-Open">LTO</a> truly has come a long way.  </p>
<h2>Getting our backup file with BACKUP</h2>
<p>
The <a href="http://technet.microsoft.com/en-us/library/ms186865.aspx">BACKUP DATABASE</a> statement can be daunting at first glance so can the SSMS wizard and options.  </p>
<p>Example:  Create a database DBA and we will run a full backup on it</p>
<pre>CREATE DATABASE TEST_DR_BACKUP 
GO
ALTER DATABASE TEST_DR_BACKUP SET RECOVERY FULL
GO</pre>
<p>Now that we have a database to backup, let&#8217;s execute a typical full backup statement</p>
<pre>BACKUP DATABASE TEST_DR_BACKUP
TO DISK = N'C:TEST_DR_BACKUP.BAK'
GO</pre>
<p>This gives us a backup file in the C drive of TEST_DR_BACKUP.BAK.  The backup contains everything we need to recover the database as is.  If you notice, this backup statement was pretty quick.  If we add CHECKSUM and the COPY_ONLY option to this statement, the execution time will be slightly longer.</p>
<pre>BACKUP DATABASE TEST_DR_BACKUP
TO DISK = N'C:TEST_DR_BACKUP.BAK'
WITH CHECKSUM,COPY_ONLY
GO</pre>
<p>Looking into the statement, we have several options to make our backups, &#8220;smart&#8221;.  One method that is extremely useful in DR is the COPY_ONLY option.  By using the COPY_ONLY option, we can create full backups of a database without affecting the LSN order in other backup strategies.  This is a powerful option given the need to get backups offsite while we are using backup strategies locally for other things.  </p>
<p>Crude but effective copy methods with batch files, .NET development and even manual efforts can also be used for moving the existing backup files.  Automating these tasks is always a key operation to put into place.  Backup/Restore operations take time and the time of a DBA is expensive when you consider all the tasks we need to cover in our day-to-day operations.  Later, we will go over a proven automated strategy and feature in SQL Server.
</p>
<h2>Actually making a backup</h2>
<p>
Running backups can be as simple or complex as you want it to be.  In order to accomplish backup/restore as a recovery method, all that is needed is disk space or tape resources.  In some cases a lot of disk space will be required, but compression can bring the cost of these requirements down to a level that is much more manageable.  </p>
<p>We also have options that can assist in preventing future problems by allowing DBAs to be proactive.  CHECKSUM option is one extremely useful option but has its own overhead on the backup executions.  CHECKSUM will allow you to detect media issues while performing the backup.  This will completely verify each page and detect if the page is torn.  Using CHECKSUM alone will cause the backup to fail and log an error of the media problems.  In order to prevent the error from stopping the backup operation, we can use the CONTINUE_AFTER_ERROR option.  This would allow the DBA to attempt a restore of the database to further detect the extent of the page errors along with compile a strategy for repairing them in the originating database.
</p>
<p>
The second statement that can be used is the RESTORE VERIFYONLY statement.  This can be done after the backup has run completely, and will verify the page checksums without actually restoring the database itself.  It is important to understand that this is useful information but not a replacement for a complete restore test.  </p>
<p>Example: We have a Full backup located at C:sql_full_backupdbadba_full_20100606.bak</p>
<p>We could test this backup set by issuing the follow RESTORE VERIFYONLY statement</p>
<pre>RESTORE VERIFYONLY
FROM DISK = N'C:sql_full_backupdbadba_full_20100606.bak'
GO </pre>
<p>Resulting in the following information if errors are not found</p>
<p><span class="MT_smaller">The backup set on file 1 is valid.</span></p>
<p>In case of a torn page, the information would be written along with the page location.  This would allow us to go farther into the problem and formulate the repair steps.
</p>
<h2>Restore is part of the backup</h2>
<p>
For backups in DR to be useful, we must test them by restoring them on a consistent schedule.   Every backup has differences as does every database.  Those differences are the state of the data and the state of the hardware when the backup was taken.  Hardware problems can cause torn pages in a database and these issues will follow through to the backups.  If a backup was completely successful and things like CHECKSUM (defined earlier) are not used to log errors, a backup can possibly fail to restore successfully and more importantly, become useless in a recovery plan.  Best practice would be to restore every Full Backup and test the recovering levels of differentials and log backups in a Full Recovery model.  </p>
<p>Automating these restore tests can help the process greatly.  SQL Server Integration Services has the facilities to do this for you.  Let’s look at a method with SSIS that can accomplish automating the restore process and reporting.  </p>
<blockquote><p>Note: large databases will add complexity and even inabilities to use these methods.  File backups and other means that prevent automation will apply but should not forego testing by restoring.</p></blockquote>
<p>Finding backups could be accomplished dynamically with expressions and variables.  For example, the backup file naming convention could us YYYYMMDD designating the day the backup was run.  A variable expression is used to find the specific file we want as</p>
<pre>"C:\sql_full_backup\dba\dba_full_" +
(DT_STR, 4, 1252)DATEPART("yyyy", @[System::ContainerStartTime]) + 
RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mm", @[System::ContainerStartTime]), 2) + 
RIGHT("0" + (DT_STR, 2, 1252)DATEPART("dd", @[System::ContainerStartTime]), 2) + ".bak"</pre>
<p>This can then be added to a File System Task for Copy as the source (as well as used in destinations). </p>
<p>This package can then be run on the same day as the Full backup to find the correct file to perform our tasks on.  </p>
<p>We can construct our package to perform the copy, restore the database and notify us of a successful restore or failures along the way.  After this is completed, we can also run ALTER statements on the restore location in order to put it into a state it will not cause problems (i.e.: log growth)  or run other application specific needs that may be part of our DR solution.  </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/ssis_restoring.gif" alt="" title="" width="298" height="286" align="center" /></div>
<p>Running this process weekly (or on the Full backup day) can show problems that may otherwise not be found.  This will prevent the day when the backups are called into action for recovery when all other DR strategies have failed.</p>
<h2>Closing with homework to read and practice</h2>
<p>Given the abilities in the features available, the tasks of having backups offsite for DR are simplified greatly.  Backups are a cost effective and sound method for DR and HA recovery but only when tested out completely.  </p>
<p>Make backups, make them often and test them by restoring them often.  Lastly, get them offsite even if that means paying for the storage of tapes.  </p>
<p>Going farther:<br />
<a href="http://technet.microsoft.com/en-us/library/ms175477.aspx">Technet, Backup Overview</a></p>
<p><a href="http://msdn.microsoft.com/en-us/library/ms186865.aspx">BOL, BACKUP (Transact-SQL)</a></p>
<p><a href="http://www.sqlskills.com/BLOGS/PAUL/post/Sample-corrupt-databases-to-play-with.aspx">Paul Randal’s corrupt database for testing</a></p>
<blockquote><p>Why did I link to Paul&#8217;s corrupt database?  This is a valuable resource to test on.  Through backup and restore and through all of this week of blogs, use this database to really see if you can recover using a DR or HA strategy. </p></blockquote>
<p>
If you liked this SQL University post, please take a moment to fill out the <a href="https://spreadsheets.google.com/a/sqlchicken.com/viewform?hl=en&amp;formkey=dDBoSW02QldrTTc2dER3WVZheUlEX3c6MQ#gid=0">SQL University Course Evaluation</a> and select HA/DR Week.  Thank you!<br />
<a href="/index.php/DataMgmt/DBProgramming/MSSQLServer/sql-university-and-why-you-should-be-att">SQL University and why you should be attending</a></p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbprogramming/the-sql-server-backup-foundation-of-any/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
	</channel>
</rss>
