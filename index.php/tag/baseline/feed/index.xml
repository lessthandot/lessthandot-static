<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>baseline &#8211; LessthanDot</title>
	<atom:link href="/index.php/tag/baseline/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>A Technical Community for IT Professionals</description>
	<lastBuildDate>Sat, 09 Mar 2019 12:50:36 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.6.1</generator>
	<item>
		<title>The Lazy DBA Series: Monitoring</title>
		<link>/index.php/datamgmt/datadesign/lazy-dba-monitoring-sql-server/</link>
		<comments>/index.php/datamgmt/datadesign/lazy-dba-monitoring-sql-server/#respond</comments>
		<pubDate>Sun, 22 Aug 2010 22:33:11 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[baseline]]></category>
		<category><![CDATA[monitoring]]></category>
		<category><![CDATA[performance]]></category>
		<category><![CDATA[red gate]]></category>
		<category><![CDATA[reporting]]></category>
		<category><![CDATA[sql response]]></category>
		<category><![CDATA[sql server 2008]]></category>

		<guid isPermaLink="false">/index.php/2010/08/lazy-dba-monitoring-sql-server/</guid>
		<description><![CDATA[There are hundreds if not thousands of T-SQL Scripts, Powershell Cmdlets, .NET programs and even old vbscript scripts and on out there to monitor SQL Server and Operating System level performance.  I’ve written hundreds of them myself over the years.  All of that time and work absolutely paid off.  I know a lot about monitoring, going after the right information when something specific is wrong.  It also gives me a bag of tricks that mostly causes laptops to run out of disk space often.  Disk is cheap though, these days.]]></description>
				<content:encoded><![CDATA[<p>There are hundreds if not thousands of T-SQL Scripts, Powershell Cmdlets, .NET programs and even old vbscript scripts out there to monitor SQL Server and Operating System level performance.  I&#8217;ve written hundreds of them myself over the years.  Without a doubt, all of that time and work absolutely paid off.  It helped me learn even more about monitoring the right things and going after the right information when something specific is wrong.  It also gives me a bag of tricks (folder named, BagOfTricks) that mostly causes laptops to run out of disk space often.  Disk is cheap these days, right?!</p>
<p>Years ago I realized that writing all those scripts and running them in SSIS, SQL Agent and such was good, but was I being efficient?  Yes, I found that being lazy was far out of reach with these methods. Even making those scripts and programs as dynamic and mobile as they could be, there was always the fact that I had to write and update them along with implement them with in some cases, the same amount of work.  Not the most efficient way to spend time as a DBA.  I mean, business wants you to work for them, not for you.  As sad as that is, it often is the fact.  In all, writing scripts is a must and you need to in order to learn internals and become more familiar with the architecture of SQL Server and the Operating Systems they are running on.  I will always recommend that and real time troubleshooting with all of indicators we retrieve in these ways can never be replaced with anything else.  What about the return of investment to the company for overall monitoring?  Beyond real time troubleshooting when a problem is found?</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba.gif" alt="" title="" width="378" height="378" /></div>
<p><strong>Monitoring by Next, Next, Finish</strong></p>
<p>Recently I was honored to be accepted into the <a href="http://www.red-gate.com/about/community_relations/friends_of_RG.htm">Friends of Red Gate Program</a>.   Red Gate has always been known for being part in the community and giving a lot (and I mean, a lot!) back to us.  Having the opportunity to work with them is truly exciting.  While being a member of this program, I’ve had the chance to run some Red Gate tools that I previously had not been able to.  One of them was, SQL Response.   For this Lazy DBA topic, we are going to use <a href="http://www.red-gate.com/products/SQL_Response/index.htm">SQL Response</a> to discuss monitoring and using your time more efficiently.  First, let’s go over SQL Response briefly to set the stage for why it provides us with more efficient monitoring.</p>
<p>I put SQL Response under the microscope and looked at everything it was doing, reading, using to do so and the impact had on my test database server.  The impact was very low (my word) and installation quick and painless.  In fact, SQL Response seems to use nothing more but what it has at its fingertips to use from SQL Server.  That being normal traces, DMVs and system tables/views.  These all are used to gather what it needs to successfully tell you, &#8220;There is a problem&#8221;.  As with any great monitoring tool, notifications by email are available in SQL Response.</p>
<p>Most of the batches that SQL Response sent to the DB Server were clean and contained statements I think all of us rely on daily.   Things such as the normal check for Agent status, Query Plan Cache and OS performance from the dm_os list of DMV’s.  Very well formed as well.  </p>
<p>The meat of SQL Response is there and is just enough to get the job done.  I’m the type of DBA that likes thin, easy to read and give me what I want, tools.  Leaving as little to no impact on the database servers while it does what I need.  This one does that by not adding a lot of &#8220;fluff&#8221; for the graphics and simply does the task is was handed.   Of course, everyone wants their reports and there needs some help in that area.  When it comes down to it, my worry is telling me when a problem exists before I have to deal with screaming users.  All of that without spending a half hour determining which animated gif is telling me know I have a CPU problem.</p>
<p>The following is a snapshot of SQL Response showing my horribly starving disk on my laptop.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba_monitor.gif" alt="" title="" width="628" height="240" /></div>
<p>There really isn’t much more to it than a clean and straight forward interface.  I like the fact that there is a recommendations section also.  This appears to be recommendations that are clear and precise. Most that I forced errors on the test database server showed recommendations that I would recommend as well.  That alone shows great thought was put into this good feature.</p>
<p>So where is all of this going?  Being efficient in monitoring is as important as the last <a href="/index.php/DataMgmt/DBAdmin/lazy-dba-sql-trace">topic</a> we discussed.  Setting up custom monitoring on your own takes a lot of time and work and while that work is being done, your database server may be in trouble.  Enlisting the vendors that supply these monitoring tools and put extreme effort towards making them work for your needs is a way to put automation, real-time notifications and limited impacting tools in place quickly and safely.</p>
<p><strong>Final Note</strong></p>
<p>There never really is a last note about monitoring tools.  You still have to put work into deciding which one is best for you database servers and your unique methods of confronting the day as a DBA.  Choose them wisely and don’t throw something on top of a production database server without knowing exactly what it does, how it does it and if it itself, will cause unwanted problems with performance.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/lazy-dba-monitoring-sql-server/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Creating a baseline for SQL Server</title>
		<link>/index.php/datamgmt/datadesign/sql-server-baseline-creation/</link>
		<comments>/index.php/datamgmt/datadesign/sql-server-baseline-creation/#comments</comments>
		<pubDate>Mon, 09 Aug 2010 09:50:25 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[baseline]]></category>
		<category><![CDATA[monitoring]]></category>
		<category><![CDATA[performance]]></category>
		<category><![CDATA[sql server]]></category>
		<category><![CDATA[sql server 2008]]></category>

		<guid isPermaLink="false">/index.php/2010/08/sql-server-baseline-creation/</guid>
		<description><![CDATA[In the last post, "Baseline, Performance Reporting and being a proactive DBA" touched on baselines and using them to set thresholds for actively monitoring performance problems on SQL Server.  From that post we briefly discussed that every database server is unique.  That even being true when the databases we attach to SQL Server are packaged installations from third party applications (like SAP, Dynamics etc...).  I received feedback from my good friend Aaron Lowe (Twitter &#124; Blog) on this topic and a very good conversation on how we create these baselines.  Aaron had a great point regarding there not being much out there on exactly how to do this so I thought I’d write up a follow-up to the article and some points you can use to create your own baselines.]]></description>
				<content:encoded><![CDATA[<p>In my last post, &#8220;<a href="/index.php/DataMgmt/DBAdmin/sql-server-baseline">Baseline, Performance Reporting and being a proactive DBA</a>&#8220;, I touched on baselines and using them to set thresholds for actively monitoring performance problems on SQL Server.  I briefly discussed that every database server is unique when the databases we attach to SQL Server are packaged installations from third party applications (like SAP, Dynamics etc&#8230;).  I received feedback from my good friend Aaron Lowe (<a href="http://twitter.com/Vendoran">Twitter</a> | <a href="http://www.aaronlowe.net/">Blog</a>) on this topic and had a very good conversation on how we create these baselines.  Aaron had a great point regarding there not being much out there on exactly how to do this so I thought I’d write up a follow-up to the article and some points you can use to create your own baselines.</p>
<p><strong>So why aren&#8217;t there a ton of posts on creating baselines?</strong></p>
<p>Here are my two bits on the topic.  Creating a baseline is almost as unique as every business and processing model.  In saying that, creating a baseline and what you capture will vary slightly.  A lot of variables like the hardware you’ve purchased, the storage type, the server technology, OS versions/editions and even SQL Server editions affect the process.  Even with these variables and the unique nature of business, we can set some guidelines of where to start and how to make our baseline. </p>
<p><strong>Baseline creation basics</strong></p>
<p>The most important thing to understand when creating a baseline is, the collection of the baseline itself will affect performance of the SQL Server and Operating System.  Given that factor, take into account that using tools to collect how the resources are being utilized will affect the overall outcome.</p>
<p>Another important factor to think about; baseline creation is not tuning time.  Your tuning should be done as its own major task.  It should also be done prior to baseline creations.  If you create a baseline based on an un-tuned database server, the baseline will be inaccurate before it is even put to use.  The baseline will be far above the levels that you reach after tuning and true spikes in performance and progressive problems can easily go on without being caught.</p>
<p>Performance Monitor (perfmon) is a tool that you can use not only for baseline capture but for overall performance for long term monitoring.  At this time I would not recommend the use of profiler in your baseline.  The reason for that is, profiler has a large impact on performance.  Even in a lot of troubleshooting scenarios, perfmon would be utilized first to identify the bottlenecks.  Then profiler can be used as a focused tool to dig out the root causes.</p>
<p><strong>Dynamic Management Views/Functions</strong></p>
<p>With the addition of Dynamic Management Views in SQL Server 2005+, we have a completely new way to add to baseline capturing.  sys.dm_os_performance_counters alone gives us a vast amount of information to work off of that was only available to perfmon.  To see the types of counters that are exposed in performance_counters, use the following query</p>

<div class="bwp-syntax-block clearfix">
<div class="bwp-syntax-toolbar"><div class="bwp-syntax-control"><a href="javascript:;" class="bwp-syntax-source-switch" title="View Source Code"></a></div></div>
<div class="bwp-syntax-wrapper clearfix bwp-syntax-simple"><table class="tsql"><thead><tr><td colspan="2"  class="head">T-SQL</td></tr></thead><tbody><tr class="li1"><td class="ln"><pre class="de1">1
2
3
</pre></td><td class="de1"><pre class="de1"><span class="kw1">SELECT</span> <span class="kw1">DISTINCT</span> <span class="br0">&#91;</span><span class="kw2">object_name</span><span class="br0">&#93;</span> &nbsp;
<span class="kw1">FROM</span> sys.<span class="br0">&#91;</span>dm_os_performance_counters<span class="br0">&#93;</span> &nbsp;
<span class="kw1">ORDER</span> <span class="kw1">BY</span><span class="br0">&#91;</span><span class="kw2">object_name</span><span class="br0">&#93;</span>; </pre></td></tr></tbody></table></div>
<div class="bwp-syntax-source"><pre class="no-parse">SELECT DISTINCT [object_name]  
FROM sys.[dm_os_performance_counters]  
ORDER BY[object_name]; </pre></div></div>

<p>In the list of counters you will see there are quite a few SQL Server counters that we can analyze.   The only problem is everything in there is restricted to SQL Server.  This means we will still need the help of perfmon and other tools like WMI reads.  I mention <a href="http://msdn.microsoft.com/en-us/library/aa394582(VS.85).aspx">Windows Management Instrumentation</a> (WMI) because my own baseline capture process consists of some WMI reads similar to the methods that Jason Massie uses <a href="http://sqlblogcasts.com/blogs/jasonmassie/archive/2007/11/29/accessing-os-performance-counters-from-tsql.aspx">here</a> and using the WMI reader task in SSIS.  WMI can get deep into the hardware and give us information so we can refine our baselines down even further.</p>
<p><strong>Perfmon counters to include in the baseline</strong></p>
<p>The counters to use in perfmon are out there already from <a href="http://technet.microsoft.com/en-us/library/cc781394%28WS.10%29.aspx">a BOL entry on Windows 2003 and SQL Server</a>.  The only variation to this listing that I deviate from is the deadlock and log growth counters.  I feel these should be already tuned and configured so they would not be a common problem.  Instead, they are counters to monitor over the long time period of time for active notification.  With the introduction of Event Notification and Extended Events, even these counters are not as commonly used any longer.  If you find that deadlocks are a problem, your baseline capture should stop and the deadlocks resolved before going on.  Deadlock, blocking or queries not tuned should not be allowed into your baseline.</p>
<p><strong>Putting it all together</strong></p>
<p>SSIS has much more to it than being an ETL product.  With baseline capture tasks, we can use it as our container in order to periodically execute the collection of information we can then set our baseline off of.</p>
<p>Take the following example to start from</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/create_base_1.gif" alt="" title="" width="628" height="451" /></div>
<p>In this control flow we can see WMI reader tasks executing to collect OS, IO and Memory information.  This all then goes into a DBA database and tables to collect the data over time.</p>
<blockquote><p><span class="MT_red">Note: before going on, let’s talk about WMI reader task.  WMI reader task exposes WMI to read in and directly into a flat file or variable.  I like to use the flat file option to minimize memory usage so when doing this, ensure you “append” to the files and not leave the default, Keep original</span></p></blockquote>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/create_base_2.gif" alt="" title="" width="524" height="179" /></div>
<p>Executing all of the WMI reads then sets them into a Data Flow task that will consume the files, convert the data types and import them into a SQL Destination.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/create_base_3.gif" alt="" title="" width="331" height="217" /></div>
<p>Collecting perfmon will already have a set file given the perfmon setup done outside of SSIS.  I recommend going over to Brent Ozar&#8217;s (<a href="http://twitter.com/brento">Twitter</a> | <a href="http://www.brentozar.com/">Blog</a>) blog on <a href="http://www.brentozar.com/archive/2006/12/dba-101-using-perfmon-for-sql-performance-tuning/">perfmon setup</a>.  It’s detailed and well drawn up to get collection of the information going. </p>
<p>Last, but definitely something that makes this easier, the execution of dm_os_performance_counters.  Since this DMV does not have a timestamp associated with it, add getdate() to the select so you can successfully pivot the data and read it for later use. </p>
<blockquote><p><span class="MT_red">Note: the SQL Server counters are all available in perfmon as well.  If executing the DMVs become longer of an execution time than adding them to the perfmon collections and reading the flat file.  This frees SQL Server resources and allows the baseline to be more accurate.  Don’t restrict to this DMV as well.  This is an example.  The memory related DMVs and on is all valuable information.</span></p></blockquote>
<p>Once all of this data is collected, SSRS can be used to analyze where your systems are leveling off.  All spikes of noticeable problems should be resolved and then a time based baseline capture performed once again.  This cycle should go on until a true baseline is captured.</p>
<p>Once the baseline data is collected (typically a week during all hours of operations) you can make decisions on specific thresholds in which an alert should be sent notifying you of a warning or critical event.</p>
<p><strong>Estimating, requesting and begging for more</strong></p>
<p>When creating a baseline and then further estimating the resources you need to maintain that level of performance, overestimating is your best friend.  There is a reason when you ask a DBA, &#8220;How much disk space do you need?&#8221; you will always be given the answer, &#8220;All of it&#8221;.  This is because, before the database server is set in place and growth analytics have been collected, we’re planning for the worst and fastest growth path.  There is nothing wrong with that method of going about resources other than budget planning.</p>
<p>Baseline should give you the starting point to also base your reporting performance over time and estimating resources required for growth and scaling the database server.</p>
<p>Once all of this information is collected, thresholds are set and you know where you stand with your database servers, you will be able to make confident and precise decisions on what the systems are capable of doing for the business and when there is a need for improvements or upgrades.</p>
<p><strong>The meat of third party tools</strong></p>
<p>Now that we have shown we can do all this work on our own, we can&#8217;t end without mentioning the tools that are out there to help us accomplish the same thing.  Anyone that has read my articles in the past knows that I am an advocate for being a lazy DBA.  This really has nothing to do with being lazy but means using everything and anything in order to make our jobs more efficient.  SSIS is great and something I’ve taken on full speed since SQL Server 2005.</p>
<p>With that said, Quest, Red Gate, Idera and a few others have this capability in their products.  Idera SQL Diagnostic Manager in the newer version has a baseline option that captures performance at a time requested and sets thresholds to it.  Quest and the others have similar baseline functionality.</p>
<p>These third party tools make life easy but more importantly, get the job done quicker and with much less human error involved.  I highly recommend if the budget is there and you can convince the decision makers of the value in them, purchase one of them that fits your environment.  </p>
<p>Still no go on the money for those nice tools and development time?  Performance Dashboard Reports have been around since SQL Server 2005.  These reports are valuable as canned reports that read from various DMV/DMFs and give you a snapshot of the SQL Server as it is when executed. OS metrics, Index usage, CPU, IO, user activity…and more is covered.  The dashboard installs right into your SSRS instance.  I personally have this setup for some database servers where a license of my other monitoring tools is not worth the cost based on the noncritical business aspect of the particular server.  (the package will be cleaned up and posted to this blog at a later date for download) I send subscriptions with Excel representations of the reports at time of executed. I save these then for a month or so and can analyze normal vs. abnormal performance.  This information can be taken and a baseline formulated from it as well.  </p>
<p>SQL Server 2005 and 2008 also have the Server Dashboard as part of the canned reports that can be run from SSMS.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/create_base_4.gif" alt="" title="" width="628" height="429" /></div>
<p>This is all very useful and valuable information about the state of your SQL Server and the databases.</p>
<p><strong>Conclusion or what we call, take away</strong></p>
<p>Baseline information can take some work to collect.  The information we collect is critical to allowing us to make decisions on when problems are out of the normal day to day scope.  We can collect true growth and progressive resource usage and base that on educated and precise decisions for upgrading to more advanced and larger hardware when the time comes.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/sql-server-baseline-creation/feed/</wfw:commentRss>
		<slash:comments>10</slash:comments>
		</item>
		<item>
		<title>Baseline, Performance Reporting and being a proactive DBA</title>
		<link>/index.php/datamgmt/datadesign/sql-server-baseline/</link>
		<comments>/index.php/datamgmt/datadesign/sql-server-baseline/#comments</comments>
		<pubDate>Thu, 05 Aug 2010 22:42:54 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[baseline]]></category>
		<category><![CDATA[monitoring]]></category>
		<category><![CDATA[performance]]></category>
		<category><![CDATA[reporting]]></category>
		<category><![CDATA[sql server 2008]]></category>

		<guid isPermaLink="false">/index.php/2010/08/sql-server-baseline/</guid>
		<description><![CDATA[As database professionals, our intent is to be as proactive as possible when it comes to delivering data with security, stability and speed in mind. Being proactive means active monitoring, reporting performance variations and most important, baseline capture. Adding to these three objectives, we can add Performance Notes to also provide key points of long [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>As database professionals, our intent is to be as proactive as possible when it comes to delivering data with security, stability and speed in mind.  Being proactive means active monitoring, reporting performance variations and most important, baseline capture.  Adding to these three objectives, we can add Performance Notes to also provide key points of long term and short term performance problems. </p>
<p>Let&#8217;s take a minute to discuss the three key points (Baselines, Active Monitoring and Reporting Performance)</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/sbTightRope.jpg" alt="" title="" width="455" height="320" /></div>
<p><strong>Baselines</strong></p>
<p>Every system is unique.  We can say that even knowing databases can be the same across systems that are common per vendor installations because every business is unique.  Adding to business differences, users are another variable in the equation.  Bringing those together, we can come to an overall equation that will typically end in a varying result.</p>
<p>Beyond normal performance tuning guidelines, capturing the unique impact on the database server is critical to know where performance during normal operations stands.  This allows managing of predictable performance changes and more importantly, monitoring of unpredictable performance variations.  Without baselines, we can tune database performance, but are not able to tune database performance to the fullest extent for the unique business and user impacts.</p>
<p><strong>Active Monitoring</strong></p>
<p>Active monitoring probably isn’t something unfamiliar.  There are several third party tools available to provide real-time monitoring of database servers.  With work, active monitoring can be achieved from within the feature set that SQL Server provides.  SQL Server Integration Services provides great value in monitoring operating system key performance indicators in next to real-time.  Power Shell can efficiently gather the same operating system level data and several other methods can apply.  (Five ways to do everything: pick the most efficient) Internally, Dynamic Management Views (DMV) and Dynamic Management Functions (DMF) provide a vast amount of information regarding performance.  These can be read, measured and notifications sent when thresholds are reached. </p>
<p>Active monitoring means next to real-time notifications of severe and critical problems affecting the users.  This comes down to the seconds and minutes game.  In order to gather information in these intervals, the performance impact that we make on the servers must be taken into consideration itself.  The development and management that goes into third party monitoring tools is vast and focused.  This makes these tools friendlier in implementations and portability.  It allows us to achieve efficiency in performing our tasks as administrators.  However, each third party monitoring tool should be chosen and matched to the particular database landscape and internal guidelines.  A good example of this is, allowing system object to be installed on instances or security level access.  Some reporting options can even make the difference in decisions.  Chose them wisely and take advantage of trial editions to beat them up so there are no surprises or neighbor envy later on.</p>
<p><strong>Reporting Performance</strong></p>
<p>Reporting performance can be confused with real-time monitoring but it is very different.  Over time, databases change and their needs change along with them.  In some cases this change is drastic and others can take months to happen.  Reporting on overall performance over time periods will show these changes.  Gradual progressive changes can show either performance problems or growth itself requiring action to prepare for the future.</p>
<p>Database growth for example is captured over long periods of time.  This falls into reporting performance for the reason we can plan to handle the growth in the future.  We can also perform tasks such as expanding the size of the database when required.  This can prevent unwanted growth during peak operating times and essentially high resource consumption and directly affecting the users.<br />
Below in the graph, we see the green points as actual growth.  With this we can forecast the growth into the future.  Without this data we may not know that in Month 11 we will need to either expand disk or replace the entire subsystem itself to handle the growth.  With that growth, resources will be consumed more so hardware upgrades to the server itself may be required.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/baseline_1.gif" alt="" title="" width="396" height="291" /></div>
<p>Reporting performance is where the term, Performance Note is introduced.  </p>
<p><strong>Performance Notes</strong></p>
<p>These events can occur over months, days and even hours.  They are subtle and large spikes from the baseline performance we captured from normal performance impacts.  These events are typically not performance problems that directly affect the users but neglected over time, can become severe problems.</p>
<p>Take the graph below for an example where CPU usage over a day is represented.  In the graph we see in the beginning of the day, utilization is very low.  Then at a certain time, CPU starts showing normal activity.  In the middle of the day a large spike in CPU occurs.  This spike should be noted and in the following days monitoring of the performance notations should be done if it is unknown to the baseline.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/baseline_2.gif" alt="" title="" width="628" height="178" /></div>
<p>Below is a graph of CPU usage over the two days after making the observation of the spike in CPU.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/baseline_3.gif" alt="" title="" width="628" height="119" /></div>
<p>We can see that the spikes occur in both days and similar times.<br />
These performance notes should be looked into closely to determine what is impacting the CPU usage during the time of the spikes.  Often the spikes are found to be normal business operations but in some cases they are found to be performance problems that should be resolved.</p>
<p>With this we can put perfmon/profiler and other tools at our disposal to work by collecting more detailed information during and around the spike in resource consumption.  For example, if we see Processor Time counter high and correlate that to other common CPU counters and SQL counters, we can determine if we have common CPU bottlenecks like compilations, recompilation, parameterization etc&#8230; issues or simply a query that was introduced to the database server without our knowledge.  Then we can act quickly on it by optimization of resources or the query(s) taking part in the spikes.</p>
<p><strong>Conclusion</strong></p>
<p>Given the methods you can take to be proactive on SQL Server to ensure problems are limited to your knowledge is a stable of being successful in delivering data to the business.  With the steps and types of monitoring and reporting described, most situations can be caught, analyzed and resolved swiftly.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/sql-server-baseline/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
	</channel>
</rss>
