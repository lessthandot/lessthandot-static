<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>restore &#8211; LessthanDot</title>
	<atom:link href="/index.php/tag/restore/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>A Technical Community for IT Professionals</description>
	<lastBuildDate>Sat, 09 Mar 2019 12:50:36 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.6.1</generator>
	<item>
		<title>Listing all RESTORE and BACKUP operations currently going on your SQL Server</title>
		<link>/index.php/datamgmt/dbprogramming/listing-all-restore-and-backup/</link>
		<comments>/index.php/datamgmt/dbprogramming/listing-all-restore-and-backup/#respond</comments>
		<pubDate>Sun, 17 Feb 2013 15:30:00 +0000</pubDate>
		<dc:creator><![CDATA[SQLDenis]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[restore]]></category>
		<category><![CDATA[sql server 2000]]></category>
		<category><![CDATA[sql server 2005]]></category>
		<category><![CDATA[sql server 2008]]></category>
		<category><![CDATA[sql server 2012]]></category>

		<guid isPermaLink="false">/index.php/2013/02/listing-all-restore-and-backup/</guid>
		<description><![CDATA[Sometimes you want to quickly see if there are any databases or logs being backed up or restored at this moment. I blogged at one point how you can check how much longer the restore will take here: How much longer will the SQL Server database restore ta&#8230;]]></description>
				<content:encoded><![CDATA[<p>Sometimes you want to quickly see if there are any databases or logs being backed up or restored at this moment. I blogged at one point how you can check how much longer the restore will take here: <a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/how-much-longer-will-the">How much longer will the SQL Server database restore take</a>. The other day someone wanted to know this information for all databases on a server, he wanted to know this for restores as well as backups. The query below will give you that info as well as the percentage that is complete for each operation</p>

<div class="bwp-syntax-block clearfix">
<div class="bwp-syntax-toolbar"><div class="bwp-syntax-control"><a href="javascript:;" class="bwp-syntax-source-switch" title="View Source Code"></a></div></div>
<div class="bwp-syntax-wrapper clearfix bwp-syntax-simple"><table class="tsql"><thead><tr><td colspan="2"  class="head">T-SQL</td></tr></thead><tbody><tr class="li1"><td class="ln"><pre class="de1">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="de1"><pre class="de1"><span class="kw1">SELECT</span> 
&nbsp; &nbsp; d.<span class="me1">PERCENT_COMPLETE</span> <span class="kw1">AS</span> <span class="br0">&#91;</span><span class="sy0">%</span>Complete<span class="br0">&#93;</span>,
&nbsp; &nbsp; d.<span class="me1">TOTAL_ELAPSED_TIME</span><span class="sy0">/</span><span class="nu0">60000</span> <span class="kw1">AS</span> ElapsedTimeMin,
&nbsp; &nbsp; d.<span class="me1">ESTIMATED_COMPLETION_TIME</span><span class="sy0">/</span><span class="nu0">60000</span> &nbsp; <span class="kw1">AS</span> TimeRemainingMin,
&nbsp; &nbsp; d.<span class="me1">TOTAL_ELAPSED_TIME</span><span class="sy0">*</span><span class="nu0">0.00000024</span> <span class="kw1">AS</span> ElapsedTimeHours,
&nbsp; &nbsp; d.<span class="me1">ESTIMATED_COMPLETION_TIME</span><span class="sy0">*</span><span class="nu0">0.00000024</span> &nbsp;<span class="kw1">AS</span> TimeRemainingHours,
&nbsp; &nbsp; d.<span class="me1">COMMAND</span> <span class="kw1">as</span> Command,
&nbsp; &nbsp; s.<span class="kw1">text</span> <span class="kw1">as</span> CommandExecuted
<span class="kw1">FROM</span> &nbsp; &nbsp;sys.<span class="me1">dm_exec_requests</span> d
<span class="sy0">CROSS</span> APPLY sys.<span class="me1">dm_exec_sql_text</span><span class="br0">&#40;</span>d.<span class="me1">sql_handle</span><span class="br0">&#41;</span><span class="kw1">as</span> s
<span class="kw1">WHERE</span> &nbsp;d.<span class="me1">COMMAND</span> <span class="sy0">LIKE</span> <span class="st0">'RESTORE DATABASE%'</span>
or d.<span class="me1">COMMAND</span>&nbsp; &nbsp; &nbsp;<span class="sy0">LIKE</span> <span class="st0">'RESTORE LOG%'</span>
<span class="sy0">OR</span> d.<span class="me1">COMMAND</span>&nbsp; &nbsp; &nbsp;<span class="sy0">LIKE</span> <span class="st0">'BACKUP DATABASE%'</span>
<span class="sy0">OR</span> d.<span class="me1">COMMAND</span>&nbsp; &nbsp; &nbsp;<span class="sy0">LIKE</span> <span class="st0">'BACKUP LOG%'</span>
<span class="kw1">ORDER</span> &nbsp; <span class="kw1">BY</span> <span class="nu0">2</span> <span class="kw1">desc</span>, <span class="nu0">3</span> <span class="kw1">DESC</span></pre></td></tr></tbody></table></div>
<div class="bwp-syntax-source"><pre class="no-parse">SELECT 
    d.PERCENT_COMPLETE AS [%Complete],
    d.TOTAL_ELAPSED_TIME/60000 AS ElapsedTimeMin,
    d.ESTIMATED_COMPLETION_TIME/60000   AS TimeRemainingMin,
    d.TOTAL_ELAPSED_TIME*0.00000024 AS ElapsedTimeHours,
    d.ESTIMATED_COMPLETION_TIME*0.00000024  AS TimeRemainingHours,
    d.COMMAND as Command,
	s.text as CommandExecuted
FROM    sys.dm_exec_requests d
CROSS APPLY sys.dm_exec_sql_text(d.sql_handle)as s
WHERE  d.COMMAND LIKE 'RESTORE DATABASE%'
or d.COMMAND	 LIKE 'RESTORE LOG%'
OR d.COMMAND	 LIKE 'BACKUP DATABASE%'
OR d.COMMAND	 LIKE 'BACKUP LOG%'
ORDER   BY 2 desc, 3 DESC</pre></div></div>

<p>Throw this in a view on your <em>Tools</em> database and you are all set.</p>
<p>This will probably also be added to SQLCop&#8217;s informational section</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbprogramming/listing-all-restore-and-backup/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>MongoDB: How to backup all the databases with one command</title>
		<link>/index.php/datamgmt/dbadmin/mongodb-how-to-backup-all/</link>
		<comments>/index.php/datamgmt/dbadmin/mongodb-how-to-backup-all/#comments</comments>
		<pubDate>Fri, 01 Feb 2013 08:48:00 +0000</pubDate>
		<dc:creator><![CDATA[SQLDenis]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[bigdata]]></category>
		<category><![CDATA[collections]]></category>
		<category><![CDATA[mongodb]]></category>
		<category><![CDATA[nosql]]></category>
		<category><![CDATA[restore]]></category>

		<guid isPermaLink="false">/index.php/2013/02/mongodb-how-to-backup-all/</guid>
		<description><![CDATA[We looked at how to backup and restore databases in the post MongoDB: How to backup and restore databases. We also looked at how to restore collection in the post MongoDB: How to restore collections. Today we are going to look at how to backup all the d&#8230;]]></description>
				<content:encoded><![CDATA[<p>We looked at how to backup and restore databases in the post <a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/mongodb-backup-and-restore-databases">MongoDB: How to backup and restore databases</a>. We also looked at how to restore collection in the post <a href="/index.php/DataMgmt/DBAdmin/mongodb-how-to-restore-collections">MongoDB: How to restore collections</a>. Today we are going to look at how to backup all the databases in one shot with one simple command. Before we get started connect to your MongoDB server, we are going to create a couple of databases</p>
<p>Run the following</p>
<pre>use MultiCollection

db.Blog.insert( { name : "Denis",  age : 20 } )
db.Blog.insert( { name : "Abe",    age : 30 } )
db.Blog.insert( { name : "John",   age : 40 } )
db.Blog.insert( { name : "Xavier", age : 10 } )
db.Blog.insert( { name : "Zen",    age : 50 } )


db.People.insert( { name : "AADenis",  age : 0020 } )
db.People.insert( { name : "AAAbe",    age : 0030 } )
db.People.insert( { name : "AAJohn",   age : 0040 } )
db.People.insert( { name : "AAXavier", age : 0010 } )
db.People.insert( { name : "AAZen",    age : 0050 } )


use SingleCollection

db.Blog.insert( { name : "Denis",  age : 20 } )
db.Blog.insert( { name : "Abe",    age : 30 } )
db.Blog.insert( { name : "John",   age : 40 } )
db.Blog.insert( { name : "Xavier", age : 10 } )
db.Blog.insert( { name : "Zen",    age : 50 } )

use TestDB

db.Test.insert( { name : "Denis",  age : 20 } )
db.Test.insert( { name : "Abe",    age : 30 } )
db.Test.insert( { name : "John",   age : 40 } )
db.Test.insert( { name : "Xavier", age : 10 } )
db.Test.insert( { name : "Zen",    age : 50 } )

use TestStuff

db.Stuff.insert( { name : "Denis",  age : 20 } )
db.Stuff.insert( { name : "Abe",    age : 30 } )
db.Stuff.insert( { name : "John",   age : 40 } )
db.Stuff.insert( { name : "Xavier", age : 10 } )
db.Stuff.insert( { name : "Zen",    age : 50 } )</pre>
<p>The code above will create 4 databases. Now it is time to back all the databases up. Open up another command window, navigate to the bin directory where MongoDB is installed</p>
<p>You can of course run the mongodump command for every database you have, for example</p>
<pre>mongodump --db MultiCollection
mongodump --db SingleCollection
mongodump --db TestDB
mongodump --db TestStuff</pre>
<p>But did you know that if you don&#8217;t specify a database that mongodump will back up all the databases? Try it out, run the following</p>
<p>mongodump </p>
<p>Here is what the output looks like</p>
<pre>C:NoSQLmongodbbin&gt;mongodump
connected to: 127.0.0.1
Fri Feb 01 05:41:33 all dbs
Fri Feb 01 05:41:35 DATABASE: MultiCollection    to     dump/MultiCollection
Fri Feb 01 05:41:35     MultiCollection.Blog to dump/MultiCollection/Blog.bson
Fri Feb 01 05:41:35              5 objects
Fri Feb 01 05:41:36     Metadata for MultiCollection.Blog to dump/MultiCollection/Blog.metadata.json
Fri Feb 01 05:41:36     MultiCollection.People to dump/MultiCollection/People.bson
Fri Feb 01 05:41:36              5 objects
Fri Feb 01 05:41:36 Metadata for MultiCollection.People to dump/MultiCollection/People.metadata.json
Fri Feb 01 05:41:36 DATABASE: SingleCollection   to     dump/SingleCollection
Fri Feb 01 05:41:36     SingleCollection.Blog to dump/SingleCollection/Blog.bson
Fri Feb 01 05:41:36              5 objects
Fri Feb 01 05:41:36     Metadata for SingleCollection.Blog to dump/SingleCollection/Blog.metadata.json
Fri Feb 01 05:41:36 DATABASE: TestDB     to     dump/TestDB
Fri Feb 01 05:41:36     TestDB.Test to dump/TestDB/Test.bson
Fri Feb 01 05:41:36              5 objects
Fri Feb 01 05:41:36     Metadata for TestDB.Test to dump/TestDB/Test.metadata.json
Fri Feb 01 05:41:36 DATABASE: TestStuff  to     dump/TestStuff
Fri Feb 01 05:41:36     TestStuff.Stuff to dump/TestStuff/Stuff.bson
Fri Feb 01 05:41:36              5 objects
Fri Feb 01 05:41:36     Metadata for TestStuff.Stuff to dump/TestStuff/Stuff.metadata.json
Fri Feb 01 05:41:36 DATABASE: admin      to     dump/admin

C:NoSQLmongodbbin&gt;</pre>
<p>If you look in your bin directory, you will see a dump directory, inside the dump directory, you will see a directory for every database that was backed up.</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/DataMgmt/Denis/MultiDBackupMongoDB.PNG?mtime=1359714862"><img alt="" src="/wp-content/uploads/blogs/DataMgmt/Denis/MultiDBackupMongoDB.PNG?mtime=1359714862" width="640" height="265" /></a></div>
<p>
That is all for this post, if you are interested in my other MongoDB posts, you can find them here:<br />
<a href="/index.php/DataMgmt/DBProgramming/creating-mongodb-as-a-service">Install MongoDB as a Windows Service</a><br />
<a href="/index.php/DataMgmt/DBProgramming/doing-upserts-in-mongodb">UPSERTs with MongoDB</a><br />
<a href="/index.php/DataMgmt/DBProgramming/mongodb-how-to-sort-results">How to sort results in MongoDB</a><br />
<a href="/index.php/DataMgmt/DBProgramming/indexes-in-mongodb">Indexes in MongoDB: A quick overview</a><br />
<a href="/index.php/DataMgmt/DBProgramming/multidocument-updates-with-mongodb">Multidocument updates with MongoDB</a><br />
<a href="/index.php/DataMgmt/DBProgramming/mongodb-how-to-include-and">MongoDB: How to include and exclude the fields you want in results</a><br />
<a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/mongodb-how-to-limit-results">MongoDB: How to limit results and how to page through results</a><br />
<a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/mongodb-backup-and-restore-databases">MongoDB: How to backup and restore databases</a><br />
<a href="/index.php/DataMgmt/DBAdmin/mongodb-how-to-restore-collections">MongoDB: How to restore collections</a></p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbadmin/mongodb-how-to-backup-all/feed/</wfw:commentRss>
		<slash:comments>4</slash:comments>
		</item>
		<item>
		<title>MongoDB: How to restore collections</title>
		<link>/index.php/datamgmt/dbadmin/mongodb-how-to-restore-collections/</link>
		<comments>/index.php/datamgmt/dbadmin/mongodb-how-to-restore-collections/#respond</comments>
		<pubDate>Thu, 31 Jan 2013 20:08:00 +0000</pubDate>
		<dc:creator><![CDATA[SQLDenis]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[bigdata]]></category>
		<category><![CDATA[collections]]></category>
		<category><![CDATA[mongodb]]></category>
		<category><![CDATA[nosql]]></category>
		<category><![CDATA[restore]]></category>

		<guid isPermaLink="false">/index.php/2013/01/mongodb-how-to-restore-collections/</guid>
		<description><![CDATA[In yesterday's post MongoDB: How to backup and restore databases we looked at how to backup and restore a database, today we are going to look at how to restore a collection from a backup. Be aware that mongorestore and mongodump have to be executed fro&#8230;]]></description>
				<content:encoded><![CDATA[<p>In yesterday&#8217;s post <a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/mongodb-backup-and-restore-databases">MongoDB: How to backup and restore databases</a> we looked at how to backup and restore a database, today we are going to look at how to restore a collection from a backup. Be aware that mongorestore and mongodump have to be executed from a command window in the bin directory where mongodb is. To execute the MongoDB commands you need to connect to mongodb first. On my PC this is in the directory C:NoSQLmongodbbin></p>
<p>In order to get started create a new database, name it MultiCollection </p>
<p>You can just run this command, it will switch to the MultiCollection database if it exists or otherwise it will create the MultiCollection database</p>
<pre>use MultiCollection</pre>
<p>Now add these two collections</p>
<pre>db.Blog.insert( { name : "Denis",  age : 20 } )
db.Blog.insert( { name : "Abe",    age : 30 } )
db.Blog.insert( { name : "John",   age : 40 } )
db.Blog.insert( { name : "Xavier", age : 10 } )
db.Blog.insert( { name : "Zen",    age : 50 } )


db.People.insert( { name : "AADenis",  age : 0020 } )
db.People.insert( { name : "AAAbe",    age : 0030 } )
db.People.insert( { name : "AAJohn",   age : 0040 } )
db.People.insert( { name : "AAXavier", age : 0010 } )
db.People.insert( { name : "AAZen",    age : 0050 } )</pre>
<p>Open a new command window, navigate to your mongodb bin directory. Now execute the <code>mongodump --db MultiCollection </code>command<br />
This command will backup the database into the dump directory, if this directory does not exist it will be created, in my case it will be located here C:NoSQLmongodbbindump. In the dump directory you will see a directory with the same name as the database that you are backing up</p>
<pre>mongodump --db MultiCollection</pre>
<p>Here is the output</p>
<pre>connected to: 127.0.0.1
Thu Jan 31 16:16:52 DATABASE: MultiCollection    to     dump/MultiCollection
Thu Jan 31 16:16:52     MultiCollection.Blog to dump/MultiCollection/Blog.bson
Thu Jan 31 16:16:52              5 objects
Thu Jan 31 16:16:52     Metadata for MultiCollection.Blog to dump/MultiCollection/Blog.metadata.json
Thu Jan 31 16:16:52     MultiCollection.People to dump/MultiCollection/People.bson
Thu Jan 31 16:16:52              5 objects
Thu Jan 31 16:16:52 Metadata for MultiCollection.People to dump/MultiCollection/People.metadata.json

C:NoSQLmongodbbin&gt;</pre>
<p>Now let&#8217;s add one more item to the People collection</p>
<pre>db.People.insert( { name : "ZZZZZZ",  age : 9999 } )</pre>
<p>Time to do the restore</p>
<pre>mongorestore dump/MultiCollection</pre>
<p>Here is the output</p>
<pre>connected to: 127.0.0.1
Thu Jan 31 16:20:52 dump/MultiCollection/Blog.bson
Thu Jan 31 16:20:52     going into namespace [MultiCollection.Blog]
Thu Jan 31 16:20:52 warning: Restoring to MultiCollection.Blog without dropping. Restored data will be inserted
raising errors; check your server log
5 objects found
Thu Jan 31 16:20:52     Creating index: { key: { _id: 1 }, ns: "MultiCollection.Blog", name: "_id_" }
Thu Jan 31 16:20:52 dump/MultiCollection/People.bson
Thu Jan 31 16:20:52     going into namespace [MultiCollection.People]
5 objects found
Thu Jan 31 16:20:52     Creating index: { key: { _id: 1 }, ns: "MultiCollection.People", name: "_id_" }</pre>
<p>Let&#8217;s take a look at what we have<br />
We should have 5 items since we did a backup before we added item 6, we restored that backup</p>
<pre>db.People.find()</pre>
<p>Output is here</p>
<pre>{ "_id" : ObjectId("510adec2d9a67956d3f4a44b"), "name" : "AADenis", "age" : 16 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a44c"), "name" : "AAAbe", "age" : 24 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a44d"), "name" : "AAJohn", "age" : 32 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a44e"), "name" : "AAXavier", "age" : 8 }
{ "_id" : ObjectId("510adec5d9a67956d3f4a44f"), "name" : "AAZen", "age" : 40 }
{ "_id" : ObjectId("510ae0b8d9a67956d3f4a451"), "name" : "ZZZZZZ", "age" : 9999 }</pre>
<p>Something is not right, we still have 6 items. If you look back at the restore output, you will see the following warning</p>
<blockquote><p>warning: Restoring to MultiCollection.Blog without dropping. Restored data will be inserted<br />
raising errors; check your server log</p></blockquote>
<p>What we have to do is drop the collections first, you do that by specifying <code>--drop</code></p>
<p>Let&#8217;s do the restore again but now with the &#8211;drop option</p>
<pre>mongorestore dump/MultiCollection --drop</pre>
<p>Here is the output </p>
<pre>connected to: 127.0.0.1
Thu Jan 31 16:25:26 dump/MultiCollection/Blog.bson
Thu Jan 31 16:25:26     going into namespace [MultiCollection.Blog]
Thu Jan 31 16:25:26      dropping
5 objects found
Thu Jan 31 16:25:26     Creating index: { key: { _id: 1 }, ns: "MultiCollection.Blog", name: "_id_" }
Thu Jan 31 16:25:26 dump/MultiCollection/People.bson
Thu Jan 31 16:25:26     going into namespace [MultiCollection.People]
Thu Jan 31 16:25:26      dropping
5 objects found
Thu Jan 31 16:25:26     Creating index: { key: { _id: 1 }, ns: "MultiCollection.People", name: "_id_" }</pre>
<pre>db.People.find()</pre>
<p>Here are the results</p>
<pre>{ "_id" : ObjectId("510adec2d9a67956d3f4a44b"), "name" : "AADenis", "age" : 16 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a44c"), "name" : "AAAbe", "age" : 24 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a44d"), "name" : "AAJohn", "age" : 32 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a44e"), "name" : "AAXavier", "age" : 8 }
{ "_id" : ObjectId("510adec5d9a67956d3f4a44f"), "name" : "AAZen", "age" : 40 }</pre>
<p>As you can see we have 5 items now</p>
<p>Let&#8217;s insert the same item again</p>
<pre>db.People.insert( { name : "ZZZZZZ",  age : 9999 } )</pre>
<p>Let&#8217;s also drop the Blog collection</p>
<pre>db.Blog.drop()</pre>
<p>Now if you do a find nothing is there</p>
<pre>db.Blog.find()</pre>
<p>In order to restore a collection, you need to use the &#8211;collection option and give the collection name, you also need to specify where the backup file is. In our case it is dump/Multicollection/Blog.bson. You will also see that information from the dump output<br />
<blockquote>MultiCollection.Blog to dump/MultiCollection/Blog.bson</p></blockquote>
<p>Run the following</p>
<pre>mongorestore --db MultiCollection --collection Blog dump/Multicollection/Blog.bson</pre>
<p>Here is the output</p>
<pre>connected to: 127.0.0.1
Thu Jan 31 17:01:28 dump/Multicollection/Blog.bson
Thu Jan 31 17:01:28     going into namespace [MultiCollection.Blog]
5 objects found
Thu Jan 31 17:01:28     Creating index: { key: { _id: 1 }, ns: "MultiCollection.Blog", name: "_id_" }</pre>
<p>Since we already dropped the collection manually we didn&#8217;t have to add the drop option. Let&#8217;s see what we have in the database now, People should have 6 items and Blog should have 5 items</p>
<pre>db.Blog.find()
{ "_id" : ObjectId("510adec1d9a67956d3f4a446"), "name" : "Denis", "age" : 20 }
{ "_id" : ObjectId("510adec1d9a67956d3f4a447"), "name" : "Abe", "age" : 30 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a448"), "name" : "John", "age" : 40 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a449"), "name" : "Xavier", "age" : 10 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a44a"), "name" : "Zen", "age" : 50 }

db.People.find()
{ "_id" : ObjectId("510adec2d9a67956d3f4a44b"), "name" : "AADenis", "age" : 16 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a44c"), "name" : "AAAbe", "age" : 24 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a44d"), "name" : "AAJohn", "age" : 32 }
{ "_id" : ObjectId("510adec2d9a67956d3f4a44e"), "name" : "AAXavier", "age" : 8 }
{ "_id" : ObjectId("510adec5d9a67956d3f4a44f"), "name" : "AAZen", "age" : 40 }
{ "_id" : ObjectId("510ae75cd9a67956d3f4a453"), "name" : "ZZZZZZ", "age" : 9999 }</pre>
<p>As you can see the People collection did not get overwritten and the Blog collection got restored</p>
<p>That is all for this post, if you are interested in my other MongoDB posts, you can find them here:<br />
<a href="/index.php/DataMgmt/DBProgramming/creating-mongodb-as-a-service">Install MongoDB as a Windows Service</a><br />
<a href="/index.php/DataMgmt/DBProgramming/doing-upserts-in-mongodb">UPSERTs with MongoDB</a><br />
<a href="/index.php/DataMgmt/DBProgramming/mongodb-how-to-sort-results">How to sort results in MongoDB</a><br />
<a href="/index.php/DataMgmt/DBProgramming/indexes-in-mongodb">Indexes in MongoDB: A quick overview</a><br />
<a href="/index.php/DataMgmt/DBProgramming/multidocument-updates-with-mongodb">Multidocument updates with MongoDB</a><br />
<a href="/index.php/DataMgmt/DBProgramming/mongodb-how-to-include-and">MongoDB: How to include and exclude the fields you want in results</a><br />
<a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/mongodb-how-to-limit-results">MongoDB: How to limit results and how to page through results</a><br />
<a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/mongodb-backup-and-restore-databases">MongoDB: How to backup and restore databases</a></p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbadmin/mongodb-how-to-restore-collections/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>MongoDB: How to backup and restore databases</title>
		<link>/index.php/datamgmt/dbadmin/mssqlserveradmin/mongodb-backup-and-restore-databases/</link>
		<comments>/index.php/datamgmt/dbadmin/mssqlserveradmin/mongodb-backup-and-restore-databases/#comments</comments>
		<pubDate>Wed, 30 Jan 2013 17:13:00 +0000</pubDate>
		<dc:creator><![CDATA[SQLDenis]]></dc:creator>
				<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[bigdata]]></category>
		<category><![CDATA[mongodb]]></category>
		<category><![CDATA[nosql]]></category>
		<category><![CDATA[restore]]></category>

		<guid isPermaLink="false">/index.php/2013/01/mongodb-backup-and-restore-databases/</guid>
		<description><![CDATA[Today it is time to learn how to backup and restore databases in MongoDB. You do have jobs setup that automatically create backups right? If you do not, please close this window and go set that up first, your data is the most important part of the organ&#8230;]]></description>
				<content:encoded><![CDATA[<p>Today it is time to learn how to backup and restore databases in MongoDB. You do have jobs setup that automatically create backups right? If you do not, please close this window and go set that up first, your data is the most important part of the organization, without data you got nothing, just ask <a href="/index.php/ITProfessionals/EthicsIT/magnolia-crashed-data-corruption-and-los">ma.gnolia</a>. That site is not around anymore because they didn&#8217;t back up their database. Enough of that let&#8217;s get started.<br />
Connect to mongodb, create a new database named blog. You can do that by just executing <code>use blog</code></p>
<p>Now insert the following 5 items in that database</p>
<pre>db.Blog.insert( { name : "Denis",  age : 20, city : "Princeton" } )
db.Blog.insert( { name : "Abe",    age : 30, city : "Amsterdam" } )
db.Blog.insert( { name : "John",   age : 40, city : "New York"  } )
db.Blog.insert( { name : "Xavier", age : 10, city : "Barcelona" } )
db.Blog.insert( { name : "Zen",    age : 50, city : "Kyoto"     } )</pre>
<p>You have now a collection named Blog with those 5 items.</p>
<p>If you want to list all the collections for a database, you can use <code>db.getCollectionNames()</code></p>
<pre>db.getCollectionNames()</pre>
<p>Here is the output</p>
<pre>[ "Blog", "system.indexes" ]</pre>
<p>Let&#8217;s get some stats for that collection</p>
<pre>db.Blog.stats()</pre>
<p>Here is the output</p>
<pre>{
        "ns" : "blog.Blog",
        "count" : 5,
        "size" : 356,
        "avgObjSize" : 71.2,
        "storageSize" : 8192,
        "numExtents" : 1,
        "nindexes" : 1,
        "lastExtentSize" : 8192,
        "paddingFactor" : 1,
        "systemFlags" : 1,
        "userFlags" : 0,
        "totalIndexSize" : 8176,
        "indexSizes" : {
                "_id_" : 8176
        },
        "ok" : 1
}</pre>
<p>
<h2>Backing Up</h2>
<p>Time to do the backup. To do a backup you can&#8217;t run if you are connected to mongodb. Open up a new command/shell window, navigate to the bin directory inside the mongodb folder. In my case this is D:mongodbbin. To do a backup we are going to call the mongodump executable inside the bin directory. Here is what the syntax will look like</p>
<p>mongodump &#8211;db {Database name} </p>
<p>You need to change the database name to something that you have. Running the command will create a directory named dump</p>
<pre>mongodump --db blog</pre>
<p>Here is what the output looks like</p>
<pre>connected to: 127.0.0.1
Wed Jan 30 13:55:56 DATABASE: blog       to     dump/blog
Wed Jan 30 13:55:56     blog.Blog to dump/blog/Blog.bson
Wed Jan 30 13:55:56              5 objects
Wed Jan 30 13:55:56     Metadata for blog.Blog to dump/blog/Blog.metadata.json

D:mongodbbin&gt;</pre>
<p>The Blog.bson file is the actual backup of the database</p>
<p>As you can see the backup also creates a metadata file Blog.metadata.json</p>
<p>All that the manifest file has in it is the following<br />
{ &#8220;indexes&#8221; : [ { &#8220;v&#8221; : 1, &#8220;key&#8221; : { &#8220;_id&#8221; : 1 }, &#8220;ns&#8221; : &#8220;blog.Blog&#8221;, &#8220;name&#8221; : &#8220;_id_&#8221; } ] }</p>
<p>As you can see it has the database as well as the collection name</p>
<p>We will talk more about the .bson file in tomorrow&#8217;s post</p>
<p>Now that we have a backup we can do a restore. But first let&#8217;s drop the collection  <img src="https://s.w.org/images/core/emoji/2/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>Execute the following in the command window connected to mongodb</p>
<pre>db.Blog.drop()</pre>
<p>Here is the output</p>
<pre>true</pre>
<p>Now if we check the stats again, there will be an error</p>
<pre>db.Blog.stats()</pre>
<pre>{ "errmsg" : "ns not found", "ok" : 0 }</pre>
<p>
<h2>Restore</h2>
<p>Now it is time to do a restore, use the command/shell window where you did the backup. The restore process uses an executable as well that needs to be executed from a command window outside the MongoDB process.</p>
<p>The command will look like the following</p>
<p><code>mongorestore directory/database</code><br />
In our case the command will look like the following</p>
<p><code>mongorestore dump/blog</code></p>
<p>Execute the following</p>
<pre>mongorestore dump/blog</pre>
<p>Here is what the output looks like</p>
<pre>connected to: 127.0.0.1
Wed Jan 30 13:59:58 dump/blog/Blog.bson
Wed Jan 30 13:59:58     going into namespace [blog.Blog]
5 objects found
Wed Jan 30 13:59:58     Creating index: { key: { _id: 1 }, ns: "blog.Blog", name: "_id_" }</pre>
<p>Now, let&#8217;s see if we have the collection back</p>
<pre>db.Blog.stats()</pre>
<pre>{
        "ns" : "blog.Blog",
        "count" : 5,
        "size" : 356,
        "avgObjSize" : 71.2,
        "storageSize" : 8192,
        "numExtents" : 1,
        "nindexes" : 1,
        "lastExtentSize" : 8192,
        "paddingFactor" : 1,
        "systemFlags" : 1,
        "userFlags" : 0,
        "totalIndexSize" : 8176,
        "indexSizes" : {
                "_id_" : 8176
        },
        "ok" : 1
}</pre>
<p>As you can see the collection is there again, the restore worked.</p>
<p>You can also restore the backup to a new database or another database. If the database does not exists it will create one for you. If I run the same command from before but use &#8211;db RestoredDB you will get the following output</p>
<pre>mongorestore -db RestoredDB dump/blog
connected to: 127.0.0.1
Wed Jan 30 14:05:26 dump/blog/Blog.bson
Wed Jan 30 14:05:26     going into namespace [RestoredDB.Blog]
5 objects found
Wed Jan 30 14:05:26     Creating index: { key: { _id: 1 }, ns: "RestoredDB.Blog", name: "_id_" }</pre>
<p>Now connecting to MongoDB again we can check that the new database is there with the same collection</p>
<pre>use RestoredDB
switched to db RestoredDB
db.Blog.find()
{ "_id" : ObjectId("510961b1e5247980b903c30c"), "name" : "Denis", "age" : 20, "city" : "Princeton" }
{ "_id" : ObjectId("510961b3e5247980b903c30d"), "name" : "Abe", "age" : 30, "city" : "Amsterdam" }
{ "_id" : ObjectId("510961b3e5247980b903c30e"), "name" : "John", "age" : 40, "city" : "New York" }
{ "_id" : ObjectId("510961b3e5247980b903c30f"), "name" : "Xavier", "age" : 10, "city" : "Barcelona"
{ "_id" : ObjectId("510961b5e5247980b903c310"), "name" : "Zen", "age" : 50, "city" : "Kyoto" }</pre>
<p>As you can see it it pretty straightforward to backup and restore databases. What if you just want to restore a collection? Aha&#8230;that will be tomorrow&#8217;s post: <a href="/index.php/DataMgmt/DBAdmin/mongodb-how-to-restore-collections">MongoDB: How to restore collections</a></p>
<p>That is all for this post, if you are interested in my other MongoDB posts, you can find them here:<br />
<a href="/index.php/DataMgmt/DBProgramming/creating-mongodb-as-a-service">Install MongoDB as a Windows Service</a><br />
<a href="/index.php/DataMgmt/DBProgramming/doing-upserts-in-mongodb">UPSERTs with MongoDB</a><br />
<a href="/index.php/DataMgmt/DBProgramming/mongodb-how-to-sort-results">How to sort results in MongoDB</a><br />
<a href="/index.php/DataMgmt/DBProgramming/indexes-in-mongodb">Indexes in MongoDB: A quick overview</a><br />
<a href="/index.php/DataMgmt/DBProgramming/multidocument-updates-with-mongodb">Multidocument updates with MongoDB</a><br />
<a href="/index.php/DataMgmt/DBProgramming/mongodb-how-to-include-and">MongoDB: How to include and exclude the fields you want in results</a><br />
<a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/mongodb-how-to-limit-results">MongoDB: How to limit results and how to page through results</a></p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbadmin/mssqlserveradmin/mongodb-backup-and-restore-databases/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>SQL Advent 2012 Day 22: Testing your backup and failover strategy</title>
		<link>/index.php/webdev/business-intelligence/testing-your-backup-and-failover/</link>
		<comments>/index.php/webdev/business-intelligence/testing-your-backup-and-failover/#respond</comments>
		<pubDate>Sat, 22 Dec 2012 21:31:00 +0000</pubDate>
		<dc:creator><![CDATA[SQLDenis]]></dc:creator>
				<category><![CDATA[Business Intelligence]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[restore]]></category>
		<category><![CDATA[sql advent 2012]]></category>
		<category><![CDATA[sql server 2008]]></category>
		<category><![CDATA[sql server 2008 r2]]></category>
		<category><![CDATA[sql server 2012]]></category>
		<category><![CDATA[testing]]></category>

		<guid isPermaLink="false">/index.php/2012/12/testing-your-backup-and-failover/</guid>
		<description><![CDATA[This is day twenty-two of the SQL Advent 2012 series of blog posts. Today we are going to look at how to test your backup and failover strategy]]></description>
				<content:encoded><![CDATA[<p>This is day twenty-two of the <a href="/index.php/DataMgmt/DBProgramming/sql-advent-2012-here-is">SQL Advent 2012 series</a> of blog posts. Today we are going to look at how to test your backup and failover strategy</p>
<p>Let&#8217;s say your CEO comes to you and asks if your backups are good, you say yes, the CEO proceeds to tell you that the board will be arriving in 5 minutes and he will do a hard unplug of your main server. Have the backup restored within 1 hour. How comfortable are you now? Do you actually even test your backups, how do you know that they are not corrupt? What about failover to the other data center, has this been tested, do you know that it will work? </p>
<p>With Hurricane Sandy causing havoc a couple of weeks ago where whole data centers ran out of fuel and generators didn&#8217;t start up, there are a whole bunch of companies rethinking their HA/DR strategy. Is it really wise having your data center not enough geographically dispersed. I mean if your main data center is in New York City and your backup data center is in Jersey City or Secaucus then you will be in trouble when a storm like Sandy comes along.</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/DataMgmt/Denis/ADvent/Storm.PNG?mtime=1356218425"><img alt="" title="Hi I am Sandy...I am here to destroy your backups" src="/wp-content/uploads/blogs/DataMgmt/Denis/ADvent/Storm.PNG?mtime=1356218425" width="560" height="328" /></a></div>
<p>One way I test backups is that I keep a hot backup server for some of the databases that are not changing as frequently as others. I basically have SQL jobs that backup the databases from one server straight to the other server, then a restore is done, permissions are fixed and checkdb is run. Another way I test the backups  with some bigger databases is that I do restores to our staging server once a week or so, these restores take about 6 hours or so.</p>
<p>You need to test restoring full backups, you also need to test applying differential and transaction log backups. Ideally you want this automated.</p>
<p>Where do you store your backups? Next to the server in a bin are all the tapes? Bad idea. You need to store the backups offsite or be doing backups to the other datacenter. Your backups cannot be <em>only </em>in the same location as the server that you are backing up from.</p>
<p>See also this post by Ted Krueger for some more backup information: <a href="/index.php/DataMgmt/DBAdmin/MSSQLServerAdmin/backups-are-for-sissies">Backups are for sissies!!!</a></p>
<p>We have a 200 page or so document that explains in detail what needs to happen if we switch data centers or if we have to rebuild a whole data center in the case of a catastrophe. How long would it take you to rebuild a server, install SQL server, installing all the apps and making sure that all the permissions are correct&#8230;&#8230;.oh what&#8230;oh you didn&#8217;t think you needed to backup master and tempdb. Hopefully you have all your jobs and SSIS packages scripted out or backed up. What about the permissions and accounts, do you know all the accounts that you need to create in case you don&#8217;t have a master backup?</p>
<p>When you boss asks next time what you do all day, make sure to tell him or her that you are making sure that in the case of a catastrophe the company is back in business in an instant, it is part of your job and your duty to yourself and the company</p>
<p>That is all for day twenty-two of the <a href="/index.php/DataMgmt/DBProgramming/sql-advent-2012-here-is">SQL Advent 2012 series</a>, come back tomorrow for the next one, you can also check out all the posts from last year here: <a href="/index.php/DataMgmt/DataDesign/sql-advent-2011-recap">SQL Advent 2011 Recap</a></p>
]]></content:encoded>
			<wfw:commentRss>/index.php/webdev/business-intelligence/testing-your-backup-and-failover/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Backup to the cloud by using duplicati</title>
		<link>/index.php/itprofessionals/softwareandconfigmgmt/backup-to-the-cloud-by/</link>
		<comments>/index.php/itprofessionals/softwareandconfigmgmt/backup-to-the-cloud-by/#respond</comments>
		<pubDate>Thu, 24 May 2012 23:53:00 +0000</pubDate>
		<dc:creator><![CDATA[SQLDenis]]></dc:creator>
				<category><![CDATA[Book Review]]></category>
		<category><![CDATA[Software and Configuration Management]]></category>
		<category><![CDATA[amazon]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[cloud]]></category>
		<category><![CDATA[open source]]></category>
		<category><![CDATA[restore]]></category>
		<category><![CDATA[skydrive]]></category>

		<guid isPermaLink="false">/index.php/2012/05/backup-to-the-cloud-by/</guid>
		<description><![CDATA[I decided to give Duplicati a try to backup some of my stuff to the cloud. Duplicati is free/libre/open-source software (FLOSS). Duplicati works with Amazon S3, Windows Live SkyDrive, Google Drive (Google Docs), Rackspace Cloud Files or WebDAV, SSH, FTP&#8230;]]></description>
				<content:encoded><![CDATA[<p>I decided to give <a href="http://www.duplicati.com/">Duplicati</a> a try to backup some of my stuff to the cloud. Duplicati is free/libre/open-source software (FLOSS). Duplicati works with Amazon S3, Windows Live SkyDrive, Google Drive (Google Docs), Rackspace Cloud Files or WebDAV, SSH, FTP. </p>
<p>Duplicati uses Pre-Internet Encryption, you know that nobody can decrypt your files. When working with the cloud you have to be in TNO (Trust No One) mode. You never know what malicious person could be on the other side. Duplicati has built-in AES-256 encryption and backups can be signed using GNU Privacy Guard</p>
<p>Duplicati is available for the following platforms</p>
<p>Duplicati 1.3.2 for Windows &#8211; (32bit, msi)<br />
Duplicati 1.3.2 for Windows &#8211; (64bit, msi)<br />
Duplicati 1.3.2 for Mac OS X &#8211; (dmg)<br />
Duplicati 1.3.2 for Debian, Ubuntu, LinuxMint &#8211; (deb)<br />
Duplicati 1.3.2 for Fedora, RedHat, OpenSuSE &#8211; (rpm)<br />
Duplicati 1.3.2 &#8211; (Binaries, zip)<br />
Duplicati 1.3.2 &#8211; (Linux binary, tgz)</p>
<p>Let&#8217;s take a look at how easy it is to use. After you install Duplicati, run it. When you run it the first time a wizard will pop up. Pick select a new backup</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati01.PNG?mtime=1337909180"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati01.PNG?mtime=1337909180" width="506" height="379" /></a></div>
<p>A bunch of typical folders that hold your pictures, music, documents etc are shown by default. I decided to create a new folder and dump some pics in that folder</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati02.PNG?mtime=1337909180"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati02.PNG?mtime=1337909180" width="501" height="372" /></a></div>
<p>Next you give a password for your backups</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati03.PNG?mtime=1337909181"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati03.PNG?mtime=1337909181" width="506" height="370" /></a></div>
<p>Then you pick where you want to back this up to</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati04.PNG?mtime=1337909182"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati04.PNG?mtime=1337909182" width="511" height="340" /></a></div>
<p>I picked SkyDrive</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati05.PNG?mtime=1337909183"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati05.PNG?mtime=1337909183" width="506" height="372" /></a></div>
<p>On the advanced page, you can add custom filters, set up the frequency and add limits</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati06.PNG?mtime=1337909235"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati06.PNG?mtime=1337909235" width="510" height="376" /></a></div>
<p>On the next page you can now fill the details in for the stuff you picked in the previous page</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati07.PNG?mtime=1337909236"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati07.PNG?mtime=1337909236" width="505" height="377" /></a></div>
<p>Backup is running&#8230;.and it is done</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati08.PNG?mtime=1337909236"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati08.PNG?mtime=1337909236" width="472" height="412" /></a></div>
<p>I checked that there was something in the SkyDrive folder</p>
<p>Next I wiped out the folder on my PC and decided to do a restore. I ran the app, clicked on the wizard and picked the restore option</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati09.PNG?mtime=1337909237"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati09.PNG?mtime=1337909237" width="505" height="376" /></a></div>
<p>I picked restore from an existing backup</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati10.PNG?mtime=1337909238"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati10.PNG?mtime=1337909238" width="504" height="376" /></a></div>
<p>I picked the backup I wanted to restore</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati11.PNG?mtime=1337909271"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati11.PNG?mtime=1337909271" width="505" height="378" /></a></div>
<p>It is downloading the backup</p>
<div class="image_block"><a href="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati12.PNG?mtime=1337909271"><img alt="" src="/wp-content/uploads/blogs/ITProfessionals/Duplicati/Duplicati12.PNG?mtime=1337909271" width="507" height="377" /></a></div>
<p>I checked the folder and all the pictures that I wiped out were restored</p>
<p>It is pretty easy to use, I will test this out over the coming weeks to get more familiar with this product</p>
<p>Duplicati is an open source project, licensed under LGPL, the source can be downloaded here http://code.google.com/p/duplicati/source/checkout</p>
<p>Try it out and let me know what you think, the main product page is here: http://www.duplicati.com/</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/itprofessionals/softwareandconfigmgmt/backup-to-the-cloud-by/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>How much longer will the SQL Server database restore take</title>
		<link>/index.php/datamgmt/datadesign/how-much-longer-will-the/</link>
		<comments>/index.php/datamgmt/datadesign/how-much-longer-will-the/#comments</comments>
		<pubDate>Fri, 02 Sep 2011 11:12:00 +0000</pubDate>
		<dc:creator><![CDATA[SQLDenis]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[dmv]]></category>
		<category><![CDATA[dynamic management views]]></category>
		<category><![CDATA[restore]]></category>

		<guid isPermaLink="false">/index.php/2011/09/how-much-longer-will-the/</guid>
		<description><![CDATA[Frequently you will be asked how much longer a restore will take because someone needs to do something with that specific database that is restoring right now

Of course we all know that the RESTORE DATABASE command has the STATS n option, this will g&#8230;]]></description>
				<content:encoded><![CDATA[<p>Frequently you will be asked how much longer a restore will take because someone needs to do something with that specific database that is restoring right now</p>
<p>Of course we all know that the RESTORE DATABASE command has the STATS n option, this will give you the percentage completed. This is nice but it doesn&#8217;t tell you when it will complete and if someone else started the restore how will you know how long it will take in that case?</p>
<p>Fear not, here is a query that will tell you exactly how long</p>

<div class="bwp-syntax-block clearfix">
<div class="bwp-syntax-toolbar"><div class="bwp-syntax-control"><a href="javascript:;" class="bwp-syntax-source-switch" title="View Source Code"></a></div></div>
<div class="bwp-syntax-wrapper clearfix bwp-syntax-simple"><table class="tsql"><thead><tr><td colspan="2"  class="head">T-SQL</td></tr></thead><tbody><tr class="li1"><td class="ln"><pre class="de1">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="de1"><pre class="de1"><span class="kw1">SELECT</span>&nbsp; 
&nbsp; &nbsp; d.<span class="me1">PERCENT_COMPLETE</span> <span class="kw1">AS</span> <span class="br0">&#91;</span><span class="sy0">%</span>Complete<span class="br0">&#93;</span>,
&nbsp; &nbsp; d.<span class="me1">TOTAL_ELAPSED_TIME</span><span class="sy0">/</span><span class="nu0">60000</span> <span class="kw1">AS</span> ElapsedTimeMin,
&nbsp; &nbsp; d.<span class="me1">ESTIMATED_COMPLETION_TIME</span><span class="sy0">/</span><span class="nu0">60000</span> &nbsp; <span class="kw1">AS</span> TimeRemainingMin,
&nbsp; &nbsp; d.<span class="me1">TOTAL_ELAPSED_TIME</span><span class="sy0">*</span><span class="nu0">0.00000024</span> <span class="kw1">AS</span> ElapsedTimeHours,
&nbsp; &nbsp; d.<span class="me1">ESTIMATED_COMPLETION_TIME</span><span class="sy0">*</span><span class="nu0">0.00000024</span>&nbsp; <span class="kw1">AS</span> TimeRemainingHours,
&nbsp; &nbsp; s.<span class="kw1">text</span> <span class="kw1">AS</span> Command
<span class="kw1">FROM</span>&nbsp; &nbsp; sys.<span class="me1">dm_exec_requests</span> d 
<span class="sy0">CROSS</span> APPLY sys.<span class="me1">dm_exec_sql_text</span><span class="br0">&#40;</span>d.<span class="me1">sql_handle</span><span class="br0">&#41;</span><span class="kw1">as</span> s
<span class="kw1">WHERE</span> &nbsp;d.<span class="me1">COMMAND</span> <span class="sy0">LIKE</span> <span class="st0">'RESTORE DATABASE%'</span>
<span class="kw1">ORDER</span> &nbsp; <span class="kw1">BY</span> <span class="nu0">2</span> <span class="kw1">desc</span>, <span class="nu0">3</span> <span class="kw1">DESC</span></pre></td></tr></tbody></table></div>
<div class="bwp-syntax-source"><pre class="no-parse">SELECT	
    d.PERCENT_COMPLETE AS [%Complete],
    d.TOTAL_ELAPSED_TIME/60000 AS ElapsedTimeMin,
    d.ESTIMATED_COMPLETION_TIME/60000	AS TimeRemainingMin,
    d.TOTAL_ELAPSED_TIME*0.00000024 AS ElapsedTimeHours,
    d.ESTIMATED_COMPLETION_TIME*0.00000024	AS TimeRemainingHours,
    s.text AS Command
FROM	sys.dm_exec_requests d 
CROSS APPLY sys.dm_exec_sql_text(d.sql_handle)as s
WHERE  d.COMMAND LIKE 'RESTORE DATABASE%'
ORDER	BY 2 desc, 3 DESC</pre></div></div>

<p>Here is the output for a fairly large database restore that I started last night</p>
<div class="tables">
<table>
<tr>
<th>%Complete</th>
<th>	ElapsedTimeMin</th>
<th>	TimeRemainingMin</th>
<th>	ElapsedTimeHours</th>
<th>	TimeRemainingHours</th>
<th>	Command</th>
</tr>
<tr>
<td>78.6186</td>
<td>	766</td>
<td>	208</td>
<td>	11.03301576</td>
<td>	2.99693760</td>
<td>	RESTORE database SomeDB</td>
</tr>
<table>
<p> As you can see the query returns elapsed time both in hours and minutes, time remaining  both in hours and minutes and also the percentage complete</p>
<p>Hopefully this will help you with those nagging types&#8230;&#8230;.<br />
</table>
</table>
</div>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/how-much-longer-will-the/feed/</wfw:commentRss>
		<slash:comments>14</slash:comments>
		</item>
		<item>
		<title>SQL Server and High Availability</title>
		<link>/index.php/datamgmt/dbprogramming/sql-server-high-availability/</link>
		<comments>/index.php/datamgmt/dbprogramming/sql-server-high-availability/#respond</comments>
		<pubDate>Fri, 11 Jun 2010 17:57:31 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[disaster recovery]]></category>
		<category><![CDATA[dr]]></category>
		<category><![CDATA[ha]]></category>
		<category><![CDATA[high availability]]></category>
		<category><![CDATA[log shipping]]></category>
		<category><![CDATA[mirroring]]></category>
		<category><![CDATA[restore]]></category>
		<category><![CDATA[sql server 2008]]></category>
		<category><![CDATA[sql university]]></category>

		<guid isPermaLink="false">/index.php/2010/06/sql-server-high-availability/</guid>
		<description><![CDATA[Realistically, 100% is unachievable given the nature of computing.  There are needs for a SQL Server and Windows Server to be rebooted at least once a year.  This is to allow for updates on both SQL Server and Windows to be maintained.  So the ranking method we use for measuring high availability is the "nines" scale.  The five nines is a goal that most database administrators and teams set for their standards.  The five nines level is a height of availability that is truly an achievement and one to be proud of.]]></description>
				<content:encoded><![CDATA[<p><img src="/wp-content/uploads/blogs/DataMgmt/sqlu_logo.gif" alt="" title="" width="150" height="166" align="left" />
<p>Welcome to the last class of HA / DR week for SQL University.  It has been a great week discussing these topics with all of you.  We <a href="/index.php/DataMgmt/DBAdmin/sqlu-taking-a-break-for-recess">recapped</a> those classes in order to highlight the key points over the week yesterday.  So far we&#8217;ve covered a great deal but really have only scratched the surface of SQL Server features for HA and DR.  Today will be another scratch in the surface regarding the High Availability points for SQL Server.  Be sure to check the resources links through this article.  They will greatly add an extension to today and further build your knowledge of the vast amount of abilities we have at our disposal.  </p>
<p>Now that HA / DR week is completed, please take a moment to rate this week (and others) by filling out the <a href="https://spreadsheets.google.com/a/sqlchicken.com/viewform?hl=en&amp;formkey=dDBoSW02QldrTTc2dER3WVZheUlEX3c6MQ#gid=0">SQL University Course Evaluation</a> and select HA/DR Week.  </p>
<h2><strong>What is HA?</strong></h2>
<p>High Availability (HA) for SQL Server can be defined in one sentence: Keep data available 100% of the time.   That really is the objective of HA and nothing short of that.</p>
<p>Realistically, 100% is unachievable given the nature of computing.  There are needs for a SQL Server and Windows Server to be rebooted at least once a year.  This is to allow for updates on both SQL Server and Windows to be maintained.  So the ranking method we use for measuring high availability is the &#8220;nines&#8221; scale.  The five nines is a goal that most database<br />
administrators and teams set for their standards.  The five nines level is a height of availability that is truly an achievement and one to be proud of.  </p>
<p>The table below illustrates the availability calculation.  This table has been used for quite some time and has been adopted by IT teams as the measurement for downtime. Downtime equates to uptime (depends on if you are talking to a CTO, CEO or an IT manager how to phrase it).  </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/ha_1.gif" alt="" title="" width="547" height="310" /></div>
<p align="center"><i><a href="http://en.wikipedia.org/wiki/High_availability">High Availability</a></i>
</p>
<p>The six nines goal is almost unachievable but can be done in some infrastructures.  Why are the six nines almost out of reach?  Given that a reboot on Windows takes a little under 2 minutes, reaching only a 31 seconds downtime goal becomes a bit farfetched.  Even if clustering and mirroring is setup, the failover times still will add up to around 15 seconds per failover.  The five nines is a goal that you can achieve with a mid-level installation and still meet the needs to keep your systems up to date yearly.  </p>
<p>Windows clustering has come a long way over the last few versions.  Windows Server 2008 in particular has a sound clustering service that is more stable than its predecessors (In this author’s opinion).  Windows clustering will give you the ability to automatically failover due to hardware failures and most Operating System failures.  The concept of this ability alone allows the setup to be valuable to HA strategies.  Clustering consists of two or more physical servers.  These physical servers act like a partnership and are always in communication with each other ensuring that their specific roles are being met.  These servers can be in a state of Active or Passive but one must be in a Active state at all times.  </p>
<p>A benefit to Windows Clustering is the storage location of the actual databases.  This would be located on disk (NAS, SAN etc…) outside the physical servers.  Given this, we enhance HA by having the ability to replicate the disk along with the safety of the cluster acting in partnership with DR.  Mirroring can be added to the cluster fully enhancing the entire landscape of the true HA strategy.  Windows Server 2008 Clustering all together is a straight forward setup and deployment but will take added knowledge and research to ensure the cluster is configured to the specifics of each environment.  With any cluster, there is complexity in the configurations and ensuring the two servers stay in sync for programs, hardware and services but administrative tasks are much lighter than they were in the past.  </p>
<h2><strong>Maintenance and downtime </strong></h2>
<p>Downtime can be caused by any type of disruption to the availability of data.  Maintenance tasks like index rebuilding, updates and ETL loads all fall into that category.  When doing large index rebuild tasks, the tables may become unavailable to the users.  In the database view of this, they are simply locked at this time.  This equates to almost complete loss of the ability to access them though and thus, a failure in our HA strategy.  These tasks should be planned out careful to prevent this type of failure in HA.  </p>
<p>Windows and SQL Server updates must be maintained.  This is not optional.  Every update should be analyzed carefully and the full extent of its impact on the systems taken into consideration.  Some updates may cause SQL Server services to stop or other supporting services that are needed to maintain availability.  If they are, the downtime should be planned carefully to prevent the HA landscape from doing its job of preventing loss and availability.  That means pausing mirroring or failing over nodes in a cluster to retain the availability of data services as much as possible.  </p>
<h2><strong>Geo-Clustering </strong></h2>
<p>Geo-Clustering allows us to plan High Availability across geographically located sites.  This is achieved through replication of disk to each site and essentially mirrors each system to the next.  This HA option of high availability as a hefty price tag on hardware and networking abilities.  Recently, Paul Randal (<a href="http://sqlskills.com/blogs/paul/">Blog</a> | <a href="http://twitter.com/paulrandal">Twitter</a>)published a White paper <a href="http://blogs.msdn.com/b/tommills/archive/2010/06/02/new-sql-server-2008-r2-high-availability-whitepaper-published.aspx"><i>Proven SQL Server Architectures for High Availability and Disaster Recovery</i></a>.  This went over HA setups (and DR) that are in use in businesses now and proven to work effectively.  This white paper goes into detail on actual in-place configurations in Geo-Clustering as well.  This class lucks out on the timing of this publication by us being able to link to it as a resource in understanding large business HA and Geo-Clustering.  </p>
<h2><strong>Database Mirroring</strong></h2>
<p>Database mirroring was introduced in SQL Server 2005 Standard and Enterprise editions.  Prior to the introduction of database mirroring, there was difficulty in achieving HA with SQL Server.  Clustering and disk replication were options but still limited.  With database mirroring the options for HA landscapes opens up greatly.  This doubled with the advancements in Windows Clustering added a great deal of enterprise abilities to SQL Server.  </p>
<p>Our landscape for data services can take on a new form for HA and exist without Windows Clustering as well.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/ha_2.gif" alt="" title="" width="405" height="386" /></div>
<p>What this type of HA landscape provides is even more stripped down overhead in administrative burdens and we have cost savings opportunities to assist in providing a HA solution.</p>
<p>In the diagram above, the landscape consists of two physical database servers.  Those database servers a standalone installations and active as each unique entity to the infrastructure.    Alias naming can be added to the scenario to ensure the clients access the database servers in the event of a failover.  Alias naming can be forgone with the changes in recent years to the ability of connections from applications to allow for a failover specification.  </p>
<pre>Data Source=myServerAddress;Failover Partner=myMirrorServerAddress;Initial Catalog=myDataBase;Integrated Security=True;</pre>
<p align="center"><i>Resource:  <a href="http://www.connectionstrings.com/sql-server-2008">ConnectionStrings.com</a></i></p>
<p>We can see in the example connection string above the Failover parameter which allows us to make applications much smarter as opposed to recent years with actually developing tests in code to determine the availability of data services.  </p>
<p>Another addition is added to the diagram that is provided with Enterprise Edition.  That is snapshots capabilities.  In this landscape (and clustering) a path can exist from another reporting server to snapshots that are taken of the mirror.  This reporting solution is a huge benefit to the entire structure of the data services by lower the activity on the OLTP side.  Reporting activity has always been an historical hardship of database administrators and keeping services for both reports and operations from preventing each other’s ability to serve the business.  </p>
<h2><strong>Working with database mirroring</strong></h2>
<p>To go deeper into database mirroring, we will work from a recent setup located at, <a href="/index.php/DataMgmt/DBAdmin/sql-server-2008-mirroring-setup">Mirroring Hands On with Developer Edition</a>.  In this setup, Developer edition is used to provide all of the possible configurations provided in mirroring.  The setup and purchase of Developer Edition is highly recommended to become familiar with these features.</p>
<p>The primary features we lose when moving to Standard Edition are Asynchronous<br />
Mirroring and Snapshot capabilities of the Mirror.  One important factor of snapshot abilities is, the snapshot cannot occur while the mirroring is in a synchronizing state or applying transactions.  In an asynchronous setup (High performance), depending on the number of transactions, this state can be harder to schedule for snapshot creations.  These two features are a large part of mirroring and the flexibility in configuring it.  Weigh in the needs of mirroring greatly and the loss of these features when not putting the budget in for Enterprise Edition.</p>
<p>After going through the developer setup in the link above, we can start to look at options in mirroring and things to watch for.</p>
<h2><strong>Operating Modes</strong></h2>
<p>
Database Mirroring allows for three operating modes.</p>
<ul>
<li>High Availability</li>
<li>High protection</li>
<li>High performance</li>
</ul>
<p>In order to achieve HA with out of the box SQL Server, we enlist in the High Availability operating mode.  This setup runs in a synchronous set of operations that apply transactions to the logs on both the mirror and principal prior to returning success commits back to the applications.  The mirroring landscape consists of three physical servers, the principal, the mirror and the witness.</p>
<blockquote><p><span class="MT_red">Note: The witness can be any edition of SQL Server but the principal and mirror must be the same edition.  The principal can be a previous version than the mirror but this is only recommended in upgrade methods.  The witness can be located on the mirror but this is not recommended due to the chance of losing the mirror and thus, losing the witness.</span></p></blockquote>
<p>The remaining two operating modes take the witness and automatic failover out of the landscape.  This is not optimal in achieving HA.  Without automation and the active ping in mirroring, the downtime is greater and human interaction is required.</p>
<h2><strong>What can go wrong?</strong></h2>
<p>The worst thing that can happen in database mirroring is losing the network behind it.  This can cause a complete loss of data services.<br />
Another known problem is referred to as Split-brain.  In this situation, both the Principal and Mirror have taken on the role of a principal.  In the landscape in which alias’s are utilized this can be a severe problem to accessing the databases.</p>
<p>Corruption (page errors) that occurs on the Principal will follow to the mirror.  Database mirroring does enlist in <i>automatic page-repair</i>.  This is now available in SQL Server 2008 and SQL Server 2008 R2.  The automated page-repair will attempt to repair any page errors that are sent to the mirror by requesting the transactions (or fresh copy of the pages) again from the principal.  This repair will work in some scenarios but if the corruption is too great and requires any type of loss in data, an error will persist.</p>
<p>When a mirror does receive errors (even if capable of fixing), the mirror goes into a suspended state.  This suspended state will persist until the error is resolved.  If the error cannot be resolved by automation of the mirroring abilities, the mirror will stay in the suspended state.  This usually will cause a clean setup of the mirroring landscape.  In a worst case scenario, the errors that came from the principal are great enough that repairing the principal is the priority. </p>
<p>Transaction log growth must be maintained in mirroring.  Database mirroring requires the databases to be in Full recovery.  This means that the transaction logs must be maintained and sized properly.  If they are not, the logs will grow and at some point the need to shrink them will undoubtedly occur.  These operations are not allowed in a mirror though and the mirror will need to be broken to do them.  It is highly recommended to take all the necessary steps to maintain a full recovery state.  Keep this in mind while performing index maintenance especially.</p>
<h2><strong>Final Thoughts</strong></h2>
<p>The combination of Windows Clustering and Database Mirroring will allow for the objective of data services to meet High Availability goals.  With Database Mirroring, we can achieve HA with the operating mode of High Availability while not needing a clustered environment (although recommended).  This landscape lowers cost and allows for flexibility in some applications that may have particular needs outside of the clustering scope.</p>
<p>Achieving High Availability should be seamless to the user community.  Failover situations must be automated in order to retain HA and limited to no exposure to the users at the time a failover is needed.  Working with the applications and vendors to achieve these goals is sometimes required but often fully achievable.</p>
<p>This concludes the SQL University DR/HA Week.  I hope you enjoyed talking about the topics and look forward to hearing all of your feedback on taking what we’ve discussed while planning your own data securing strategies with HA and DR.  As with all of the SQL University weeks, your feedback is greatly appreciated so we know how we are doing.  When you have a moment please take a moment to fill out the <a href="https://spreadsheets.google.com/a/sqlchicken.com/viewform?hl=en&amp;formkey=dDBoSW02QldrTTc2dER3WVZheUlEX3c6MQ#gid=0">SQL University Course Evaluation</a> and select HA/DR Week.  </p>
<p>Thank you for attending once again and thanks to Jorge Segarra (<a href="http://sqlchicken.com/">Blog</a> | <a href="http://twitter.com/sqlchicken">Twitter</a>) for allowing me the pleasure of discussing all of these topics with you.  </p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbprogramming/sql-server-high-availability/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>SQL University &#8211;  Recess time!</title>
		<link>/index.php/datamgmt/dbprogramming/sqlu-taking-a-break-for-recess/</link>
		<comments>/index.php/datamgmt/dbprogramming/sqlu-taking-a-break-for-recess/#respond</comments>
		<pubDate>Thu, 10 Jun 2010 17:10:13 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[disaster recovery]]></category>
		<category><![CDATA[dr]]></category>
		<category><![CDATA[ha]]></category>
		<category><![CDATA[high availability]]></category>
		<category><![CDATA[log shipping]]></category>
		<category><![CDATA[mirroring]]></category>
		<category><![CDATA[restore]]></category>
		<category><![CDATA[sql server 2008]]></category>
		<category><![CDATA[sql university]]></category>

		<guid isPermaLink="false">/index.php/2010/06/sqlu-taking-a-break-for-recess/</guid>
		<description><![CDATA[The recess bell just rang for SQL University HA / DR classrooms.  While all of the SQL kiddies are running around the playground and playing with the things they have learned over this semester, the chalkboard is going to get a workout so when they get back, they can take the notes they slacked on earlier.]]></description>
				<content:encoded><![CDATA[<p>
The recess bell just rang for SQL University HA / DR classrooms.  While all of the SQL kiddies are running around the playground and playing with the things they have learned over this semester, the chalkboard is going to get a workout.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/bart.gif" alt="" title="" width="628" height="339" /></div>
<p align="center"><i>Image courtesy of <a href="http://www.addletters.com/pictures/bart-simpson-generator/233605.htm">Bart Simpson Chalkboard Generator</a> and <a href="http://toadworld.com/BLOGS/tabid/67/EntryID/543/Default.aspx">linkback to Jeff Smith excellent article</a></i></p>
<p>
Over the last week we’ve gone over a lot regarding HA and DR.  The first day we defined situations and the key factors that are needed to be successful in obtaining secure data services and high availability of those data services.  Any of these two strategies to protect our data against disasters, local and remote; always start with the definitions required to plan out the implementation.  We learned together that just throwing things like log shipping into the mix may not truly give us the protection we need.  This would happen if we leave important business entities out of our planning and document how our systems would come back from disasters.
</p>
<p>We done this over the week by showing the features that SQL Server has to offer without much added cost.  When budget is available, we also discussed briefly landscapes like Geo-clustering, SAN replication and truly next to real-time mirrored data centers for recovering in the event of disasters.  These were terms in passing so we didn’t venture off the scope of each class but their importance is there nonetheless. </p>
<p>
We defined a list of the important notes we set off to discuss that effect the decisions of how we can accomplish HA and DR</p>
<ol>
<li>Size of databases</li>
<li>Network capabilities</li>
<li>Budget</li>
<li>Features available per edition of SQL Server</li>
<li>Maintenance load</li>
<li>Allowable downtime</li>
<li>Personnel resources required</li>
<li>Initial setup downtime</li>
<li>Will DR fit into the HA strategy?</li>
<li>Documentation – knowledge transfer</li>
<li>Can we test this?</li>
</ol>
<p>Automation of Disaster and Recovery played an important part in our discussions.  Do we automate is the key to decide in your unique strategies.  DR typical is a manual failover process while HA only becomes HA when automation is playing into the events of a localized disaster.</p>
<p>SQL Server backups were stressed on day two as the foundation of all that is DR (and HA recovery).  With careful planning and testing on both HA and DR, we achieve secure and always available data services.  But when even that strategy fails, we must turn to our backups to recover both the data and the DR and HA strategies themselves.  </p>
<p>Today we discussed log shipping as a cost effective disaster and recovery method.  Log shipping is a great alternative when budgets are low.  Problems do arise in the effectiveness when our data services are larger than our check books though.  All-in-all, log shipping can provide security in a DR situation and leaving it as a possibility when strategizing is always good. </p>
<p>So now we move to our last day and we will discuss High Availability.  The class will show database mirroring in our newer versions of SQL Server.  We’ll discuss things like split-brain scenarios in mirroring and certificate usage so we can mirror across different domains.  Finally, we need to briefly discuss SAN replication and geo-clustering again.  Although we will be brief on this topic, we want to emphasize them because when the budget is there, nothing really matches them for real-world HA.</p>
<p><strong>Recess is now over so let’s get back to work!</strong></p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbprogramming/sqlu-taking-a-break-for-recess/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Log Shipping for cheap DR</title>
		<link>/index.php/datamgmt/dbprogramming/log-ship-to-dr-sqlu/</link>
		<comments>/index.php/datamgmt/dbprogramming/log-ship-to-dr-sqlu/#comments</comments>
		<pubDate>Thu, 10 Jun 2010 10:50:34 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[backup]]></category>
		<category><![CDATA[disaster recovery]]></category>
		<category><![CDATA[dr]]></category>
		<category><![CDATA[ha]]></category>
		<category><![CDATA[high availability]]></category>
		<category><![CDATA[log shipping]]></category>
		<category><![CDATA[mirroring]]></category>
		<category><![CDATA[restore]]></category>
		<category><![CDATA[sql server 2008]]></category>

		<guid isPermaLink="false">/index.php/2010/06/log-ship-to-dr-sqlu/</guid>
		<description><![CDATA[Welcome to day three of HA and DR week of SQL University.  Today we are going to look at cheap DR.  Yes, setting up DR can be inexpensive.  The best part of this strategy is it comes along with most of the editions of SQL Server.  The method is Log Shipping.  Log shipping (LS) has a bad name in the Disaster / Recovery (DR) world.  There are concerns with the ability to fail back to primary sites in the case of disasters, and LS is often thought of as a maintenance intense setup along with file mess.  Today’s class will go over some methods to handle these and other concerns, along with the simplicity of configuring and monitoring LS in SQL Server 2008 (R2).]]></description>
				<content:encoded><![CDATA[<p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/sqlu_logo.gif" alt="" title="" width="150" height="166" align="left" /></div>
<p>Welcome to day three of HA and DR week of SQL University.  Today we are going to look at cheap DR.  Yes, setting up DR can be inexpensive.  The best part of this strategy is it comes along with most of the editions of SQL Server.  The method is Log Shipping.  </p>
<p>Log shipping (LS) has a bad name in the Disaster / Recovery (DR) world.  There are concerns with the ability to fail back to primary sites in the case of disasters, and LS is often thought of as a maintenance intense setup along with file mess.  Today’s class will go over some methods to handle these and other concerns, along with the simplicity of configuring and monitoring LS in SQL Server 2008 (R2).
</p>
<p></p>
<h2><strong>What is Log Shipping</strong></h2>
<p>
Log Shipping consists of three events.  Backup transaction log, Copy remotely and Restore to subscriber(s).  Any one primary (publisher or the logs) can have one or more secondary databases (subscriber to the logs).  </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_7.gif" alt="" title="" width="500" height="444" /></div>
<p>When configuring Log Shipping, all of the configuration settings are held in the MSDB database. </p>
<p>These tables consist off the following</p>
<blockquote><p>log_shipping_primary_databases<br />
log_shipping_primary_secondaries<br />
log_shipping_monitor_primary<br />
log_shipping_monitor_history_detail<br />
log_shipping_monitor_error_detail<br />
log_shipping_secondary<br />
log_shipping_secondary_databases<br />
log_shipping_monitor_secondary</p></blockquote>
<p>The system procedures for configuring log shipping are located in the master database.</p>
<blockquote><p>sp_add_log_shipping_monitor_jobs<br />
sp_add_log_shipping_primary<br />
sp_add_log_shipping_secondary<br />
sp_create_log_shipping_monitor_account<br />
sp_delete_log_shipping_monitor_info<br />
sp_delete_log_shipping_monitor_jobs<br />
sp_delete_log_shipping_primary<br />
sp_delete_log_shipping_secondary<br />
sp_get_log_shipping_monitor_info<br />
sp_log_shipping_get_date_from_file<br />
sp_log_shipping_in_sync<br />
sp_log_shipping_monitor_backup<br />
sp_log_shipping_monitor_restore<br />
sp_remove_log_shipping_monitor_account<br />
sp_update_log_shipping_monitor_info</p></blockquote>
<p>
When Log Shipping is enabled and configured, all subscribers must be in either Recovering or Read-Only (Standby) status.  Placing a subscriber into a Read-Only mode is common, but when a log is applied to the subscriber, all connections must be closed.  This is due to the subscriber database being required to go into recovering so the log can be applied.  Often, subscribers are used for reporting so considering the state of connectivity to the database is critical in being effective for availability.</p>
<p>Log shipping has some definite advantages on its side for being used in DR.  The overhead of the processing (backup, copy and restore) can be managed and, if thought through well, can leave a very small footprint on the normal operations of the database servers.  Maintenance is very minimal for both setup and administration.  Most database administrators already work closely with backup and restore, so log shipping is easy to learn.  The major cost in log shipping is disk for storage, a secondary SQL Server and the network backbone to handle the files moving across the lines.  This, compared to some DR cost overhead, is very minimal.
</p>
<p>
In the following steps we will set up log shipping completely on a local default SQL Server instance.  </p>
<blockquote><p><span class="MT_red">Note: as always, database size is a factor.  Databases in the Terybyte range can easily be logged shipped given the resources behind it.  Steps that are outlined will change greatly in time of execution due to the size of a database.  An initial restore for one will take some time at first.</span>  </p></blockquote>
<h2><strong>Setting up Log Shipping with SSMS</strong></h2>
<p>
A few things should be prepared prior to configuring Log Shipping. </p>
<ul>
<li>Share location for the log backups on the publisher</li>
<li>Share location for the log backups to be copied to on the subscribers</li>
<li>Security setup for accessing these shares (Agent account by default)</li>
</ul>
<p>Log Shipping can be setup completely using T-SQL, but in the push for the “point, click and run” theory of SSMS, we will use it to setup, configure and get our lab running.</p>
<p>Log shipping is available in every edition of SQL Server but Express.  In order to show our setup we will be using Developer Edition.  Developer Edition is available for a very low cost and is identical in features to Enterprise.  We will be using the AdventureWorks database to setup Log Shipping.  If you do not have this database, you can download it here.
</p>
<ol>
<li>Open SSMS and connect to your SQL Server.</li>
<li>Expand the databases tree, right click AdventureWorks and select properties</li>
<li>Select Transaction Log Shipping on the right</li>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_1.gif" alt="" title="" width="628" height="561" /></div>
<p>By default, Log Shipping is disabled on each database.  In order to go further, we need to check “Enable this as a primary database in a log shipping configuration”.  This will set the transaction log backups available for us to open and configure.</p>
<blockquote><p><span class="MT_red">Note: If you have other transaction log backups running, they should be turned off prior to starting the new Log Shipping plans.</span></p></blockquote>
<li>Click the &#8220;Backup Settings&#8221; button to open the configuration wizard.</li>
<p>Earlier we mentioned preparing for Log Shipping and the shares required.  You can use admin shares (e.g. \onpnt_xpsd$) but this isn’t recommended as the admin shares should be for administrative purposes only.  For our setup we will be using the following for processing backups:</p>
<p>\onpnt_xpspub_logs<br />
\onpnt_xpssub_logs</p>
<li>Enter your share into the “Network path to backup folder” field</li>
<p>For now, we will leave the default 72 hours to retain log backups.  </p>
<blockquote><p><span class="MT_red">Note: the retention of the log backups must be taken into consideration for recovery from backups.  Take into consideration the retention of other Full and Differential backups when setting this removal option.  You do not want to delete log backups that could be required to recover to a point in time by restores.</span></p></blockquote>
<p>The log shipping configuration will add a SQL Server Agent job that will monitor the thresholds set when configuring them.  If the table log_shipping_monitor_primary shows a backup date greater than the backup thresh hold value, an error will be raised in SQL Server.   In order to actively be notified, operators need to be setup on the agent and the job so you will receive these errors as they occur. </p>
<p>I would not recommend leaving the default Job name.  Make use of this name so you can easily find the job and know what it is for.  If you work on SQL Servers that have hundreds of jobs or Job Servers, using meaningful names in your environment make maintenance much faster and easier on everyone.</p>
<p>The next step is to determine the interval of the log backups.  The default 15 minutes is common but in a high-transaction database, 15 minutes can mean severe loss of data in a disaster.  I recommend really putting some thought into this setting.  Ensure you do not hurt performance with backups tripping over themselves or occurring so often they cause problems.  At the same time, ensure that you are protecting against the least amount of loss the business will accept.</p>
<p>Leave the default 15 minutes for now.  If you want to alter this schedule, click the Schedule button and the SQL Agent scheduling window will come up.  </p>
<p>To show compression, next to &#8220;Set backup compression&#8221;, select Compress Backup.  If you are on an earlier version than 2008 R2 and any edition other than Enterprise or Developer, this option will not be available.  SQL Server 2008 R2 allows compression in Standard and Enterprise (including Developer).  Pre-R2, only Enterprise and Developer have this option. </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_2.gif" alt="" title="" width="628" height="646" /></div>
<li>Click OK after ensuring everything is completed as shown above.</li>
<p>The next step in the process is to set any subscribers and monitoring servers if you use them outside the publisher.  In preparation you can restore a full backup and bring the tail log into the AdventureWorks subscriber database.  This can also be done from the subscriber steps.</p>
<li>Click Add under the Secondary databases</li>
<li>Click Connect and connect to the instance you want to Log Ship to</li>
<li>On the Initialize Secondary Database tab, select &#8220;Yes, generate a full backup of the primary…&#8221;</li>
<p>This will back the AdventureWorks database up and restore it as the database we specify to be the subscriber.   Ensure if you do this lab on a single SQL Server to change the name of the Secondary database to something other than the default of the primary.  </p>
<li>Click the restore options and ensure the data and log files go into the correct directories per your disk configurations. </li>
<li>Select the Copy Files tab and enter the share we created earlier (\onpnt_xpssub_logs)</li>
<p>We can leave the default schedule to restore again, but I still recommend changing the job name to a something more meaningful and easy to read </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_3.gif" alt="" title="" width="628" height="543" /></div>
<li>Click the Restore Transaction Log tab and select Standby Mode and Disconnect user in the database when restoring backups.  This will be required to prevent restore problems.</li>
<li>Click OK and OK again to save all of our configurations.  </li>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_4.gif" alt="" title="" width="628" height="551" /></div>
<p>After clicking OK, a dialog will be shown while the backup and restore of AdventureWorks runs.  The SQL Agent jobs that will control log shipping will also be created after these steps succeed. </p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_5.gif" alt="" title="" width="628" height="334" /></div>
<p>Once the restore is done and logs have shipped, you will start to notice them moving in the publication and subscriber shares</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_6.gif" alt="" title="" width="628" height="282" /></div>
</ol>
<p>Now that LS is running we can look into the process and logging of the events.  The log_shipping_monitor_history table is extremely useful for validating the entire process between the instances.  The Message column has logged information that will explain in detail the process that is occurring:</p>
<pre>select [message] from log_shipping_monitor_history_detail</pre>
<p>Results:</p>
<blockquote><p>Starting transaction log copy. Secondary ID: &#8217;07af9839-f962-40db-9392-2107e9cc2053&#8242;<br />
Retrieving copy settings. Secondary ID: &#8217;07af9839-f962-40db-9392-2107e9cc2053&#8242;<br />
Retrieved copy settings. Primary Server: &#8216;ONPNT_XPS&#8217;, Primary Database: &#8216;AdventureWorks&#8217;, Backup Source Directory: &#8216;\onpnt_xpspub_logs&#8217;, Backup Destination Directory: &#8216;\onpnt_xpssub_logs&#8217;, Last Copied File: &#8216;<none>&#8216;<br />
Copying log backup files. Primary Server: &#8216;ONPNT_XPS&#8217;, Primary Database: &#8216;AdventureWorks&#8217;, Backup Source Directory: &#8216;\onpnt_xpspub_logs&#8217;, Backup Destination Directory: &#8216;\onpnt_xpssub_logs&#8217;<br />
Checking to see if any previously copied log backup files that are required by the restore operation are missing. Secondary ID: &#8217;07af9839-f962-40db-9392-2107e9cc2053&#8242;<br />
The copy operation was successful. Secondary ID: &#8217;07af9839-f962-40db-9392-2107e9cc2053&#8242;, Number of log backup files copied: 0<br />
Starting transaction log copy. Secondary ID: &#8217;07af9839-f962-40db-9392-2107e9cc2053&#8242;</none></p></blockquote>
<p>Another extremely practical usage of these tables is, in the event of a disaster, being able to later analyze data that may have been lost in log backups that did not get copied to secondary servers.  The log_shipping_secondary has a column, &#8220;last_copied_file&#8221;.  This column has helped me determine exactly where a subscribing database is at in the restores several times in the past.    </p>
<p>SSMS and built in reporting already available also provides us with a great way to monitor conditions of log shipping.  Right click the database server in SSMS, scroll to reports and in standard reports, click the Transaction Log Shipping Status.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_9.gif" alt="" title="" width="500" height="406" /></div>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/logship_10.gif" alt="" title="" width="1007" height="270" /></div>
</p>
<h2><strong>Catches but not pitfalls</strong></h2>
<p>
Now that log shipping is setup we should discuss some pitfalls to watch for.</p>
<p>One big problem that comes up with any fully logged database is maintenance tasks.  Index maintenance is a prime example.  The growth in logging on the transaction log that happens from index maintenance while in Full recovery is large.  Once this maintenance and logging begins, the log grows and this means the backups grow as they do.  Coming up with the best time to do these types of maintenance tasks and the interval in which you should log ship the transaction logs is critical for this.  If you have a 10GB log file and you rebuild a 5GB Index, it will take the log space to do the rebuild.  This could cause growth in the log which is something we don’t really want happening a lot.  So if the scheduled log backup is set to keep the free space down in the log during these tasks and normal operations, you can manage the logs very well and keep them in check while performing to the best they can.</p>
<p>
Networks will need to be able to handle the files moving.  Imagine if you start to copy a 10GB file over the regular LAN that users are connected to and working on.  This happens in regular offices often.  Users copy large files down or up to user home directories and it slows the entire network.  The only way to clean it up is to stop the copy or wait for it to finish.  This can be the same problem if the network isn’t configured to handle it.  Meet with the network administrators and make sure everyone knows the traffic that will be added to the network. </p>
<p>
With any database that is shipped, mirrored or restored to another site, remember that logins, SQL Agent Jobs, configurations outside of the databases and objects such as endpoints or linked servers will not be sent.  These must be done outside of the normal tasks.  SSIS can assist in this with the use of SMO or the Transfer Server Objects Task.  PowerShell can also bring in a useful container to work these tasks on schedules.  It is a good idea to move these as SQL script files to the offsite server and apply them.  Test and test this often.</p>
<h2><strong>Bell rang!</strong></h2>
<p>Log shipping is a quick and great method for small to mid-size databases or databases that have a good foundation and planned strategy for dealing with the pitfalls of large logs.  Administration is light and monitoring is very well done and built into SQL Server for use.  There are added benefits of LSN tracking in the logs and file monitoring as well.  The log shipping system tables have a wealth of information in them that can be used for other tasks.  </p>
<p>When planning your own DR strategy, leave log shipping on the list of options.  The cost factor and resource utilization may have it in the lead for being a good choice for your safety measures with Disaster / Recovery.  </p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbprogramming/log-ship-to-dr-sqlu/feed/</wfw:commentRss>
		<slash:comments>6</slash:comments>
		</item>
	</channel>
</rss>
