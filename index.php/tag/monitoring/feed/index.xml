<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>monitoring &#8211; LessthanDot</title>
	<atom:link href="/index.php/tag/monitoring/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>A Technical Community for IT Professionals</description>
	<lastBuildDate>Sat, 09 Mar 2019 12:50:36 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.6.1</generator>
	<item>
		<title>Monitoring and Logging as a Service &#8211; Introduction</title>
		<link>/index.php/enterprisedev/instrumentation/monitoring-and-logging-as-a-service/</link>
		<comments>/index.php/enterprisedev/instrumentation/monitoring-and-logging-as-a-service/#comments</comments>
		<pubDate>Fri, 29 Jun 2012 10:03:00 +0000</pubDate>
		<dc:creator><![CDATA[Eli Weinstock-Herman (tarwn)]]></dc:creator>
				<category><![CDATA[Instrumentation]]></category>
		<category><![CDATA[logging]]></category>
		<category><![CDATA[monitoring]]></category>
		<category><![CDATA[operations]]></category>

		<guid isPermaLink="false">/index.php/2012/06/monitoring-and-logging-as-a-service/</guid>
		<description><![CDATA[Monitoring [in IT] sucks and I am probably more critical of its state than most IT people. Over the next few posts I'm going to integrate a sample application with several logging and data services, evaluating them against my own needs and expectations. Besides the comparison, and perhaps more importantly, the examples will show how easy it is to instrument our applications and start getting visibility into what is actually happening behind the scenes.]]></description>
				<content:encoded><![CDATA[<p><a href="http://lusislog.blogspot.com/2011/06/why-monitoring-sucks.html" title="Why Monitoring Sucks">Monitoring [in IT] sucks</a> and I am probably more critical of its state than most IT people. Over the next few posts I&#8217;m going to integrate a sample application with several logging and data services, evaluating them against my own needs and expectations. Besides the comparison, and perhaps more importantly, the examples will show how easy it is to instrument our applications and start getting visibility into what is actually happening behind the scenes.</p>
<h2>Monitoring in IT is Behind</h2>
<p>Ten years ago I regularly worked with monitoring systems that recorded thousands of data points per second. They ran on beige desktop systems shoved in network closets, dell towers that were beginning to leave the beige behind, and 2U and 4U servers. They communicated through 1/10 switch ports, often fixed at 1 Mbps. They had to understand 10&#8217;s to 100&#8217;s of proprietary data protocols. They ran on IDE drives, storing years worth of data despite the drives barely pushing 100GB.</p>
<p>And yet they were recording thousands of data points per second, in real time, and storing it for historical analysis. They drove highly customizable interfaces supporting complex animations, graphs, meters, drag and drop design, and custom controls. Statistical analysis tools were available to analyze data from a dynamic number of sources across a user defined number of metrics in real time. Alerting systems provided email, SMS, and pager alerts while monitoring fixed alarm values, statistical anomalies, and trend-based alerting that alerted based on the projected state. Then there were the external applications, APIs, excel plugins, &#8230; Ten years ago.</p>
<p>This was the state of monitoring in the manufacturing world a decade ago, running on hardware and operating systems less capable than most of the cellphones we carry today. </p>
<h2>But IT Demand is Continuing to Grow</h2>
<p>In IT, the most mature market for monitoring and data collection is infrastructure (systems and network). But &#8220;most mature&#8221; is <a href="http://lusislog.blogspot.com/2011/06/why-monitoring-sucks.html" title="Why Monitoring Sucks">still behind</a>. The fact that so many systems barely have the concept of recipes for common monitoring targets, or lock our data behind such limited interfaces, leaves most of the commercial options closer to 15 years behind. </p>
<p>Software development lags even farther behind. </p>
<p>The current state of monitoring for most of the application world hovers between nonexistent and barely proactive. I have seen as many applications lacking even basic error logging as I have those with it, and the presence of even regular daily or weekly state checks has been even less prevalent. The state of application monitoring can be summed up as &#8220;We usually know when the system breaks, sometimes we get those messages immediately, and sometimes we actually read them&#8221;. </p>
<p>This market is beginning to emerge more, and hopefully that movement will start driving more tool development. Somewhere between fixed value monitoring and reporting on unstructured log data, we are building more businesses that value transparency in our application behavior. And even though most of the tools are better suited to infrastructure or basic log parsing, there is still an enormous potential in that first level of visibility beyond logging to a local file or an email address.</p>
<h2>Tools to be Reviewed</h2>
<p>Over the next couple posts, I&#8217;ll introduce some sample code that integrates an application with several logging services. The application to be instrumented is an ASP.net MVC 3 application that has a very basic view with links to a short and longer running action. Most of the services to be reviewed are built around managing unstructured log data, which isn&#8217;t a perfect fit, but does give us a lot of capability very cheaply.</p>
<p>The goal is to capture data consistently with as little impact to the code and performance of the original application as possible. Once data has been captured we then want to make it usable, creating some useful searches and visualizations to generate value from that data. </p>
<p>As a secondary goal, I hope the simplicity of the example and integration code will inspire more developers to start adding transparency to their code, not just for the benefit it will bring them, but also to put more pressure on tooling to improve.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/enterprisedev/instrumentation/monitoring-and-logging-as-a-service/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Diagnostic Manager &#8211; Custom Alerts</title>
		<link>/index.php/datamgmt/dbadmin/mssqlserveradmin/diagnostic-manager-custom-alerts/</link>
		<comments>/index.php/datamgmt/dbadmin/mssqlserveradmin/diagnostic-manager-custom-alerts/#respond</comments>
		<pubDate>Fri, 13 Apr 2012 09:43:00 +0000</pubDate>
		<dc:creator><![CDATA[Kevin Conan]]></dc:creator>
				<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[backups]]></category>
		<category><![CDATA[diagnostic manager]]></category>
		<category><![CDATA[idera]]></category>
		<category><![CDATA[monitoring]]></category>
		<category><![CDATA[sql]]></category>

		<guid isPermaLink="false">/index.php/2012/04/diagnostic-manager-custom-alerts/</guid>
		<description><![CDATA[One of the great things about Diagnostic Manager is the ability to create custom alerts.  This is something I believe needs a bit more work, but even in its current state, it is really useful.]]></description>
				<content:encoded><![CDATA[<p>One of the great things about Diagnostic Manager is the ability to create custom alerts.  This is something I believe needs a bit more work, but even in its current state, it is really useful.</p>
<p>The example that we&#8217;ll walk through is something that would actually make a great built in alert.  We will create an alert based on the time of our oldest current database backup.</p>
<p>To start off, click on &#8220;Administration&#8221; in the bottom right hand corner:</p>
<p style="text-align: left;"><a href="/media/users/kconan/DM - Administration Menu.JPG?mtime=1332527611"><img src="/wp-content/uploads/users/kconan/DM - Administration Menu.JPG?mtime=1332527611" alt="" width="302" height="203" /></a></p>
<p>Next Click on Custom Counters (there are two spots that you can click on for it):</p>
<div class="image_block">
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Administration Menu2.JPG?mtime=1332527857"><img src="/wp-content/uploads/users/kconan/DM - Administration Menu2.JPG?mtime=1332527857" alt="" width="603" height="310" /></a></div>
</div>
<div class="image_block" style="text-align: left;">Now in the menu ribbon, click on Add.</div>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter Add.jpg?mtime=1332528329"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter Add.jpg?mtime=1332528329" alt="" width="364" height="161" /></a></div>
<p>On the first screen of the wizard, click on Next at the bottom.</p>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter Wizard 1.JPG?mtime=1332528009"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter Wizard 1.JPG?mtime=1332528009" alt="" width="633" height="564" /></a></div>
<p>We are going to base our Custom Counter/Alert on a SQL Query, so choose the 3rd option: Custom SQL Script.</p>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter Wizard 2.JPG?mtime=1332528329"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter Wizard 2.JPG?mtime=1332528329" alt="" width="630" height="558" /></a></div>
<p>On the next screen we add the SQL Query.</p>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter Wizard 3.JPG?mtime=1332529354"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter Wizard 3.JPG?mtime=1332529354" alt="" width="639" height="564" /></a></div>
<p><!-- codeblock="tsql" -->
<pre>&lt;code&gt;
SELECT ROUND(MAX(DATEDIFF(MINUTE,LastBackUpTaken,GETDATE()))/60.0,2)  
FROM  
( 
SELECT sdb.Name AS DatabaseName
,COALESCE(MAX(bus.backup_finish_date),GETDATE()-100) AS LastBackUpTaken    
FROM master..sysdatabases sdb    
LEFT OUTER JOIN msdb.dbo.backupset bus    
ON bus.database_name = sdb.name    
WHERE sdb.Name NOT IN ('tempdb','model')   
AND bus.type = 'L'   
GROUP BY sdb.Name  
) 
DBList;</pre>
<p>*Note the code provided above is different from the screenshot. While proof reading this article, I realized that I forgot include the backup type in the query.</p>
<p>On the next screen we want to leave the Customize Calculation Type set to "Use collected value".  We are doing this because we want to base the alert off the most recent value.  If we had chosen to set it to "Use per second value since last collection" then it would base the alert on the difference between the most recent two results of our query.</p>
<p>On the same screen we also want to leave the Customize Scale Factor at 1.  We are doing this because we've already set our query to return the number of hours since the latest current backup was completed.</p>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter Wizard 4.JPG?mtime=1332529893"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter Wizard 4.JPG?mtime=1332529893" alt="" width="633" height="563" /></a></div>
<p>If we click on the test button we can run the query on all or some of our monitored instances to ensure it works.  When you are finished testing, click on Done and then Next.</p>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter Test.jpg?mtime=1332529893"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter Test.jpg?mtime=1332529893" alt="" width="644" height="549" /></a></div>
<p>You want to be sure to put a meaningful and descriptive name on the next screen.  This is the name that will appear in the alerts screen.</p>
<p>For my case, I don't have many custom alerts so I keep keep the category set to Custom for all of them.</p>
<p>I setthe description that you see in the screenshot below so that I include the code to find which databases need to be backed up is at my fingertips.</p>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter Wizard 5.JPG?mtime=1332531299"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter Wizard 5.JPG?mtime=1332531299" alt="" width="635" height="563" /></a></div>
<p>On the next screen, we want to choose the option of "Higher values are worse than lower values" because a higher number means it's been longer since the last backup.</p>
<p>I like to set the informational warning at 24 hours.  At that point it is not a problem, but something I want to be aware of.  The warning is set at 26 hours because it could just be that a backup is running long for some reason.  When we hit 28 hours that means that something is definately not right.</p>
<p>This is the type of counter that I want on every instance no matter it is used for, so I check the bottom check box.</p>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter Wizard 6.JPG?mtime=1332531299"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter Wizard 6.JPG?mtime=1332531299" alt="" width="634" height="563" /></a></div>
<p>The next screen just tells you that you have completed the wizard and warns you that you still need to link the custom counter to your monitored SQL Instances.</p>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter Wizard 7.JPG?mtime=1332531299"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter Wizard 7.JPG?mtime=1332531299" alt="" width="633" height="555" /></a></div>
<p>On the next screen you can select whether or not you want to link this new customer counter your monitored SQL Instances right away.</p>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter Wizard 8.JPG?mtime=1332531784"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter Wizard 8.JPG?mtime=1332531784" alt="" width="624" height="141" /></a></div>
<p>The other way to link the custom counter is to select the counter and choose link from the menu above it.</p>
<div class="image_block" style="text-align: left;"><a href="/media/users/kconan/DM - Custom Counter List.jpg?mtime=1332531784"><img src="/wp-content/uploads/users/kconan/DM - Custom Counter List.jpg?mtime=1332531784" alt="" width="595" height="172" /></a></div>
<p>I won't do any screenshots of the screen where you choose which servers to link because it is a very straight forward screen where you select the SQL Instance from the left side and click on the add button.</p>
<p>At the start of this blog post, I mentioned that there are some change that would be really helpful.  What I meant by that is adding the option to include a second query that would be executed and the results displayed as a "drill in" option for the counter.</p>
<p>You can also easily turn this counter into one for log backups</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbadmin/mssqlserveradmin/diagnostic-manager-custom-alerts/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>What does a SQL Server DBA need to check for daily, weekly, monthly, quarterly and yearly?</title>
		<link>/index.php/datamgmt/datadesign/what-does-a-sql-server-1/</link>
		<comments>/index.php/datamgmt/datadesign/what-does-a-sql-server-1/#respond</comments>
		<pubDate>Tue, 18 Oct 2011 17:46:00 +0000</pubDate>
		<dc:creator><![CDATA[SQLDenis]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[monitoring]]></category>

		<guid isPermaLink="false">/index.php/2011/10/what-does-a-sql-server-1/</guid>
		<description><![CDATA[I am putting a wiki page together on what a SQL Server DBA needs to monitor daily, weekly, monthly, quarterly and yearly. So far I have created a couple of things here: What does a DBA need to check for daily, weekly, monthly, quarterly and yearly

So&#8230;]]></description>
				<content:encoded><![CDATA[<p>I am putting a wiki page together on what a SQL Server DBA needs to monitor daily, weekly, monthly, quarterly and yearly. So far I have created a couple of things here: <a href="http://wiki.lessthandot.com/index.php/What_does_a_DBA_need_to_check_for_daily%2C_weekly%2C_monthly%2C_quarterly_and_yearly">What does a DBA need to check for daily, weekly, monthly, quarterly and yearly</a></p>
<p>So far I have added only a few items and a couple of blurbs, I would like to expand that and need some input from you the readers. What do you monitor</p>
<p>Daily<br />
Weekly<br />
Monthly<br />
Quarterly<br />
Yearly</p>
<p>Leave me a comment, include any links that have some good scrips also, I will link to those</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/what-does-a-sql-server-1/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>The Lazy DBA Series: Monitoring</title>
		<link>/index.php/datamgmt/datadesign/lazy-dba-monitoring-sql-server/</link>
		<comments>/index.php/datamgmt/datadesign/lazy-dba-monitoring-sql-server/#respond</comments>
		<pubDate>Sun, 22 Aug 2010 22:33:11 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[baseline]]></category>
		<category><![CDATA[monitoring]]></category>
		<category><![CDATA[performance]]></category>
		<category><![CDATA[red gate]]></category>
		<category><![CDATA[reporting]]></category>
		<category><![CDATA[sql response]]></category>
		<category><![CDATA[sql server 2008]]></category>

		<guid isPermaLink="false">/index.php/2010/08/lazy-dba-monitoring-sql-server/</guid>
		<description><![CDATA[There are hundreds if not thousands of T-SQL Scripts, Powershell Cmdlets, .NET programs and even old vbscript scripts and on out there to monitor SQL Server and Operating System level performance.  I’ve written hundreds of them myself over the years.  All of that time and work absolutely paid off.  I know a lot about monitoring, going after the right information when something specific is wrong.  It also gives me a bag of tricks that mostly causes laptops to run out of disk space often.  Disk is cheap though, these days.]]></description>
				<content:encoded><![CDATA[<p>There are hundreds if not thousands of T-SQL Scripts, Powershell Cmdlets, .NET programs and even old vbscript scripts out there to monitor SQL Server and Operating System level performance.  I&#8217;ve written hundreds of them myself over the years.  Without a doubt, all of that time and work absolutely paid off.  It helped me learn even more about monitoring the right things and going after the right information when something specific is wrong.  It also gives me a bag of tricks (folder named, BagOfTricks) that mostly causes laptops to run out of disk space often.  Disk is cheap these days, right?!</p>
<p>Years ago I realized that writing all those scripts and running them in SSIS, SQL Agent and such was good, but was I being efficient?  Yes, I found that being lazy was far out of reach with these methods. Even making those scripts and programs as dynamic and mobile as they could be, there was always the fact that I had to write and update them along with implement them with in some cases, the same amount of work.  Not the most efficient way to spend time as a DBA.  I mean, business wants you to work for them, not for you.  As sad as that is, it often is the fact.  In all, writing scripts is a must and you need to in order to learn internals and become more familiar with the architecture of SQL Server and the Operating Systems they are running on.  I will always recommend that and real time troubleshooting with all of indicators we retrieve in these ways can never be replaced with anything else.  What about the return of investment to the company for overall monitoring?  Beyond real time troubleshooting when a problem is found?</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba.gif" alt="" title="" width="378" height="378" /></div>
<p><strong>Monitoring by Next, Next, Finish</strong></p>
<p>Recently I was honored to be accepted into the <a href="http://www.red-gate.com/about/community_relations/friends_of_RG.htm">Friends of Red Gate Program</a>.   Red Gate has always been known for being part in the community and giving a lot (and I mean, a lot!) back to us.  Having the opportunity to work with them is truly exciting.  While being a member of this program, I’ve had the chance to run some Red Gate tools that I previously had not been able to.  One of them was, SQL Response.   For this Lazy DBA topic, we are going to use <a href="http://www.red-gate.com/products/SQL_Response/index.htm">SQL Response</a> to discuss monitoring and using your time more efficiently.  First, let’s go over SQL Response briefly to set the stage for why it provides us with more efficient monitoring.</p>
<p>I put SQL Response under the microscope and looked at everything it was doing, reading, using to do so and the impact had on my test database server.  The impact was very low (my word) and installation quick and painless.  In fact, SQL Response seems to use nothing more but what it has at its fingertips to use from SQL Server.  That being normal traces, DMVs and system tables/views.  These all are used to gather what it needs to successfully tell you, &#8220;There is a problem&#8221;.  As with any great monitoring tool, notifications by email are available in SQL Response.</p>
<p>Most of the batches that SQL Response sent to the DB Server were clean and contained statements I think all of us rely on daily.   Things such as the normal check for Agent status, Query Plan Cache and OS performance from the dm_os list of DMV’s.  Very well formed as well.  </p>
<p>The meat of SQL Response is there and is just enough to get the job done.  I’m the type of DBA that likes thin, easy to read and give me what I want, tools.  Leaving as little to no impact on the database servers while it does what I need.  This one does that by not adding a lot of &#8220;fluff&#8221; for the graphics and simply does the task is was handed.   Of course, everyone wants their reports and there needs some help in that area.  When it comes down to it, my worry is telling me when a problem exists before I have to deal with screaming users.  All of that without spending a half hour determining which animated gif is telling me know I have a CPU problem.</p>
<p>The following is a snapshot of SQL Response showing my horribly starving disk on my laptop.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba_monitor.gif" alt="" title="" width="628" height="240" /></div>
<p>There really isn’t much more to it than a clean and straight forward interface.  I like the fact that there is a recommendations section also.  This appears to be recommendations that are clear and precise. Most that I forced errors on the test database server showed recommendations that I would recommend as well.  That alone shows great thought was put into this good feature.</p>
<p>So where is all of this going?  Being efficient in monitoring is as important as the last <a href="/index.php/DataMgmt/DBAdmin/lazy-dba-sql-trace">topic</a> we discussed.  Setting up custom monitoring on your own takes a lot of time and work and while that work is being done, your database server may be in trouble.  Enlisting the vendors that supply these monitoring tools and put extreme effort towards making them work for your needs is a way to put automation, real-time notifications and limited impacting tools in place quickly and safely.</p>
<p><strong>Final Note</strong></p>
<p>There never really is a last note about monitoring tools.  You still have to put work into deciding which one is best for you database servers and your unique methods of confronting the day as a DBA.  Choose them wisely and don’t throw something on top of a production database server without knowing exactly what it does, how it does it and if it itself, will cause unwanted problems with performance.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/lazy-dba-monitoring-sql-server/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Creating a baseline for SQL Server</title>
		<link>/index.php/datamgmt/datadesign/sql-server-baseline-creation/</link>
		<comments>/index.php/datamgmt/datadesign/sql-server-baseline-creation/#comments</comments>
		<pubDate>Mon, 09 Aug 2010 09:50:25 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[baseline]]></category>
		<category><![CDATA[monitoring]]></category>
		<category><![CDATA[performance]]></category>
		<category><![CDATA[sql server]]></category>
		<category><![CDATA[sql server 2008]]></category>

		<guid isPermaLink="false">/index.php/2010/08/sql-server-baseline-creation/</guid>
		<description><![CDATA[In the last post, "Baseline, Performance Reporting and being a proactive DBA" touched on baselines and using them to set thresholds for actively monitoring performance problems on SQL Server.  From that post we briefly discussed that every database server is unique.  That even being true when the databases we attach to SQL Server are packaged installations from third party applications (like SAP, Dynamics etc...).  I received feedback from my good friend Aaron Lowe (Twitter &#124; Blog) on this topic and a very good conversation on how we create these baselines.  Aaron had a great point regarding there not being much out there on exactly how to do this so I thought I’d write up a follow-up to the article and some points you can use to create your own baselines.]]></description>
				<content:encoded><![CDATA[<p>In my last post, &#8220;<a href="/index.php/DataMgmt/DBAdmin/sql-server-baseline">Baseline, Performance Reporting and being a proactive DBA</a>&#8220;, I touched on baselines and using them to set thresholds for actively monitoring performance problems on SQL Server.  I briefly discussed that every database server is unique when the databases we attach to SQL Server are packaged installations from third party applications (like SAP, Dynamics etc&#8230;).  I received feedback from my good friend Aaron Lowe (<a href="http://twitter.com/Vendoran">Twitter</a> | <a href="http://www.aaronlowe.net/">Blog</a>) on this topic and had a very good conversation on how we create these baselines.  Aaron had a great point regarding there not being much out there on exactly how to do this so I thought I’d write up a follow-up to the article and some points you can use to create your own baselines.</p>
<p><strong>So why aren&#8217;t there a ton of posts on creating baselines?</strong></p>
<p>Here are my two bits on the topic.  Creating a baseline is almost as unique as every business and processing model.  In saying that, creating a baseline and what you capture will vary slightly.  A lot of variables like the hardware you’ve purchased, the storage type, the server technology, OS versions/editions and even SQL Server editions affect the process.  Even with these variables and the unique nature of business, we can set some guidelines of where to start and how to make our baseline. </p>
<p><strong>Baseline creation basics</strong></p>
<p>The most important thing to understand when creating a baseline is, the collection of the baseline itself will affect performance of the SQL Server and Operating System.  Given that factor, take into account that using tools to collect how the resources are being utilized will affect the overall outcome.</p>
<p>Another important factor to think about; baseline creation is not tuning time.  Your tuning should be done as its own major task.  It should also be done prior to baseline creations.  If you create a baseline based on an un-tuned database server, the baseline will be inaccurate before it is even put to use.  The baseline will be far above the levels that you reach after tuning and true spikes in performance and progressive problems can easily go on without being caught.</p>
<p>Performance Monitor (perfmon) is a tool that you can use not only for baseline capture but for overall performance for long term monitoring.  At this time I would not recommend the use of profiler in your baseline.  The reason for that is, profiler has a large impact on performance.  Even in a lot of troubleshooting scenarios, perfmon would be utilized first to identify the bottlenecks.  Then profiler can be used as a focused tool to dig out the root causes.</p>
<p><strong>Dynamic Management Views/Functions</strong></p>
<p>With the addition of Dynamic Management Views in SQL Server 2005+, we have a completely new way to add to baseline capturing.  sys.dm_os_performance_counters alone gives us a vast amount of information to work off of that was only available to perfmon.  To see the types of counters that are exposed in performance_counters, use the following query</p>

<div class="bwp-syntax-block clearfix">
<div class="bwp-syntax-toolbar"><div class="bwp-syntax-control"><a href="javascript:;" class="bwp-syntax-source-switch" title="View Source Code"></a></div></div>
<div class="bwp-syntax-wrapper clearfix bwp-syntax-simple"><table class="tsql"><thead><tr><td colspan="2"  class="head">T-SQL</td></tr></thead><tbody><tr class="li1"><td class="ln"><pre class="de1">1
2
3
</pre></td><td class="de1"><pre class="de1"><span class="kw1">SELECT</span> <span class="kw1">DISTINCT</span> <span class="br0">&#91;</span><span class="kw2">object_name</span><span class="br0">&#93;</span> &nbsp;
<span class="kw1">FROM</span> sys.<span class="br0">&#91;</span>dm_os_performance_counters<span class="br0">&#93;</span> &nbsp;
<span class="kw1">ORDER</span> <span class="kw1">BY</span><span class="br0">&#91;</span><span class="kw2">object_name</span><span class="br0">&#93;</span>; </pre></td></tr></tbody></table></div>
<div class="bwp-syntax-source"><pre class="no-parse">SELECT DISTINCT [object_name]  
FROM sys.[dm_os_performance_counters]  
ORDER BY[object_name]; </pre></div></div>

<p>In the list of counters you will see there are quite a few SQL Server counters that we can analyze.   The only problem is everything in there is restricted to SQL Server.  This means we will still need the help of perfmon and other tools like WMI reads.  I mention <a href="http://msdn.microsoft.com/en-us/library/aa394582(VS.85).aspx">Windows Management Instrumentation</a> (WMI) because my own baseline capture process consists of some WMI reads similar to the methods that Jason Massie uses <a href="http://sqlblogcasts.com/blogs/jasonmassie/archive/2007/11/29/accessing-os-performance-counters-from-tsql.aspx">here</a> and using the WMI reader task in SSIS.  WMI can get deep into the hardware and give us information so we can refine our baselines down even further.</p>
<p><strong>Perfmon counters to include in the baseline</strong></p>
<p>The counters to use in perfmon are out there already from <a href="http://technet.microsoft.com/en-us/library/cc781394%28WS.10%29.aspx">a BOL entry on Windows 2003 and SQL Server</a>.  The only variation to this listing that I deviate from is the deadlock and log growth counters.  I feel these should be already tuned and configured so they would not be a common problem.  Instead, they are counters to monitor over the long time period of time for active notification.  With the introduction of Event Notification and Extended Events, even these counters are not as commonly used any longer.  If you find that deadlocks are a problem, your baseline capture should stop and the deadlocks resolved before going on.  Deadlock, blocking or queries not tuned should not be allowed into your baseline.</p>
<p><strong>Putting it all together</strong></p>
<p>SSIS has much more to it than being an ETL product.  With baseline capture tasks, we can use it as our container in order to periodically execute the collection of information we can then set our baseline off of.</p>
<p>Take the following example to start from</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/create_base_1.gif" alt="" title="" width="628" height="451" /></div>
<p>In this control flow we can see WMI reader tasks executing to collect OS, IO and Memory information.  This all then goes into a DBA database and tables to collect the data over time.</p>
<blockquote><p><span class="MT_red">Note: before going on, let’s talk about WMI reader task.  WMI reader task exposes WMI to read in and directly into a flat file or variable.  I like to use the flat file option to minimize memory usage so when doing this, ensure you “append” to the files and not leave the default, Keep original</span></p></blockquote>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/create_base_2.gif" alt="" title="" width="524" height="179" /></div>
<p>Executing all of the WMI reads then sets them into a Data Flow task that will consume the files, convert the data types and import them into a SQL Destination.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/create_base_3.gif" alt="" title="" width="331" height="217" /></div>
<p>Collecting perfmon will already have a set file given the perfmon setup done outside of SSIS.  I recommend going over to Brent Ozar&#8217;s (<a href="http://twitter.com/brento">Twitter</a> | <a href="http://www.brentozar.com/">Blog</a>) blog on <a href="http://www.brentozar.com/archive/2006/12/dba-101-using-perfmon-for-sql-performance-tuning/">perfmon setup</a>.  It’s detailed and well drawn up to get collection of the information going. </p>
<p>Last, but definitely something that makes this easier, the execution of dm_os_performance_counters.  Since this DMV does not have a timestamp associated with it, add getdate() to the select so you can successfully pivot the data and read it for later use. </p>
<blockquote><p><span class="MT_red">Note: the SQL Server counters are all available in perfmon as well.  If executing the DMVs become longer of an execution time than adding them to the perfmon collections and reading the flat file.  This frees SQL Server resources and allows the baseline to be more accurate.  Don’t restrict to this DMV as well.  This is an example.  The memory related DMVs and on is all valuable information.</span></p></blockquote>
<p>Once all of this data is collected, SSRS can be used to analyze where your systems are leveling off.  All spikes of noticeable problems should be resolved and then a time based baseline capture performed once again.  This cycle should go on until a true baseline is captured.</p>
<p>Once the baseline data is collected (typically a week during all hours of operations) you can make decisions on specific thresholds in which an alert should be sent notifying you of a warning or critical event.</p>
<p><strong>Estimating, requesting and begging for more</strong></p>
<p>When creating a baseline and then further estimating the resources you need to maintain that level of performance, overestimating is your best friend.  There is a reason when you ask a DBA, &#8220;How much disk space do you need?&#8221; you will always be given the answer, &#8220;All of it&#8221;.  This is because, before the database server is set in place and growth analytics have been collected, we’re planning for the worst and fastest growth path.  There is nothing wrong with that method of going about resources other than budget planning.</p>
<p>Baseline should give you the starting point to also base your reporting performance over time and estimating resources required for growth and scaling the database server.</p>
<p>Once all of this information is collected, thresholds are set and you know where you stand with your database servers, you will be able to make confident and precise decisions on what the systems are capable of doing for the business and when there is a need for improvements or upgrades.</p>
<p><strong>The meat of third party tools</strong></p>
<p>Now that we have shown we can do all this work on our own, we can&#8217;t end without mentioning the tools that are out there to help us accomplish the same thing.  Anyone that has read my articles in the past knows that I am an advocate for being a lazy DBA.  This really has nothing to do with being lazy but means using everything and anything in order to make our jobs more efficient.  SSIS is great and something I’ve taken on full speed since SQL Server 2005.</p>
<p>With that said, Quest, Red Gate, Idera and a few others have this capability in their products.  Idera SQL Diagnostic Manager in the newer version has a baseline option that captures performance at a time requested and sets thresholds to it.  Quest and the others have similar baseline functionality.</p>
<p>These third party tools make life easy but more importantly, get the job done quicker and with much less human error involved.  I highly recommend if the budget is there and you can convince the decision makers of the value in them, purchase one of them that fits your environment.  </p>
<p>Still no go on the money for those nice tools and development time?  Performance Dashboard Reports have been around since SQL Server 2005.  These reports are valuable as canned reports that read from various DMV/DMFs and give you a snapshot of the SQL Server as it is when executed. OS metrics, Index usage, CPU, IO, user activity…and more is covered.  The dashboard installs right into your SSRS instance.  I personally have this setup for some database servers where a license of my other monitoring tools is not worth the cost based on the noncritical business aspect of the particular server.  (the package will be cleaned up and posted to this blog at a later date for download) I send subscriptions with Excel representations of the reports at time of executed. I save these then for a month or so and can analyze normal vs. abnormal performance.  This information can be taken and a baseline formulated from it as well.  </p>
<p>SQL Server 2005 and 2008 also have the Server Dashboard as part of the canned reports that can be run from SSMS.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/create_base_4.gif" alt="" title="" width="628" height="429" /></div>
<p>This is all very useful and valuable information about the state of your SQL Server and the databases.</p>
<p><strong>Conclusion or what we call, take away</strong></p>
<p>Baseline information can take some work to collect.  The information we collect is critical to allowing us to make decisions on when problems are out of the normal day to day scope.  We can collect true growth and progressive resource usage and base that on educated and precise decisions for upgrading to more advanced and larger hardware when the time comes.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/sql-server-baseline-creation/feed/</wfw:commentRss>
		<slash:comments>10</slash:comments>
		</item>
		<item>
		<title>Baseline, Performance Reporting and being a proactive DBA</title>
		<link>/index.php/datamgmt/datadesign/sql-server-baseline/</link>
		<comments>/index.php/datamgmt/datadesign/sql-server-baseline/#comments</comments>
		<pubDate>Thu, 05 Aug 2010 22:42:54 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[baseline]]></category>
		<category><![CDATA[monitoring]]></category>
		<category><![CDATA[performance]]></category>
		<category><![CDATA[reporting]]></category>
		<category><![CDATA[sql server 2008]]></category>

		<guid isPermaLink="false">/index.php/2010/08/sql-server-baseline/</guid>
		<description><![CDATA[As database professionals, our intent is to be as proactive as possible when it comes to delivering data with security, stability and speed in mind. Being proactive means active monitoring, reporting performance variations and most important, baseline capture. Adding to these three objectives, we can add Performance Notes to also provide key points of long [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>As database professionals, our intent is to be as proactive as possible when it comes to delivering data with security, stability and speed in mind.  Being proactive means active monitoring, reporting performance variations and most important, baseline capture.  Adding to these three objectives, we can add Performance Notes to also provide key points of long term and short term performance problems. </p>
<p>Let&#8217;s take a minute to discuss the three key points (Baselines, Active Monitoring and Reporting Performance)</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/sbTightRope.jpg" alt="" title="" width="455" height="320" /></div>
<p><strong>Baselines</strong></p>
<p>Every system is unique.  We can say that even knowing databases can be the same across systems that are common per vendor installations because every business is unique.  Adding to business differences, users are another variable in the equation.  Bringing those together, we can come to an overall equation that will typically end in a varying result.</p>
<p>Beyond normal performance tuning guidelines, capturing the unique impact on the database server is critical to know where performance during normal operations stands.  This allows managing of predictable performance changes and more importantly, monitoring of unpredictable performance variations.  Without baselines, we can tune database performance, but are not able to tune database performance to the fullest extent for the unique business and user impacts.</p>
<p><strong>Active Monitoring</strong></p>
<p>Active monitoring probably isn’t something unfamiliar.  There are several third party tools available to provide real-time monitoring of database servers.  With work, active monitoring can be achieved from within the feature set that SQL Server provides.  SQL Server Integration Services provides great value in monitoring operating system key performance indicators in next to real-time.  Power Shell can efficiently gather the same operating system level data and several other methods can apply.  (Five ways to do everything: pick the most efficient) Internally, Dynamic Management Views (DMV) and Dynamic Management Functions (DMF) provide a vast amount of information regarding performance.  These can be read, measured and notifications sent when thresholds are reached. </p>
<p>Active monitoring means next to real-time notifications of severe and critical problems affecting the users.  This comes down to the seconds and minutes game.  In order to gather information in these intervals, the performance impact that we make on the servers must be taken into consideration itself.  The development and management that goes into third party monitoring tools is vast and focused.  This makes these tools friendlier in implementations and portability.  It allows us to achieve efficiency in performing our tasks as administrators.  However, each third party monitoring tool should be chosen and matched to the particular database landscape and internal guidelines.  A good example of this is, allowing system object to be installed on instances or security level access.  Some reporting options can even make the difference in decisions.  Chose them wisely and take advantage of trial editions to beat them up so there are no surprises or neighbor envy later on.</p>
<p><strong>Reporting Performance</strong></p>
<p>Reporting performance can be confused with real-time monitoring but it is very different.  Over time, databases change and their needs change along with them.  In some cases this change is drastic and others can take months to happen.  Reporting on overall performance over time periods will show these changes.  Gradual progressive changes can show either performance problems or growth itself requiring action to prepare for the future.</p>
<p>Database growth for example is captured over long periods of time.  This falls into reporting performance for the reason we can plan to handle the growth in the future.  We can also perform tasks such as expanding the size of the database when required.  This can prevent unwanted growth during peak operating times and essentially high resource consumption and directly affecting the users.<br />
Below in the graph, we see the green points as actual growth.  With this we can forecast the growth into the future.  Without this data we may not know that in Month 11 we will need to either expand disk or replace the entire subsystem itself to handle the growth.  With that growth, resources will be consumed more so hardware upgrades to the server itself may be required.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/baseline_1.gif" alt="" title="" width="396" height="291" /></div>
<p>Reporting performance is where the term, Performance Note is introduced.  </p>
<p><strong>Performance Notes</strong></p>
<p>These events can occur over months, days and even hours.  They are subtle and large spikes from the baseline performance we captured from normal performance impacts.  These events are typically not performance problems that directly affect the users but neglected over time, can become severe problems.</p>
<p>Take the graph below for an example where CPU usage over a day is represented.  In the graph we see in the beginning of the day, utilization is very low.  Then at a certain time, CPU starts showing normal activity.  In the middle of the day a large spike in CPU occurs.  This spike should be noted and in the following days monitoring of the performance notations should be done if it is unknown to the baseline.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/baseline_2.gif" alt="" title="" width="628" height="178" /></div>
<p>Below is a graph of CPU usage over the two days after making the observation of the spike in CPU.</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/baseline_3.gif" alt="" title="" width="628" height="119" /></div>
<p>We can see that the spikes occur in both days and similar times.<br />
These performance notes should be looked into closely to determine what is impacting the CPU usage during the time of the spikes.  Often the spikes are found to be normal business operations but in some cases they are found to be performance problems that should be resolved.</p>
<p>With this we can put perfmon/profiler and other tools at our disposal to work by collecting more detailed information during and around the spike in resource consumption.  For example, if we see Processor Time counter high and correlate that to other common CPU counters and SQL counters, we can determine if we have common CPU bottlenecks like compilations, recompilation, parameterization etc&#8230; issues or simply a query that was introduced to the database server without our knowledge.  Then we can act quickly on it by optimization of resources or the query(s) taking part in the spikes.</p>
<p><strong>Conclusion</strong></p>
<p>Given the methods you can take to be proactive on SQL Server to ensure problems are limited to your knowledge is a stable of being successful in delivering data to the business.  With the steps and types of monitoring and reporting described, most situations can be caught, analyzed and resolved swiftly.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/sql-server-baseline/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
	</channel>
</rss>
