<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>sqlpass &#8211; LessthanDot</title>
	<atom:link href="/index.php/tag/sqlpass/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>A Technical Community for IT Professionals</description>
	<lastBuildDate>Sat, 09 Mar 2019 12:50:36 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.6.1</generator>
	<item>
		<title>The Lazy DBA Series: Wizards!</title>
		<link>/index.php/datamgmt/datadesign/lazy-dba-sql-server-wizards/</link>
		<comments>/index.php/datamgmt/datadesign/lazy-dba-sql-server-wizards/#comments</comments>
		<pubDate>Wed, 25 Aug 2010 13:30:19 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[database administration]]></category>
		<category><![CDATA[import wizard]]></category>
		<category><![CDATA[sql server]]></category>
		<category><![CDATA[sqlpass]]></category>

		<guid isPermaLink="false">/index.php/2010/08/lazy-dba-sql-server-wizards/</guid>
		<description><![CDATA[Today on the lazy DBA we're going to talk about why Gandalf couldn’t open the door to the mines of Moria.  OK, we're not going to talk about Gandalf or even Trolls.  Although some of us might work with a few Trolls.  We are going to talk about Wizards without pointy hats.]]></description>
				<content:encoded><![CDATA[<p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba2.gif" alt="" title="" width="209" height="209" /></div>
<p>Today on the lazy DBA we&#8217;re going to talk about why Gandalf couldn’t open the door to the mines of Moria.  OK, we&#8217;re not going to talk about Gandalf or even Trolls.  Although some of us might work with a few Trolls.  We are going to talk about Wizards without pointy hats. </p>
<p><strong>Scenario</strong></p>
<p>A great way to discuss any topic is to put it directly in front of us with a real life situation.  Take the following example…</p>
<p><i>A DBA (Bilbo) has been working on a major upgrade to SQL Server 2008 for months.  It has taken just about every minute of their time, given the number of database servers, the initiative to consolidate and being the only DBA employed.  One morning the Director of Marketing (Arwen) comes to the DBA in a panic.  See, Arwen forgot that in order to update every item for the company with the new marketing descriptions, she needed to give an Excel spreadsheet with them to Bilbo so he would import them.</i></p>
<p>What does Bilbo do?  The company stands to lose hundreds of thousands of dollars when the competitor’s item descriptions catch the eye of potential buyers and business is lost.</p>
<ul>
<li>Option 1<br />
Bilbo tells Arwen that he needs eight hours to develop an SSIS package in order to grab the Excel file, SELECT * FROM Sheet1$ into the buffer, merge that to another result set of the existing descriptions and item ID’s in order to successfully update each one row-by-row.  Bilbo is an SSIS god too so the eight hours isn’t questioned.</li>
<li>Option 2<br />
Bilbo becomes an efficient, lean and mean DBA and pulls up the Import/Export wizard.  Imports Sheet1$, writes a SELECT to JOIN the item master on the Sheet1$ by item ID, validates the match and changes SELECT to UPDATE.  Oh yeah, this option takes 10 minutes.</li>
</ul>
<p><strong>Picking the option</strong></p>
<p>I don’t think we need to discuss the efficient option here.  Option 1 has all of the stable and set process steps that any import process should have.  We bring data in from two sources, we match data and then we update based on the matches.  This is an efficient method for a task that would be done daily, weekly and even monthly.  Why isn’t option 1 all that great for the task at hand?</p>
<p>BIDS is an Integrated Development Environment and SQL Server Integrated Services is a platform that you should always use to manage imports and other tasks.  The power is endless.  However, the power can be misused as a bloated form of SSMS.  One rule that I try to set when creating SSIS package for any task is, “Will I need to save this?”  If I have no intentions of saving the finished product, then typically that package and SSIS was not required and I could have fully accomplished the task with other tools at my disposal.  At the same time, I could accomplish the task quicker and more efficient while not debating on how to use SSIS for it.  For clarification, the nice part of some of these wizards is, they use SSIS in the background to build packages as you work through them.  This allowing you to save the wizards configurations as a package at the end.</p>
<p><strong>Wizards in SQL Server</strong></p>
<p>For a number of years, Wizards have invaded the functionality of all of the tools we have for SQL Server.  Not only have they invaded, they have done so in large numbers.  We have…</p>
<ol>
<li>Database Mirroring Configuration/Setup</li>
<li>Replication Setup</li>
<li>Import/Export</li>
<li>Agent New Job </li>
<li>List goes on…</li>
</ol>
<p>Replication is a big one on this list.  I can’t remember the last time I set replication up without the wizard.  Yes, I can do it in T-SQL with the supporting procedures (that get called behind the wizard), but why would I?  Efficiency is a critical part in getting these tasks done.</p>
<p><strong>Option 2 played out</strong></p>
<p>I’m sure some SSIS masters are still saying that the package would take 10 minutes and that is the best route.  Wizards are the devil and I only write T-SQL because I’m cool.  OK, let’s open our minds just a little.</p>
<p>Option 2 played out…</p>
<p>Excel file comes in from Arwen</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_1.gif" alt="" title="" width="362" height="311" /></div>
<p>Open SSMS and find your DBA database (that you should have in Simple recovery and is a place where you should do all of your godly DBA, Developer work)</p>
<p>Right click the DB&#8211;>Tasks&#8211;>Import Data…</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_2.gif" alt="" title="" width="357" height="330" /></div>
<p>Select Excel as the source and find your XLS file</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_3.gif" alt="" title="" width="467" height="116" /></div>
<p>Hit next and leave the destination as your DBA work area.  Hit next again to go to the mappings definition.  Check the first tab (or whichever the right sheet is)</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_4.gif" alt="" title="" width="636" height="244" /></div>
<p>Name the table that you want to create and hit Next and then Finish</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_5.gif" alt="" title="" width="547" height="450" /></div>
<p>Write up a quick SELECT with whatever filters are required and join it to the destination of the ITEMMSTR</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_6.gif" alt="" title="" width="628" height="477" /></div>
<p>Then Alter the statement to make it an UPDATE</p>
<p><strong>Done!</strong></p>
<p><strong>Lazy is good but…</strong></p>
<p>Wizards can and will make you more efficient at your job.  Of course wizards can only do so far and you will find them restrictive when you have much more logic and conditions that need to be applied to the situation and task at hand.  That is the point though.  The combination of using wizards to get certain tasks done faster with the same stability will allow for the time needed on the bigger tasks that require a full assault approach. </p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/lazy-dba-sql-server-wizards/feed/</wfw:commentRss>
		<slash:comments>4</slash:comments>
		</item>
		<item>
		<title>The Lazy DBA Series: High Availability</title>
		<link>/index.php/datamgmt/datadesign/lazy-dba-high-availability/</link>
		<comments>/index.php/datamgmt/datadesign/lazy-dba-high-availability/#comments</comments>
		<pubDate>Tue, 24 Aug 2010 14:11:44 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[database administration]]></category>
		<category><![CDATA[sql server]]></category>
		<category><![CDATA[sqlpass]]></category>

		<guid isPermaLink="false">/index.php/2010/08/lazy-dba-high-availability/</guid>
		<description><![CDATA[Over the years I've had the chance to help a lot of people when it came to SQL Server. One of the things I have enjoyed the most in that time was helping with HA and DR topics. In my opinion, a truly recognized accomplishment for any DBA and Systems Team is to successfully secure the companies data and systems. The security I'm referring to isn’t to deny or grant a user access to data, but the security of data from disasters.Disasters can happen at any time. They never give us much warning either. Luckily, over many of our careers, we will never see a true disaster in action. Some will never have the opportunity to even see their High Availability go into action other than tests. Does that mean we don’t need these mechanisms and safeguards in place? (That was even hard to write) From the largest database down to the smallest script, everything should have a recovery plan. If it is important to the business and holds a key function, it has value and is worth the cost of adding the HA and DR solution to secure it.The topic at hand is High Availability, so let’s not accidentally cross to the DR side.]]></description>
				<content:encoded><![CDATA[<p>Over the years I’ve had the chance to help a lot of people when it came to SQL Server.  One of the things I have enjoyed the most in that time was helping with HA and DR topics.  In my opinion, a truly recognized accomplishment for any DBA and Systems Team is to successfully secure the companies data and systems. The security I’m referring to isn’t to deny or grant a user access to data, but the security of data from disasters.</p>
<p>Disasters can happen at any time.  They never give us much warning either.  Luckily, over many of our careers, we will never see a true disaster in action.  Some will never have the opportunity to even see their High Availability go into action other than tests.  Does that mean we don’t need these mechanisms and safeguards in place?  (That was even hard to write)  From the largest database down to the smallest script, everything should have a recovery plan.  If it is important to the business and holds a key function, it has value and is worth the cost of adding the HA and DR solution to secure it.</p>
<p>The topic at hand is High Availability, so let’s not accidentally cross to the DR side.</p>
<p><strong>Lazy High Availability</strong></p>
<p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba2.gif" alt="" title="" width="209" height="209" align="left" /></div>
<p>We’re not going to discuss dark fiber, monstrous SAN replication technology or even Windows Clustering.  Reality check: High Availability has work involved.  There is no way around doing some work and taking the time to learn what you need to know to setup HA successfully.  Planning must be done as well or the process of providing availability may fail.  However, there is a lazy theme here and we will find it.  It has been there since SQL Server 2005.  In fact, the option has been there for the majority of SQL Server installations since SQL Server 2005 SP1.  The feature is Database Mirroring.  With SQL Server 2005 SP1, it was made available to Standard Edition installations as well as Enterprise.</p>
<p>Database Mirroring is not a basic architecture and does take some time researching and reading things like: the internals of how the logs are streamed to the mirror databases, redone, queued, performance impacts, threading considerations on maximum expectations of databases while using mirroring and overall internal error handling.  Mouth full!  OK, we need to take the time to read and understand the internal functions that are performed.  Regardless of the technology and use we find for it, this must be done to be successful.  In the most recent article, The Lazy DBA Series: SQL Trace / Profiler, I would not send anyone off opening and running the default template in profiler on a production database server while the peak of the company&#8217;s processing is being done.  Reading about these tools and features is a must and can never be skipped in the process.  (Ok, preaching done!)</p>
<p>Database Mirroring setup on the other hand, is very straight forward and the setup is painless once you have the knowledge.   In fact, the most painful part in Database Mirroring will probably be the time waiting to copy exceedingly large databases to your designated mirror SQL Server over your network.  Even troubleshooting errors have a small list of areas that cause setup problems.  These common problems fall into mirror database not in a recovering level to start the redo process, security, forceful blocking such as port blocking and network configurations.</p>
<p>Everything that you need to perform a successful mirror of a database is right there in SQL Server (minus the hardware).  Database Mirroring works utilizing <a href="http://msdn.microsoft.com/en-us/library/ms179511.aspx">ENDPOINTS</a> as the primary communication method.  Again, endpoints are part of SQL Server and easily configured.  The best lazy part of all of what we are talking about is, this can and most commonly is, all done with the Database Mirroring Wizard.  Everyone hates wizards, right?  Sorry everyone, but some of those wizards make my job go fast and allow more important things to be done.  You can see the complete setup using the wizard in this article, <a href="/index.php/DataMgmt/DBAdmin/sql-server-2008-mirroring-setup">Mirroring Hands on with Developer Edition</a>.  As you probably guessed, look for a future Lazy DBA article on Wizards (and their magic spells).</p>
<p>Years ago, High Availability cost us a lot in time and money.  In fact it was even a stressful task to propose to management the funding required to put HA solutions in place.  However, recent years have seen more hardware for much less cost.  Disk is no longer in the hundreds of thousands for a solid solution and server technology can be purchased with a solid internal backbone of technology at a great price.</p>
<p>Database Mirroring isn’t a replacement for clustering and should/can be used in conjunction.  However, it can be used while clustering is not available.  In situations where database servers are designated as mirrors and with newer methods in handling failover events in connections, the physical server itself is not as critical of a piece as it once was.  Every application has needs and the statement I just made can be torn apart by those needs.  Make sure you research the task of a failover itself before thinking things will automatically start working simply because a mirror became your principal.</p>
<p><strong>Recap the laziness</strong></p>
<p>Setup is a task that can be accomplished with much less effort that was needed in previous years of HA technology if Database Mirroring fits the strategy.</p>
<p><a href="/index.php/DataMgmt/DBAdmin/how-to-monitor-database-mirroring">Monitoring can be done with Event Notifications</a> which is a quick setup and solid method.</p>
<p>Licensing is not as much of an issue as you may think.  If the Mirror is designated as a Mirror, it does not require a license until in use due to a failover.<br />
From the SQL Server 2008 pricing and licensing document (this is also the case in SQL Server 2005) </p>
<blockquote><p> <i><a href="http://download.microsoft.com/.../2008%20SQL%20Licensing%20overview%20final.docx">Passive Servers. The passive server does not require a license given that no queries are being executed against it.</a></i> </p></blockquote>
<blockquote><p>Be careful with this.  No queries means, no queries.  Make sure you read the licensing guidelines thoroughly before making your purchase or take the time to contact Microsoft with questions.</p></blockquote>
<p>Have fun because you really can have HA at a low cost on both price and maintenance sides.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/lazy-dba-high-availability/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>The Lazy DBA Series: SQL Trace / Profiler</title>
		<link>/index.php/datamgmt/datadesign/lazy-dba-sql-trace/</link>
		<comments>/index.php/datamgmt/datadesign/lazy-dba-sql-trace/#comments</comments>
		<pubDate>Fri, 20 Aug 2010 11:05:50 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[database administration]]></category>
		<category><![CDATA[sql server]]></category>
		<category><![CDATA[sqlpass]]></category>

		<guid isPermaLink="false">/index.php/2010/08/lazy-dba-sql-trace/</guid>
		<description><![CDATA[SQL Trace (and use of profiler as a front end to SQL Trace procedures) is far from a secret.  Performance tuning in the days of 2005/2008 has become much easier with the advent of DMVs but SQL Trace still has its place in the realm of, "Why is this happening".  Performance issues real-time and tuning becomes almost, easy while watching exactly what it going on with the batches hitting out database servers.  The only downfall is the impact SQL Trace plays on an active database server.  This can be done strategically and with minimal impact from a well thought through plan of attack.]]></description>
				<content:encoded><![CDATA[<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba.gif" alt="" title="" width="378" height="378" /></div>
<p>SQL Trace (and use of profiler as a front end to SQL Trace procedures) is far from a secret.  Performance tuning in the days of 2005/2008 has become much easier with the advent of DMVs but SQL Trace still has its place in the realm of, &#8220;Why is this happening&#8221;.  Performance tuning becomes almost easy while watching exactly what is going on with the batches hitting our database servers.  The only downfall is the impact SQL Trace plays on an active database server.  This can be done strategically and with minimal impact from a well thought through plan of attack.  </p>
<p>So what does SQL Trace have to do with being a lazy DBA?</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba_1.gif" alt="" title="" width="374" height="239" /></div>
<p><strong>Let me define what this series of blogs will show.</strong></p>
<p><i><strong>Lazy DBA</strong>: An efficient process of utilizing automation, native tools and or third party tools to assist in showing problems, possible solutions and active and long term monitoring of our database servers.</i></p>
<p>So we can see that being lazy in reality has little to do with being truly lazy.  It is the art of being better at our jobs with speed and quick results while not writing some nasty TSQL script for five days to find out the worst running query that is stored in cache.  Now that we have defined lazy in the best of ways, let’s look back at the SQL Trace functionality.  SQL Trace has the ability to show us everything that an application is sending to our database server.  True statement!  </p>
<p><strong>Scenario time:</strong></p>
<p><i>You are the only DBA in the entire world employed at your company.  This means you are the lucky and only person that does reporting, integrations, import/extract&#8230;All of it!  Director needs to update the ERP system for every single one of the ten thousand customers that are in the database.   You know the database but know it will also take you days just to determine just what columns need this update due to the most awful naming convention in a database you have ever seen.  </i></p>
<p>You are in for all-nighters for at least two days because this director holds your job in their hands.  What if I told you that you could get it done in around 20 minutes with these steps?</p>
<ol>
<li>Identify the columns the data is pulled from (5 minutes)</li>
<li>Import the new data that needs to be integrated (1 minute)</li>
<li>Write a select on the identified columns joined to the updated data (5 minutes)</li>
<li>Backup the table(s) (2 minutes)</li>
<li>Change SELECT to UPDATE with little alterations to the original query (7 minutes)</li>
</ol>
<p>OK, realistically that might be 22 minutes if the tables are big.  Might even be 22 hours if they are REALLY big.  You get the point and yes, it depends.</p>
<p>The key problem here is the identification of the columns.  That in reality takes up some time when you have extremely large (or small) databases that you don’t have the entity relational diagram memorized.  I still don’t have AdventureWorks memorized, so that isn’t so farfetched.  </p>
<p>Here is how to do this as a lazy DBA.  Open SQL Server Profiler.  Set up a typical default template trace.  Connect it to the database server (preferably development).  Before starting profiler, open the application now or get someone that can open it to do so.  Get ready to open the screen that shows the data as it passes through the application.  Hit start on the trace and then go into the application view to edit the data.  Notice I said edit.  Some application that use views or other accessing methods may make this more difficult than it needs to be.  Typically, edit is a safe bet that you get down to the base tables.</p>
<p>Stop profiler immediately once you see the transaction go by or know the window in the application is fully loaded.  </p>
<p>Depending on the method you used to store the profiler output, open either the table or trace files.  Use a LIKE statement to find the entries that consist of your login used first.  Then refine to the actual data you used in the application to filter on.  Such as customer or item number that you used to get to the screens.  </p>
<p>Refine that down (on minute 3 or 4 now) and then grab the columns.  (Copy/paste takes at least a minute)</p>
<p>Next step, import the data that needs to be used to update the database.  Hopefully they made this easy on you and didn’t give you TPS report printouts.   Write your SELECT statement and verify that the return count matches the update count (or more depending on requirements) and then validate they match up on the key relationships that you used to join on. </p>
<p>Always make sure you can recover, so make a backup of the tables in question.  Keep in mind constraints! Don’t EVER remove them just because it might work.  Data Integrity is important to you!</p>
<p>Alter your statement to an UPDATE on a join or derived table and execute it.</p>
<p>This is the best part: Email the director and say it is all ready for them to validate in development.  Sit back and relax now.  You were just lazy and it paid off by wowing them.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/lazy-dba-sql-trace/feed/</wfw:commentRss>
		<slash:comments>5</slash:comments>
		</item>
	</channel>
</rss>
