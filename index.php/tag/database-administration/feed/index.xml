<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>database administration &#8211; LessthanDot</title>
	<atom:link href="/index.php/tag/database-administration/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>A Technical Community for IT Professionals</description>
	<lastBuildDate>Sat, 09 Mar 2019 12:50:36 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.6.1</generator>
	<item>
		<title>Running older compatibility levels in newer versions</title>
		<link>/index.php/datamgmt/dbadmin/upgrading-comp-level-sql-server/</link>
		<comments>/index.php/datamgmt/dbadmin/upgrading-comp-level-sql-server/#comments</comments>
		<pubDate>Tue, 28 Dec 2010 15:18:38 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[database administration]]></category>
		<category><![CDATA[sql server 2008]]></category>
		<category><![CDATA[sql server upgrade]]></category>

		<guid isPermaLink="false">/index.php/2010/12/upgrading-comp-level-sql-server/</guid>
		<description><![CDATA[One very sound argument that many DBAs have for pushing an upgrade of SQL Server is the fact that you have the ability to leave a database in an older compatibility level.  In some ways, compatibility levels in a SQL Server database govern the feature set that the database will utilize.  SQL Server will treat the database as though it is still being hosted on the server version that matches the compatibility level the database has been left in.]]></description>
				<content:encoded><![CDATA[<p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/comp_level.gif" alt="" title="" width="121" height="91" align="left" /></div>
<p>One very sound argument that many DBAs have for pushing an upgrade of SQL Server is the fact that you have the ability to leave a database in an older compatibility level.  In some ways, compatibility levels in a SQL Server database govern the feature set that the database will utilize.  SQL Server will treat the database as though it is still being hosted on the server version that matches the compatibility level the database has been left in.  </p>
<p>
<strong>Not faulting them for saying no</strong></p>
<p>In some cases, having this ability to leave a database running in an older state is extremely useful.  This is seen in cases when a team has set the initiative for upgrading SQL Server to a higher release.  A hardship of DBAs is the fact that we are governed on our database servers by the databases that are hosted on them; this being set by the vendors and even in house designed databases and their ability to upgrade.  Allowing compatibility levels to remain at older versions while still upgrading binary support of SQL Server allows us, to a certain extent, the ability to push upgrades to SQL Server, while not completely disabling a database that may not take advantage of new functionality.  In many cases the lack of support for upgrading SQL Server databases is set forth due to the resources of the vendors and in house custom databases not fully testing the upgraded databases.  In this case the, “Better safe than sorry” term is taken to heart.</p>
<p>Some key facts that may cause a compatibility upgrade to cause problems are changes in how SETs are handled, the removal of the *=, =* to requiring OUTER JOIN syntax and an extremely common issue is the use of SET XACT_ABORT OFF in triggers to prevent failing entire batches based on an error raised by a single transaction.  In SQL Server 2000, SET XACT_ABORT OFF was allowed in triggers while higher versions do not allow this.  This was an extremely bad practice that was used in order to prevent triggers from causing entire operations from rolling back.  While this may sound like a good idea, it can cause integrity issues and data that is incomplete.</p>
<p>Every business has to manage resources and those resources may be placed in an area where the business initiative is better at that time.  In saying that, you cannot fault the fact that these databases may not fully support an upgrade from, say, compatibility level 80 to 90.  What we can do is work with the business, describing the upgrade paths of allowing a database to stay in an older compatibility level, while we take full advantage of upgrading the server for other data services.  </p>
<p><strong>The other side</strong></p>
<p>At the same time, we as DBAs have the ability to bring a case to the whiteboard for a database to be brought to a higher compatibility level.</p>
<p>Recall that we mentioned that a database left in an older compatibility level simply does not take advantage of SQL Server and new features.  Upgrading SQL Server and any other system at this level brings several advantages.  For example, take the introduction of Data Management Views and Data Management Functions.  With SQL Server 2005 or compatibility level 90, SQL Server introduced DMV/DMFs into the administration area.  DMV/DMFs have brought with them a new heightened level to all aspects of administering SQL Server.  Some examples are Resources utilization, Query performance, Session management and even real-time advanced troubleshooting for events that otherwise would force us to run resource-intense processing such as Profiler or Performance Monitor, simply to determine one area that is hindering the data services from performing to the best of their abilities.  Before you throw your hands up and say DMVs and DMFs are available in compatibility level 80, please read on.  We can take advantage of DMVs and DMFs in compatibility level 80 but run into problems that otherwise would not be an issue when utilizing them.  This is later covered in a link to a post by Paul Randal as part of an excellent series on myths in SQL Server.  One thing pointed out is the use of OBJECT_ID with a 3-part name passed in for the object name.  Other things that do not work are DMFs such as the helpful sys.dm_exec_sql_text().  Using this DMF in compatibility level 80 results in a syntax error over a meaningful error and may lead to lengthy searches for syntax problems when the DMF is simply not supported.  This DMF also brings up the topic that Jonathan Kehayias (<a href="http://twitter.com/sqlsarg">Twitter</a> | <a href="http://sqlblog.com/blogs/jonathan_kehayias/default.aspx">Blog</a>) blogged about on an <a href="http://sqlblog.com/blogs/jonathan_kehayias/archive/2010/05/04/master-database-compatibility-level-after-an-in-place-upgrade.aspx">in place upgrade of SQL Server to 2008</a> and the master database being left in 80 compatibility. </p>
<p>Most functionality can be done by passing queries through a database that is 90+ compatibility levels but is a workaround that can be avoided by upgrading the levels.  Forming well written code is essential to our database servers.  This goes for application level query support and administrative level support.  If we write code that is matched to the version including reserved words, functions, syntax related changes and operational commands, we leave ourselves in a state where upgrades will be less of a hassle and more manageable.</p>
<p>So, we know that we can utilize things like DMVs but we lengthen the time in developing with them, along with not writing queries that are formed using best for the version we are working on.  Of course we could handle this and keep pushing these changes as we go.  Unfortunately, this line of thought commonly ends in hitting a wall at some point, and forces a vast amount of energy to be put into major changes when the overall changes of moving forward over time could have prevented them.  This all equates to money saved on resources and providing better products.</p>
<p>There is a great disconnect, if you will, from the new version of SQL Server and enhancements made with the database left in a compatibility level that does not match the server engine.  This may cause the database processing to not take full advantage of how the upgraded changes have been set in the engine.  This may or may not cause a performance change in your database.  In most cases, it will not be a noticeable performance problem.  In larger and higher transactional databases, you may see an improvement in moving to the new compatibility level.  When we are in the situations with high-end transactional database servers, every performance increase that we can take advantage of should be accepted.  This is the reason we upgrade at all.</p>
<p><strong>Why not just leave it then?</strong></p>
<p>After going through all of the advantages and disadvantages to leaving compatibility levels that do not match the SQL Server base compatibility, you may be asking yourself why even bother upgrading at all.  The problem with allowing the older compatibility levels to remain is the fact that SQL Server upgrades will persist and those upgrades drop support for later versions as they move forward.  At the time of this article, SQL Server 2008 only will go back to SQL Server 2000 compatibility level of 80.</p>
<p>There is a sound argument in asking for a database to be tested in a newer compatibility level by simply staying with the growth and advancements of SQL Server.  Testing will expose some bad practices that can be fixed, utilization of enhanced T-SQL statements, writing reusable and portable code to other databases across your landscape and possible performance gains with allowing the newer versions of SQL Server to handle databases.</p>
<p>Resources that should be reviewed when determining if you should upgrade your compatibility level on SQL Server:<br />
<a href="http://www.sqlskills.com/BLOGS/PAUL/post/A-DBA-myth-a-day-(1330)-you-cannot-run-DMVs-when-in-the-80-compat-mode-(T-SQL-Tuesday-005).aspx">DMV/DMF on Compatibility level 80 – Paul Randal</a> (<a href="http://twitter.com/paulrandal">Twitter</a> | <a href="http://www.sqlskills.com/blogs/paul/">Blog</a>)<br />
<a href="http://msdn.microsoft.com/en-us/library/ms178653.aspx">Listing of what you lose and gain (and vice versa) &#8211; BOL</a><br />
<a href="http://technet.microsoft.com/en-us/library/cc707787.aspx">Backward compatibility &#8211; BOL</a><br />
<strong>Should I flip the switch?</strong></p>
<p>At this point you’ve decided to either change your compatibility level to match the currently running SQL Server level.  Changing the compatibility level is a simple process and does not alter actual data that is stored in the database.  Since this change is looked at as quick and painless, it is commonly recommended to simply go ahead and make the change at any given time.  There are considerations that must be taken into account though before you change the compatibility level.  Again, changing the compatibility level brings the database into a level that fully utilizes the current level of the engine.  This may change the results while transactions are processing.  This is part in the possibility of the plan being based on the old and new compatibility levels.  Making a compatibility level change should be done when there are no active user sessions on the database.  Another potentially high impact effect of changing the compatibility level is the event forcing a recompile of all procedures in the database.  Unnecessary recompile events can cause performance deterioration due to consumption of more resources and time of execution.</p>
<p>Before changing your compatibility level, check the link on the ALTER DATABASE statement for making the change itself.  There is a best practice section in which BOL describes the steps to take when upgrading the level.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/dbadmin/upgrading-comp-level-sql-server/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>The Lazy DBA Series: Wizards!</title>
		<link>/index.php/datamgmt/datadesign/lazy-dba-sql-server-wizards/</link>
		<comments>/index.php/datamgmt/datadesign/lazy-dba-sql-server-wizards/#comments</comments>
		<pubDate>Wed, 25 Aug 2010 13:30:19 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[database administration]]></category>
		<category><![CDATA[import wizard]]></category>
		<category><![CDATA[sql server]]></category>
		<category><![CDATA[sqlpass]]></category>

		<guid isPermaLink="false">/index.php/2010/08/lazy-dba-sql-server-wizards/</guid>
		<description><![CDATA[Today on the lazy DBA we're going to talk about why Gandalf couldn’t open the door to the mines of Moria.  OK, we're not going to talk about Gandalf or even Trolls.  Although some of us might work with a few Trolls.  We are going to talk about Wizards without pointy hats.]]></description>
				<content:encoded><![CDATA[<p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba2.gif" alt="" title="" width="209" height="209" /></div>
<p>Today on the lazy DBA we&#8217;re going to talk about why Gandalf couldn’t open the door to the mines of Moria.  OK, we&#8217;re not going to talk about Gandalf or even Trolls.  Although some of us might work with a few Trolls.  We are going to talk about Wizards without pointy hats. </p>
<p><strong>Scenario</strong></p>
<p>A great way to discuss any topic is to put it directly in front of us with a real life situation.  Take the following example…</p>
<p><i>A DBA (Bilbo) has been working on a major upgrade to SQL Server 2008 for months.  It has taken just about every minute of their time, given the number of database servers, the initiative to consolidate and being the only DBA employed.  One morning the Director of Marketing (Arwen) comes to the DBA in a panic.  See, Arwen forgot that in order to update every item for the company with the new marketing descriptions, she needed to give an Excel spreadsheet with them to Bilbo so he would import them.</i></p>
<p>What does Bilbo do?  The company stands to lose hundreds of thousands of dollars when the competitor’s item descriptions catch the eye of potential buyers and business is lost.</p>
<ul>
<li>Option 1<br />
Bilbo tells Arwen that he needs eight hours to develop an SSIS package in order to grab the Excel file, SELECT * FROM Sheet1$ into the buffer, merge that to another result set of the existing descriptions and item ID’s in order to successfully update each one row-by-row.  Bilbo is an SSIS god too so the eight hours isn’t questioned.</li>
<li>Option 2<br />
Bilbo becomes an efficient, lean and mean DBA and pulls up the Import/Export wizard.  Imports Sheet1$, writes a SELECT to JOIN the item master on the Sheet1$ by item ID, validates the match and changes SELECT to UPDATE.  Oh yeah, this option takes 10 minutes.</li>
</ul>
<p><strong>Picking the option</strong></p>
<p>I don’t think we need to discuss the efficient option here.  Option 1 has all of the stable and set process steps that any import process should have.  We bring data in from two sources, we match data and then we update based on the matches.  This is an efficient method for a task that would be done daily, weekly and even monthly.  Why isn’t option 1 all that great for the task at hand?</p>
<p>BIDS is an Integrated Development Environment and SQL Server Integrated Services is a platform that you should always use to manage imports and other tasks.  The power is endless.  However, the power can be misused as a bloated form of SSMS.  One rule that I try to set when creating SSIS package for any task is, “Will I need to save this?”  If I have no intentions of saving the finished product, then typically that package and SSIS was not required and I could have fully accomplished the task with other tools at my disposal.  At the same time, I could accomplish the task quicker and more efficient while not debating on how to use SSIS for it.  For clarification, the nice part of some of these wizards is, they use SSIS in the background to build packages as you work through them.  This allowing you to save the wizards configurations as a package at the end.</p>
<p><strong>Wizards in SQL Server</strong></p>
<p>For a number of years, Wizards have invaded the functionality of all of the tools we have for SQL Server.  Not only have they invaded, they have done so in large numbers.  We have…</p>
<ol>
<li>Database Mirroring Configuration/Setup</li>
<li>Replication Setup</li>
<li>Import/Export</li>
<li>Agent New Job </li>
<li>List goes on…</li>
</ol>
<p>Replication is a big one on this list.  I can’t remember the last time I set replication up without the wizard.  Yes, I can do it in T-SQL with the supporting procedures (that get called behind the wizard), but why would I?  Efficiency is a critical part in getting these tasks done.</p>
<p><strong>Option 2 played out</strong></p>
<p>I’m sure some SSIS masters are still saying that the package would take 10 minutes and that is the best route.  Wizards are the devil and I only write T-SQL because I’m cool.  OK, let’s open our minds just a little.</p>
<p>Option 2 played out…</p>
<p>Excel file comes in from Arwen</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_1.gif" alt="" title="" width="362" height="311" /></div>
<p>Open SSMS and find your DBA database (that you should have in Simple recovery and is a place where you should do all of your godly DBA, Developer work)</p>
<p>Right click the DB&#8211;>Tasks&#8211;>Import Data…</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_2.gif" alt="" title="" width="357" height="330" /></div>
<p>Select Excel as the source and find your XLS file</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_3.gif" alt="" title="" width="467" height="116" /></div>
<p>Hit next and leave the destination as your DBA work area.  Hit next again to go to the mappings definition.  Check the first tab (or whichever the right sheet is)</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_4.gif" alt="" title="" width="636" height="244" /></div>
<p>Name the table that you want to create and hit Next and then Finish</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_5.gif" alt="" title="" width="547" height="450" /></div>
<p>Write up a quick SELECT with whatever filters are required and join it to the destination of the ITEMMSTR</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazxywizard_6.gif" alt="" title="" width="628" height="477" /></div>
<p>Then Alter the statement to make it an UPDATE</p>
<p><strong>Done!</strong></p>
<p><strong>Lazy is good but…</strong></p>
<p>Wizards can and will make you more efficient at your job.  Of course wizards can only do so far and you will find them restrictive when you have much more logic and conditions that need to be applied to the situation and task at hand.  That is the point though.  The combination of using wizards to get certain tasks done faster with the same stability will allow for the time needed on the bigger tasks that require a full assault approach. </p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/lazy-dba-sql-server-wizards/feed/</wfw:commentRss>
		<slash:comments>4</slash:comments>
		</item>
		<item>
		<title>The Lazy DBA Series: High Availability</title>
		<link>/index.php/datamgmt/datadesign/lazy-dba-high-availability/</link>
		<comments>/index.php/datamgmt/datadesign/lazy-dba-high-availability/#comments</comments>
		<pubDate>Tue, 24 Aug 2010 14:11:44 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[database administration]]></category>
		<category><![CDATA[sql server]]></category>
		<category><![CDATA[sqlpass]]></category>

		<guid isPermaLink="false">/index.php/2010/08/lazy-dba-high-availability/</guid>
		<description><![CDATA[Over the years I've had the chance to help a lot of people when it came to SQL Server. One of the things I have enjoyed the most in that time was helping with HA and DR topics. In my opinion, a truly recognized accomplishment for any DBA and Systems Team is to successfully secure the companies data and systems. The security I'm referring to isn’t to deny or grant a user access to data, but the security of data from disasters.Disasters can happen at any time. They never give us much warning either. Luckily, over many of our careers, we will never see a true disaster in action. Some will never have the opportunity to even see their High Availability go into action other than tests. Does that mean we don’t need these mechanisms and safeguards in place? (That was even hard to write) From the largest database down to the smallest script, everything should have a recovery plan. If it is important to the business and holds a key function, it has value and is worth the cost of adding the HA and DR solution to secure it.The topic at hand is High Availability, so let’s not accidentally cross to the DR side.]]></description>
				<content:encoded><![CDATA[<p>Over the years I’ve had the chance to help a lot of people when it came to SQL Server.  One of the things I have enjoyed the most in that time was helping with HA and DR topics.  In my opinion, a truly recognized accomplishment for any DBA and Systems Team is to successfully secure the companies data and systems. The security I’m referring to isn’t to deny or grant a user access to data, but the security of data from disasters.</p>
<p>Disasters can happen at any time.  They never give us much warning either.  Luckily, over many of our careers, we will never see a true disaster in action.  Some will never have the opportunity to even see their High Availability go into action other than tests.  Does that mean we don’t need these mechanisms and safeguards in place?  (That was even hard to write)  From the largest database down to the smallest script, everything should have a recovery plan.  If it is important to the business and holds a key function, it has value and is worth the cost of adding the HA and DR solution to secure it.</p>
<p>The topic at hand is High Availability, so let’s not accidentally cross to the DR side.</p>
<p><strong>Lazy High Availability</strong></p>
<p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba2.gif" alt="" title="" width="209" height="209" align="left" /></div>
<p>We’re not going to discuss dark fiber, monstrous SAN replication technology or even Windows Clustering.  Reality check: High Availability has work involved.  There is no way around doing some work and taking the time to learn what you need to know to setup HA successfully.  Planning must be done as well or the process of providing availability may fail.  However, there is a lazy theme here and we will find it.  It has been there since SQL Server 2005.  In fact, the option has been there for the majority of SQL Server installations since SQL Server 2005 SP1.  The feature is Database Mirroring.  With SQL Server 2005 SP1, it was made available to Standard Edition installations as well as Enterprise.</p>
<p>Database Mirroring is not a basic architecture and does take some time researching and reading things like: the internals of how the logs are streamed to the mirror databases, redone, queued, performance impacts, threading considerations on maximum expectations of databases while using mirroring and overall internal error handling.  Mouth full!  OK, we need to take the time to read and understand the internal functions that are performed.  Regardless of the technology and use we find for it, this must be done to be successful.  In the most recent article, The Lazy DBA Series: SQL Trace / Profiler, I would not send anyone off opening and running the default template in profiler on a production database server while the peak of the company&#8217;s processing is being done.  Reading about these tools and features is a must and can never be skipped in the process.  (Ok, preaching done!)</p>
<p>Database Mirroring setup on the other hand, is very straight forward and the setup is painless once you have the knowledge.   In fact, the most painful part in Database Mirroring will probably be the time waiting to copy exceedingly large databases to your designated mirror SQL Server over your network.  Even troubleshooting errors have a small list of areas that cause setup problems.  These common problems fall into mirror database not in a recovering level to start the redo process, security, forceful blocking such as port blocking and network configurations.</p>
<p>Everything that you need to perform a successful mirror of a database is right there in SQL Server (minus the hardware).  Database Mirroring works utilizing <a href="http://msdn.microsoft.com/en-us/library/ms179511.aspx">ENDPOINTS</a> as the primary communication method.  Again, endpoints are part of SQL Server and easily configured.  The best lazy part of all of what we are talking about is, this can and most commonly is, all done with the Database Mirroring Wizard.  Everyone hates wizards, right?  Sorry everyone, but some of those wizards make my job go fast and allow more important things to be done.  You can see the complete setup using the wizard in this article, <a href="/index.php/DataMgmt/DBAdmin/sql-server-2008-mirroring-setup">Mirroring Hands on with Developer Edition</a>.  As you probably guessed, look for a future Lazy DBA article on Wizards (and their magic spells).</p>
<p>Years ago, High Availability cost us a lot in time and money.  In fact it was even a stressful task to propose to management the funding required to put HA solutions in place.  However, recent years have seen more hardware for much less cost.  Disk is no longer in the hundreds of thousands for a solid solution and server technology can be purchased with a solid internal backbone of technology at a great price.</p>
<p>Database Mirroring isn’t a replacement for clustering and should/can be used in conjunction.  However, it can be used while clustering is not available.  In situations where database servers are designated as mirrors and with newer methods in handling failover events in connections, the physical server itself is not as critical of a piece as it once was.  Every application has needs and the statement I just made can be torn apart by those needs.  Make sure you research the task of a failover itself before thinking things will automatically start working simply because a mirror became your principal.</p>
<p><strong>Recap the laziness</strong></p>
<p>Setup is a task that can be accomplished with much less effort that was needed in previous years of HA technology if Database Mirroring fits the strategy.</p>
<p><a href="/index.php/DataMgmt/DBAdmin/how-to-monitor-database-mirroring">Monitoring can be done with Event Notifications</a> which is a quick setup and solid method.</p>
<p>Licensing is not as much of an issue as you may think.  If the Mirror is designated as a Mirror, it does not require a license until in use due to a failover.<br />
From the SQL Server 2008 pricing and licensing document (this is also the case in SQL Server 2005) </p>
<blockquote><p> <i><a href="http://download.microsoft.com/.../2008%20SQL%20Licensing%20overview%20final.docx">Passive Servers. The passive server does not require a license given that no queries are being executed against it.</a></i> </p></blockquote>
<blockquote><p>Be careful with this.  No queries means, no queries.  Make sure you read the licensing guidelines thoroughly before making your purchase or take the time to contact Microsoft with questions.</p></blockquote>
<p>Have fun because you really can have HA at a low cost on both price and maintenance sides.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/lazy-dba-high-availability/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>The Lazy DBA Series: SQL Trace / Profiler</title>
		<link>/index.php/datamgmt/datadesign/lazy-dba-sql-trace/</link>
		<comments>/index.php/datamgmt/datadesign/lazy-dba-sql-trace/#comments</comments>
		<pubDate>Fri, 20 Aug 2010 11:05:50 +0000</pubDate>
		<dc:creator><![CDATA[Ted Krueger (onpnt)]]></dc:creator>
				<category><![CDATA[Data Modelling and Design]]></category>
		<category><![CDATA[Database Administration]]></category>
		<category><![CDATA[Database Programming]]></category>
		<category><![CDATA[Microsoft SQL Server]]></category>
		<category><![CDATA[Microsoft SQL Server Admin]]></category>
		<category><![CDATA[database administration]]></category>
		<category><![CDATA[sql server]]></category>
		<category><![CDATA[sqlpass]]></category>

		<guid isPermaLink="false">/index.php/2010/08/lazy-dba-sql-trace/</guid>
		<description><![CDATA[SQL Trace (and use of profiler as a front end to SQL Trace procedures) is far from a secret.  Performance tuning in the days of 2005/2008 has become much easier with the advent of DMVs but SQL Trace still has its place in the realm of, "Why is this happening".  Performance issues real-time and tuning becomes almost, easy while watching exactly what it going on with the batches hitting out database servers.  The only downfall is the impact SQL Trace plays on an active database server.  This can be done strategically and with minimal impact from a well thought through plan of attack.]]></description>
				<content:encoded><![CDATA[<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba.gif" alt="" title="" width="378" height="378" /></div>
<p>SQL Trace (and use of profiler as a front end to SQL Trace procedures) is far from a secret.  Performance tuning in the days of 2005/2008 has become much easier with the advent of DMVs but SQL Trace still has its place in the realm of, &#8220;Why is this happening&#8221;.  Performance tuning becomes almost easy while watching exactly what is going on with the batches hitting our database servers.  The only downfall is the impact SQL Trace plays on an active database server.  This can be done strategically and with minimal impact from a well thought through plan of attack.  </p>
<p>So what does SQL Trace have to do with being a lazy DBA?</p>
<div class="image_block"><img src="/wp-content/uploads/blogs/DataMgmt/lazydba_1.gif" alt="" title="" width="374" height="239" /></div>
<p><strong>Let me define what this series of blogs will show.</strong></p>
<p><i><strong>Lazy DBA</strong>: An efficient process of utilizing automation, native tools and or third party tools to assist in showing problems, possible solutions and active and long term monitoring of our database servers.</i></p>
<p>So we can see that being lazy in reality has little to do with being truly lazy.  It is the art of being better at our jobs with speed and quick results while not writing some nasty TSQL script for five days to find out the worst running query that is stored in cache.  Now that we have defined lazy in the best of ways, let’s look back at the SQL Trace functionality.  SQL Trace has the ability to show us everything that an application is sending to our database server.  True statement!  </p>
<p><strong>Scenario time:</strong></p>
<p><i>You are the only DBA in the entire world employed at your company.  This means you are the lucky and only person that does reporting, integrations, import/extract&#8230;All of it!  Director needs to update the ERP system for every single one of the ten thousand customers that are in the database.   You know the database but know it will also take you days just to determine just what columns need this update due to the most awful naming convention in a database you have ever seen.  </i></p>
<p>You are in for all-nighters for at least two days because this director holds your job in their hands.  What if I told you that you could get it done in around 20 minutes with these steps?</p>
<ol>
<li>Identify the columns the data is pulled from (5 minutes)</li>
<li>Import the new data that needs to be integrated (1 minute)</li>
<li>Write a select on the identified columns joined to the updated data (5 minutes)</li>
<li>Backup the table(s) (2 minutes)</li>
<li>Change SELECT to UPDATE with little alterations to the original query (7 minutes)</li>
</ol>
<p>OK, realistically that might be 22 minutes if the tables are big.  Might even be 22 hours if they are REALLY big.  You get the point and yes, it depends.</p>
<p>The key problem here is the identification of the columns.  That in reality takes up some time when you have extremely large (or small) databases that you don’t have the entity relational diagram memorized.  I still don’t have AdventureWorks memorized, so that isn’t so farfetched.  </p>
<p>Here is how to do this as a lazy DBA.  Open SQL Server Profiler.  Set up a typical default template trace.  Connect it to the database server (preferably development).  Before starting profiler, open the application now or get someone that can open it to do so.  Get ready to open the screen that shows the data as it passes through the application.  Hit start on the trace and then go into the application view to edit the data.  Notice I said edit.  Some application that use views or other accessing methods may make this more difficult than it needs to be.  Typically, edit is a safe bet that you get down to the base tables.</p>
<p>Stop profiler immediately once you see the transaction go by or know the window in the application is fully loaded.  </p>
<p>Depending on the method you used to store the profiler output, open either the table or trace files.  Use a LIKE statement to find the entries that consist of your login used first.  Then refine to the actual data you used in the application to filter on.  Such as customer or item number that you used to get to the screens.  </p>
<p>Refine that down (on minute 3 or 4 now) and then grab the columns.  (Copy/paste takes at least a minute)</p>
<p>Next step, import the data that needs to be used to update the database.  Hopefully they made this easy on you and didn’t give you TPS report printouts.   Write your SELECT statement and verify that the return count matches the update count (or more depending on requirements) and then validate they match up on the key relationships that you used to join on. </p>
<p>Always make sure you can recover, so make a backup of the tables in question.  Keep in mind constraints! Don’t EVER remove them just because it might work.  Data Integrity is important to you!</p>
<p>Alter your statement to an UPDATE on a join or derived table and execute it.</p>
<p>This is the best part: Email the director and say it is all ready for them to validate in development.  Sit back and relax now.  You were just lazy and it paid off by wowing them.</p>
]]></content:encoded>
			<wfw:commentRss>/index.php/datamgmt/datadesign/lazy-dba-sql-trace/feed/</wfw:commentRss>
		<slash:comments>5</slash:comments>
		</item>
	</channel>
</rss>
